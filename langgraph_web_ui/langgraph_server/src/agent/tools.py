"""
tools.py - ì—ì´ì „íŠ¸ ë„êµ¬ ì •ì˜
==============================

ì´ íŒŒì¼ì€ ì—ì´ì „íŠ¸ë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„êµ¬(Tools)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ë„êµ¬ëŠ” ì—ì´ì „íŠ¸ê°€ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

í˜„ì¬ ë„êµ¬:
- Tavily Search: ì›¹ ê²€ìƒ‰ API (ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½)
- Read URL: íŠ¹ì • URLì˜ ì „ì²´ ë‚´ìš©ì„ ì½ì–´ì˜¤ê¸°

ë„êµ¬ë€?
- LLMì´ ì§ì ‘ í•  ìˆ˜ ì—†ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
- ì˜ˆ: ì›¹ ê²€ìƒ‰, ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ, API í˜¸ì¶œ, ê³„ì‚° ë“±
"""

import os
import httpx
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool


# ================================================================
# Tavily Search Tool (ì›¹ ê²€ìƒ‰ ë„êµ¬)
# ================================================================
# 
# TavilyëŠ” AI ì—ì´ì „íŠ¸ë¥¼ ìœ„í•´ ìµœì í™”ëœ ê²€ìƒ‰ APIì…ë‹ˆë‹¤.
# ê²€ìƒ‰ ê²°ê³¼ì˜ ìš”ì•½ê³¼ URLì„ ë°˜í™˜í•©ë‹ˆë‹¤.
#
# ì‚¬ìš© ë°©ë²•:
#   result = tavily_tool.invoke("ê²€ìƒ‰í•  ì¿¼ë¦¬")
#
# ë°˜í™˜ í˜•ì‹:
#   [{"content": "ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½...", "url": "https://..."}, ...]

tavily_tool = TavilySearchResults(
    max_results=5  # ë” ë§ì€ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì¦ê°€
)


# ================================================================
# URL Reader Tool (ì›¹ í˜ì´ì§€ ë‚´ìš© ì½ê¸°)
# ================================================================
# 
# íŠ¹ì • URLì˜ ì‹¤ì œ ë‚´ìš©ì„ ì½ì–´ì™€ì„œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
# Tavilyê°€ ìš”ì•½ë§Œ ì œê³µí•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, ì „ì²´ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
#
# ì‚¬ìš© ë°©ë²•:
#   result = read_url_tool.invoke("https://arxiv.org/abs/...")
#
# ë°˜í™˜ í˜•ì‹:
#   "ì›¹ í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ë‚´ìš© (ìµœëŒ€ 8000ì)"

@tool
def read_url_tool(url: str) -> str:
    """
    Read the full content of a web page URL.
    Use this tool to get detailed information from a specific URL found during search.
    
    Args:
        url: The URL to read (e.g., "https://arxiv.org/abs/2412.03801")
    
    Returns:
        The text content of the web page (max 8000 characters)
    
    Example:
        read_url_tool("https://arxiv.org/html/2412.03801v1")
    """
    try:
        # HTTP ìš”ì²­ìœ¼ë¡œ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (403 ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ í˜„ì‹¤ì ì¸ í—¤ë”)
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Connection": "keep-alive",
        }
        
        with httpx.Client(timeout=15.0, follow_redirects=True) as client:
            response = client.get(url, headers=headers)
            response.raise_for_status()
        
        content = response.text
        
        # HTML íƒœê·¸ ê°„ë‹¨íˆ ì œê±° (BeautifulSoup ì—†ì´)
        import re
        # script, style íƒœê·¸ ì œê±°
        content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
        # HTML íƒœê·¸ ì œê±°
        content = re.sub(r'<[^>]+>', ' ', content)
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        content = re.sub(r'\s+', ' ', content).strip()
        
        # í† í° ì œí•œì„ ìœ„í•´ 8000ìë¡œ ìë¥´ê¸°
        if len(content) > 8000:
            content = content[:8000] + "... [truncated]"
        
        # ë¡œê·¸ëŠ” parallel_researcher_nodeì—ì„œ ì •ë ¬ í›„ ì¶œë ¥ë¨
        return content
        
    except httpx.TimeoutException:
        return f"Error: Timeout while reading URL: {url}"
    except httpx.HTTPStatusError as e:
        return f"Error: HTTP {e.response.status_code} for URL: {url}"
    except Exception as e:
        return f"Error reading URL {url}: {str(e)}"


# ================================================================
# Think Tool - ì „ëµì  ì‚¬ê³  (Phase 2)
# ================================================================
# 
# Analyzerê°€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•  ë•Œ ì „ëµì ìœ¼ë¡œ ìƒê°í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” ë„êµ¬.
# ReAct íŒ¨í„´ì—ì„œ ì˜ê°ì„ ë°›ì•„, í–‰ë™ ì „ì— ìƒê°ì„ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.
#
# ì‚¬ìš© ë°©ë²•:
#   result = think_tool.invoke("What did I find? What's missing? Should I continue?")
#
# ë°˜í™˜ í˜•ì‹:
#   "Thought recorded: [thought content]"

@tool
def think_tool(thought: str) -> str:
    """
    Strategic thinking tool for reflection and planning.
    Use this AFTER analyzing search results to decide next steps.
    
    This helps you:
    - Reflect on what information was found
    - Identify gaps in the research
    - Plan the next search query strategically
    - Decide whether to continue or stop searching
    
    Args:
        thought: Your strategic thinking about the current research state.
                 Should include:
                 - Key findings so far
                 - What's still missing
                 - Whether more research is needed
                 - Specific next query if needed
    
    Returns:
        Acknowledgment of the recorded thought
    
    Example:
        think_tool("Found comparison articles but missing performance benchmarks. Need to search for 'LangGraph vs CrewAI performance'")
    """
    # í™˜ê²½ë³€ìˆ˜ë¡œ verbose ì²´í¬
    verbose = os.environ.get("VERBOSE_LOGGING", "false").lower() == "true"
    if verbose:
        print(f"ğŸ’­ Think: {thought}")
    else:
        display = thought if len(thought) <= 100 else thought[:100] + "..."
        print(f"ğŸ’­ Think: {display}")
    return f"Thought recorded: {thought}"


# ================================================================
# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
# ================================================================
# 
# ResearcherëŠ” ì´ ì„¸ ë„êµ¬ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
# 1. tavily_tool: ë¨¼ì € ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ URL ì°¾ê¸°
# 2. read_url_tool: ì°¾ì€ URLì˜ ìƒì„¸ ë‚´ìš© ì½ê¸°
# 3. think_tool: ì „ëµì  ì‚¬ê³  (Phase 2)

tools = [tavily_tool, read_url_tool, think_tool]
