#!/usr/bin/env python
"""
mcp_server.py - Deep Researchìš© ì»¤ìŠ¤í…€ MCP ì„œë²„
================================================

Phase 11: ì¶”ê°€ ì—°êµ¬ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” MCP ì„œë²„

ì œê³µ ë„êµ¬:
1. í…ìŠ¤íŠ¸ ë¶„ì„:
   - summarize_text: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½
   - extract_key_points: í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ
   - count_words: ë‹¨ì–´/ë¬¸ì ìˆ˜ í†µê³„

2. íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼:
   - read_file: íŒŒì¼ ë‚´ìš© ì½ê¸°
   - list_files: ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡
   - save_research: ì—°êµ¬ ê²°ê³¼ ì €ì¥

3. ì¶”ê°€ ê²€ìƒ‰:
   - search_wikipedia: ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰
"""

import os
from mcp.server.fastmcp import FastMCP

# MCP ì„œë²„ ìƒì„±
mcp = FastMCP("deep-research-tools")

# ================================================================
# 1. í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬
# ================================================================

@mcp.tool()
def summarize_text(text: str, max_length: int = 200) -> str:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.
    
    Args:
        text: ìš”ì•½í•  í…ìŠ¤íŠ¸
        max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´ (ê¸°ë³¸ê°’: 200ì)
    
    Returns:
        ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    if len(text) <= max_length:
        return text
    
    # ê°„ë‹¨í•œ ìš”ì•½: ì²« ë¶€ë¶„ + ë§ˆì§€ë§‰ ë¶€ë¶„
    half = max_length // 2
    return f"{text[:half]}... {text[-half:]}"


@mcp.tool()
def extract_key_points(text: str, num_points: int = 5) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        num_points: ì¶”ì¶œí•  í¬ì¸íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    
    Returns:
        í•µì‹¬ í¬ì¸íŠ¸ ëª©ë¡
    """
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    # ìƒìœ„ Nê°œ ë¬¸ì¥ ì„ íƒ (ê¸¸ì´ ê¸°ì¤€)
    sorted_sentences = sorted(sentences, key=len, reverse=True)
    top_sentences = sorted_sentences[:num_points]
    
    result = "Key Points:\n"
    for i, sentence in enumerate(top_sentences, 1):
        result += f"  {i}. {sentence[:100]}...\n"
    
    return result


@mcp.tool()
def count_words(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ì„¸ì„œ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    
    Returns:
        ë‹¨ì–´ ìˆ˜ ë° ë¬¸ì ìˆ˜ í†µê³„
    """
    words = text.split()
    chars = len(text)
    sentences = text.count('.') + text.count('!') + text.count('?')
    
    return f"Stats: {len(words)} words, {chars} chars, ~{sentences} sentences"


# ================================================================
# 2. íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ ë„êµ¬
# ================================================================

# í—ˆìš©ëœ ë””ë ‰í† ë¦¬ (ë³´ì•ˆì„ ìœ„í•´ ì œí•œ)
ALLOWED_DIRS = [
    os.path.expanduser("~/LangGraph_PJT"),
    "/tmp/deep_research"
]


@mcp.tool()
def read_file(file_path: str, max_chars: int = 5000) -> str:
    """
    íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    
    Args:
        file_path: ì½ì„ íŒŒì¼ ê²½ë¡œ
        max_chars: ìµœëŒ€ ì½ì„ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 5000)
    
    Returns:
        íŒŒì¼ ë‚´ìš© ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
    """
    try:
        # ë³´ì•ˆ: í—ˆìš©ëœ ë””ë ‰í† ë¦¬ë§Œ ì ‘ê·¼
        abs_path = os.path.abspath(os.path.expanduser(file_path))
        allowed = any(abs_path.startswith(d) for d in ALLOWED_DIRS)
        
        if not allowed:
            return f"Error: Access denied. Allowed directories: {ALLOWED_DIRS}"
        
        if not os.path.exists(abs_path):
            return f"Error: File not found: {file_path}"
        
        with open(abs_path, 'r', encoding='utf-8') as f:
            content = f.read(max_chars)
            
        if len(content) >= max_chars:
            content += f"\n... (truncated at {max_chars} chars)"
            
        return content
        
    except Exception as e:
        return f"Error reading file: {str(e)}"


@mcp.tool()
def list_files(directory: str, extension: str = "") -> str:
    """
    ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        directory: íƒìƒ‰í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        extension: í•„í„°ë§í•  í™•ì¥ì (ì˜ˆ: ".md", ".py")
    
    Returns:
        íŒŒì¼ ëª©ë¡
    """
    try:
        abs_path = os.path.abspath(os.path.expanduser(directory))
        allowed = any(abs_path.startswith(d) for d in ALLOWED_DIRS)
        
        if not allowed:
            return f"Error: Access denied. Allowed directories: {ALLOWED_DIRS}"
        
        if not os.path.isdir(abs_path):
            return f"Error: Not a directory: {directory}"
        
        files = []
        for f in os.listdir(abs_path):
            if extension and not f.endswith(extension):
                continue
            full_path = os.path.join(abs_path, f)
            file_type = "DIR" if os.path.isdir(full_path) else "FILE"
            files.append(f"[{file_type}] {f}")
        
        return f"Files in {directory}:\n" + "\n".join(files[:50])
        
    except Exception as e:
        return f"Error listing directory: {str(e)}"


@mcp.tool()
def save_research(filename: str, content: str) -> str:
    """
    ì—°êµ¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        filename: ì €ì¥í•  íŒŒì¼ ì´ë¦„ (í™•ì¥ì í¬í•¨)
        content: ì €ì¥í•  ë‚´ìš©
    
    Returns:
        ì €ì¥ ê²°ê³¼ ë©”ì‹œì§€
    """
    try:
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = "/tmp/deep_research"
        os.makedirs(save_dir, exist_ok=True)
        
        # íŒŒì¼ ê²½ë¡œ (ë³´ì•ˆ: ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ë§Œ ì €ì¥)
        safe_filename = os.path.basename(filename)  # ê²½ë¡œ ì£¼ì… ë°©ì§€
        file_path = os.path.join(save_dir, safe_filename)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return f"âœ… Saved to: {file_path} ({len(content)} chars)"
        
    except Exception as e:
        return f"Error saving file: {str(e)}"


# ================================================================
# 3. ì¶”ê°€ ê²€ìƒ‰ ë„êµ¬
# ================================================================

@mcp.tool()
def search_wikipedia(query: str, sentences: int = 3) -> str:
    """
    ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        sentences: ë°˜í™˜í•  ë¬¸ì¥ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
    """
    try:
        import urllib.request
        import urllib.parse
        import json
        
        # Wikipedia API í˜¸ì¶œ
        encoded_query = urllib.parse.quote(query)
        url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{encoded_query}"
        
        req = urllib.request.Request(url, headers={'User-Agent': 'DeepResearch/1.0'})
        with urllib.request.urlopen(req, timeout=10) as response:
            data = json.loads(response.read().decode())
        
        title = data.get('title', 'Unknown')
        extract = data.get('extract', 'No content found.')
        
        # ë¬¸ì¥ ìˆ˜ ì œí•œ
        sentences_list = extract.split('. ')[:sentences]
        limited_extract = '. '.join(sentences_list)
        if not limited_extract.endswith('.'):
            limited_extract += '.'
        
        return f"ğŸ“š Wikipedia: {title}\n\n{limited_extract}"
        
    except Exception as e:
        return f"Wikipedia search failed: {str(e)}"


if __name__ == "__main__":
    print("ğŸš€ Starting Deep Research MCP Server...")
    print("ğŸ“¦ Available tools: summarize_text, extract_key_points, count_words")
    print("ğŸ“ File tools: read_file, list_files, save_research")
    print("ğŸ” Search tools: search_wikipedia")
    mcp.run(transport="stdio")
