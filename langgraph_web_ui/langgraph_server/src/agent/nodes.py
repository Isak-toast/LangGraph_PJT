"""
nodes.py - Deep Research ë…¸ë“œ êµ¬í˜„
===================================

5ê°œ ë…¸ë“œë¡œ êµ¬ì„±ëœ Deep Research ì‹œìŠ¤í…œ:
1. Planner: ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½
2. Searcher: ì›¹ ê²€ìƒ‰ (Tavily)
3. ContentReader: URL ë‚´ìš© ì½ê¸°
4. Analyzer: ì •ë³´ ë¶„ì„ + ì¶”ê°€ ê²€ìƒ‰ íŒë‹¨
5. Writer: ìµœì¢… ì‘ë‹µ ì‘ì„±

ê·¸ë˜í”„ êµ¬ì¡°:
  Planner â†’ Searcher â†’ ContentReader â†’ Analyzer â†’ Writer
                 â†‘                          â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      (ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì‹œ)
"""

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from src.agent.state import DeepResearchState
from src.agent.tools import tavily_tool, read_url_tool, think_tool


# ================================================================
# LLM ë° ì„¤ì • ì´ˆê¸°í™” (Phase 6: Multi-LLM)
# ================================================================

from src.agent.config import research_config

# ì—­í• ë³„ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = research_config.get_llm("analyzer")  # ê¸°ë³¸ LLM (ë¶„ì„ìš©)
planner_llm = research_config.get_llm("planner")
writer_llm = research_config.get_llm("writer")
critic_llm = research_config.get_llm("critic")

# ================================================================
# ë¡œê¹… ì„¤ì •
# ================================================================

# í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì „ì—­ ì„¤ì •ìœ¼ë¡œ verbose ëª¨ë“œ ì œì–´
import os
VERBOSE_MODE = research_config.verbose_mode


def truncate_text(text: str, max_len: int = 200, force_full: bool = False) -> str:
    """í…ìŠ¤íŠ¸ ìë¥´ê¸° (verbose ëª¨ë“œë©´ ì „ì²´ ì¶œë ¥)
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        max_len: ìµœëŒ€ ê¸¸ì´ (ê¸°ë³¸ 200ì)
        force_full: Trueë©´ ë¬´ì¡°ê±´ ì „ì²´ ì¶œë ¥
    
    Returns:
        ì˜ë¦° í…ìŠ¤íŠ¸ (í•„ìš”ì‹œì—ë§Œ ... ì¶”ê°€)
    """
    if force_full or VERBOSE_MODE:
        return text
    
    if len(text) <= max_len:
        return text  # ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ (... ì—†ì´)
    
    return text[:max_len] + "..."


# ================================================================
# 0. Clarify ë…¸ë“œ - ì§ˆë¬¸ ë¶„ì„ ë° ëª…í™•í™” (Phase 3)
# ================================================================

CLARIFY_PROMPT = """You are a QUERY ANALYZER. Assess if the user's question needs clarification.

<Task>
Analyze the user query for:
1. Ambiguous terms or acronyms that might have multiple meanings
2. Missing context (time period, scope, specific technology)
3. Unclear intent (asking for comparison vs explanation vs tutorial)
</Task>

<Decision Criteria>
NEEDS_CLARIFICATION when:
- Contains acronyms without context (e.g., "RAG" could be Retrieval-Augmented Generation or other)
- Timeframe is unclear for trending topics
- Comparing items without specifying criteria
- Very broad topics without focus

CLEAR when:
- Query is specific and well-defined
- Context is sufficient for research
- Intent is obvious

Most queries are CLEAR. Only flag truly ambiguous ones.
</Decision Criteria>

<Output Format>
{
    "needs_clarification": boolean,
    "clarification_question": "question to ask user (if needed)" or null,
    "analysis": "brief analysis of the query",
    "detected_topics": ["topic1", "topic2"]
}
</Output Format>
"""

def clarify_node(state: DeepResearchState) -> dict:
    """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ëª…í™•í™” í•„ìš” ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” Clarify ë…¸ë“œ"""
    
    messages = state.get("messages", [])
    user_query = ""
    for msg in messages:
        if isinstance(msg, HumanMessage) or (hasattr(msg, 'type') and msg.type == 'human'):
            user_query = msg.content
            break
    
    print(f"\nğŸ” Clarify: Analyzing query...")
    print(f"   â””â”€ Query: {truncate_text(user_query, 80)}")
    
    try:
        # LLMì—ê²Œ ì§ˆë¬¸ ë¶„ì„ ìš”ì²­
        structured_llm = llm.with_structured_output({
            "type": "object",
            "properties": {
                "needs_clarification": {"type": "boolean"},
                "clarification_question": {"type": "string"},
                "analysis": {"type": "string"},
                "detected_topics": {"type": "array", "items": {"type": "string"}}
            },
            "required": ["needs_clarification", "analysis", "detected_topics"]
        })
        
        result = structured_llm.invoke([
            SystemMessage(content=CLARIFY_PROMPT),
            HumanMessage(content=f"Analyze this query: {user_query}")
        ])
        
        needs_clarification = result.get("needs_clarification", False)
        clarification_question = result.get("clarification_question")
        analysis = result.get("analysis", "")
        topics = result.get("detected_topics", [])
        
        # ë¡œê¹…
        status = "ğŸŸ¡ Needs clarification" if needs_clarification else "ğŸŸ¢ Clear"
        print(f"   â””â”€ Status: {status}")
        print(f"   â””â”€ Analysis: {truncate_text(analysis, 150)}")
        print(f"   â””â”€ Topics: {', '.join(topics[:5])}")
        
        if needs_clarification and clarification_question:
            print(f"   â””â”€ Suggested question: {clarification_question}")
        
        return {
            "needs_clarification": needs_clarification,
            "clarification_question": clarification_question if needs_clarification else None,
            "query_analysis": analysis
        }
        
    except Exception as e:
        print(f"âŒ Clarify error: {e}")
        # ì—ëŸ¬ ì‹œ ëª…í™•í™” ë¶ˆí•„ìš”ë¡œ ì²˜ë¦¬
        return {
            "needs_clarification": False,
            "clarification_question": None,
            "query_analysis": f"Analysis failed: {e}"
        }


# ================================================================
# 1. Planner ë…¸ë“œ - ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½
# ================================================================

PLANNER_PROMPT = """You are a RESEARCH PLANNER. Your job is to create a research strategy.

<Task>
Analyze the user's question and create a comprehensive research plan.
</Task>

<Requirements>
1. Create multiple search queries (in English for better results)
2. Identify focus areas to explore
3. Determine appropriate depth level (1=quick, 2=medium, 3=deep)
</Requirements>

<Output_Format>
{
    "search_queries": ["query1", "query2", "query3"],
    "focus_areas": ["area1", "area2"],
    "depth_level": 2
}

depth_level: 1=quick, 2=medium, 3=deep
</Output_Format>

<Examples>
- "LangGraph Vision AI papers" â†’ queries: ["LangGraph Vision AI paper", "LangGraph computer vision", "LangGraph image processing agent"]
- "AI trends 2024" â†’ queries: ["AI trends 2024", "machine learning trends 2024", "generative AI advances 2024"]
</Examples>

<Guidelines>
- Create 2-4 diverse search queries to get comprehensive results
- Use English for search queries for broader results
- Ensure queries cover different aspects of the topic
</Guidelines>
"""

def planner_node(state: DeepResearchState) -> dict:
    """ë¦¬ì„œì¹˜ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” Planner ë…¸ë“œ"""
    
    messages = state["messages"]
    user_query = ""
    
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage) or (hasattr(msg, 'type') and msg.type == 'human'):
            user_query = msg.content
            break
    
    print(f"ğŸ“‹ Planner: Creating research plan for: {user_query[:50]}")
    
    # LLMì—ê²Œ ê³„íš ìƒì„± ìš”ì²­ (Phase 6: planner_llm ì‚¬ìš©)
    structured_llm = planner_llm.with_structured_output({
        "type": "object",
        "properties": {
            "search_queries": {"type": "array", "items": {"type": "string"}},
            "focus_areas": {"type": "array", "items": {"type": "string"}},
            "depth_level": {"type": "integer", "minimum": 1, "maximum": 3}
        },
        "required": ["search_queries", "focus_areas", "depth_level"]
    })
    
    try:
        plan = structured_llm.invoke(f"{PLANNER_PROMPT}\n\nUser Question: {user_query}")
        queries = plan.get('search_queries', [])
        print(f"\nğŸ“‹ Planner: Generated {len(queries)} queries")
        print("   â””â”€ Queries:")
        for i, q in enumerate(queries, 1):
            print(f"      [{i}] {q}")
        if plan.get('focus_areas'):
            print(f"   â””â”€ Focus: {', '.join(plan.get('focus_areas', []))}")
    except Exception as e:
        print(f"âŒ Planner error: {e}")
        plan = {
            "search_queries": [user_query],
            "focus_areas": ["general"],
            "depth_level": 2
        }
    
    return {
        "research_plan": plan,
        "current_query_index": 0,
        "research_iteration": 1,
        "search_results": [],
        "urls_to_read": [],
        "read_contents": [],
        "findings": []
    }


# ================================================================
# 2. Searcher ë…¸ë“œ - ì›¹ ê²€ìƒ‰
# ================================================================

def searcher_node(state: DeepResearchState) -> dict:
    """Tavily ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” Searcher ë…¸ë“œ"""
    
    plan = state.get("research_plan", {})
    queries = plan.get("search_queries", [])
    current_idx = state.get("current_query_index", 0)
    iteration = state.get("research_iteration", 1)
    
    # ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    next_query = state.get("next_search_query")
    if next_query:
        query = next_query
        print(f"ğŸ” Searcher [{iteration}]: Follow-up search for: {query}")
    elif current_idx < len(queries):
        query = queries[current_idx]
        print(f"ğŸ” Searcher [{iteration}]: Searching for: {query}")
    else:
        return {"search_results": [], "urls_to_read": []}
    
    try:
        results = tavily_tool.invoke(query)
        urls = [r.get("url", "") for r in results if r.get("url")]
        
        print(f"\nğŸ” Searcher: Found {len(results)} results")
        print("   â””â”€ URLs found:")
        for i, url in enumerate(urls[:5], 1):
            print(f"      [{i}] {url}")
        print("   â””â”€ Snippets:")
        for r in results[:3]:
            snippet = truncate_text(r.get('content', '').replace('\n', ' '), 200)
            print(f"      â€¢ {snippet}")
        
        # think_tool: ê²€ìƒ‰ í›„ ì „ëµì  ë¶„ì„
        snippets_summary = " | ".join([r.get('content', '')[:100] for r in results[:3]])
        think_result = think_tool.invoke(
            f"Query: {query} | Found {len(results)} results, {len(urls)} URLs. "
            f"Key snippets: {snippets_summary[:300]}. "
            f"Assessment: Is this sufficient or need more specific search?"
        )
        
        return {
            "search_results": results,
            "urls_to_read": urls[:5],
            "current_query_index": current_idx + 1,
            "next_search_query": None
        }
    except Exception as e:
        print(f"âŒ Searcher error: {e}")
        return {"search_results": [], "urls_to_read": []}


# ================================================================
# 3. ContentReader ë…¸ë“œ - URL ë‚´ìš© ì½ê¸°
# ================================================================

def content_reader_node(state: DeepResearchState) -> dict:
    """ë³¸ë¬¸ ë‚´ìš©ì„ ì½ëŠ” ContentReader ë…¸ë“œ"""
    
    urls = state.get("urls_to_read", [])
    existing_contents = state.get("read_contents", [])
    
    if not urls:
        print("ğŸ“– ContentReader: No URLs to read")
        return {"read_contents": existing_contents}
    
    print(f"\nğŸ“– ContentReader: Reading {len(urls[:3])} URLs")
    
    new_contents = []
    for url in urls[:3]:  # ìƒìœ„ 3ê°œë§Œ ì½ê¸° (í† í° ì ˆì•½)
        try:
            content = read_url_tool.invoke(url)
            new_contents.append({
                "url": url,
                "content": content[:4000],  # ê° URL 4000ì ì œí•œ
                "title": url.split("/")[-1]
            })
            preview = truncate_text(content.replace('\n', ' '), 300)
            print(f"   â””â”€ [{truncate_text(url, 60)}]")
            print(f"      Preview: {preview}")
        except Exception as e:
            print(f"   âœ— Failed: {truncate_text(url, 40)} ({e})")
    
    # ê¸°ì¡´ ë‚´ìš© + ìƒˆ ë‚´ìš©
    all_contents = existing_contents + new_contents
    
    return {"read_contents": all_contents, "urls_to_read": []}


# ================================================================
# 4. Analyzer ë…¸ë“œ - ì •ë³´ ë¶„ì„ + ì¶”ê°€ ê²€ìƒ‰ íŒë‹¨
# ================================================================

ANALYZER_PROMPT = """You are a RESEARCH ANALYZER. Analyze the collected information.

<Task>
1. Extract key findings from the search results and read contents
2. Determine if the information is sufficient to answer the user's question
3. If more research is needed, suggest a specific search query
</Task>

<Show_Your_Thinking>
BEFORE making a decision, think strategically about:
- What key information did I find?
- What's still missing to fully answer the question?
- Is additional research worth the time cost?
- What specific query would fill the gaps?
</Show_Your_Thinking>

<Decision_Criteria>
STOP researching (needs_more_research=false) when:
- You have 3+ quality sources covering the main points
- You found specific data, examples, or expert opinions
- Additional searches would likely return duplicate information

CONTINUE researching (needs_more_research=true) when:
- Key aspects of the question are unanswered
- You only have 1-2 low-quality sources
- You're missing specific examples or data
</Decision_Criteria>

<Hard_Limits>
- Maximum 3 research iterations (enforced by system)
- Stop if you have enough information for a good answer
- Prefer quality over quantity
</Hard_Limits>

<Output_Format>
{
    "findings": ["finding1", "finding2", ...],
    "needs_more_research": true/false,
    "next_search_query": "specific query if more research needed"
}
</Output_Format>
"""


def analyzer_node(state: DeepResearchState) -> dict:
    """ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” Analyzer ë…¸ë“œ"""
    
    search_results = state.get("search_results", [])
    read_contents = state.get("read_contents", [])
    iteration = state.get("research_iteration", 1)
    existing_findings = state.get("findings", [])
    
    print(f"ğŸ”¬ Analyzer [{iteration}]: Analyzing {len(search_results)} results, {len(read_contents)} contents")
    
    # ë¶„ì„í•  ë‚´ìš© ì¤€ë¹„
    content_summary = ""
    for r in search_results[:5]:
        content_summary += f"- {r.get('content', '')[:500]}\n"
    for c in read_contents:
        content_summary += f"- [URL: {c.get('url', '')}] {c.get('content', '')[:800]}\n"
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    user_query = ""
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    # LLM ë¶„ì„
    structured_llm = llm.with_structured_output({
        "type": "object",
        "properties": {
            "findings": {"type": "array", "items": {"type": "string"}},
            "needs_more_research": {"type": "boolean"},
            "next_search_query": {"type": "string"}
        },
        "required": ["findings", "needs_more_research"]
    })
    
    try:
        prompt = f"""{ANALYZER_PROMPT}

User Question: {user_query}
Research Iteration: {iteration}/3

Collected Information:
{content_summary[:6000]}

Existing Findings: {existing_findings}
"""
        analysis = structured_llm.invoke(prompt)
        
        new_findings = existing_findings + analysis.get("findings", [])
        needs_more = analysis.get("needs_more_research", False)
        next_query = analysis.get("next_search_query", "")
        
        # ìµœëŒ€ 3íšŒ ë°˜ë³µ ì œí•œ
        if iteration >= 3:
            needs_more = False
            print("\nğŸ”¬ Analyzer: Max iterations reached, proceeding to Writer")
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        print(f"\nğŸ”¬ Analyzer [{iteration}]: Analyzed {len(search_results)} results, {len(read_contents)} contents")
        if analysis.get("findings"):
            print("   â””â”€ New findings:")
            for i, finding in enumerate(analysis.get("findings", [])[:5], 1):
                preview = truncate_text(finding.replace('\n', ' '), 150)
                print(f"      [{i}] {preview}")
        
        if needs_more:
            print(f"   â””â”€ Decision: More research needed")
            print(f"   â””â”€ Next query: {next_query}")
        else:
            print(f"   â””â”€ Decision: Research complete ({len(new_findings)} total findings)")
        
        return {
            "findings": new_findings,
            "needs_more_research": needs_more,
            "next_search_query": next_query if needs_more else None,
            "research_iteration": iteration + 1 if needs_more else iteration
        }
        
    except Exception as e:
        print(f"âŒ Analyzer error: {e}")
        return {
            "findings": existing_findings,
            "needs_more_research": False,
            "next_search_query": None
        }


# ================================================================
# 5. Compress ë…¸ë“œ - ì—°êµ¬ ê²°ê³¼ ì••ì¶• (Phase 1)
# ================================================================

COMPRESS_PROMPT = """You are a RESEARCH COMPRESSOR. Your job is to clean up and compress research findings.

<Task>
Clean up information gathered from research. Remove duplicates, preserve key facts with citations.
All relevant information should be preserved but in a cleaner, more organized format.
</Task>

<Guidelines>
1. Remove duplicate or redundant information
2. Preserve ALL key facts, statistics, and insights
3. Group related findings together
4. Add inline citations [1], [2], etc. for each source
5. Include a Sources section at the end
</Guidelines>

<Output Format>
## Key Findings
- Finding 1 [1]
- Finding 2 [2]
- ...

## Detailed Information
(Organized, deduplicated content with citations)

## Sources
[1] URL or source name
[2] URL or source name
...
</Output Format>

<Citation Rules>
- Assign each unique URL a citation number [1], [2], [3]...
- Use citations inline after each fact
- List all sources at the end with corresponding numbers
- Number sources sequentially without gaps
</Citation Rules>
"""

def compress_node(state: DeepResearchState) -> dict:
    """ì—°êµ¬ ê²°ê³¼ë¥¼ ì••ì¶•í•˜ê³  ì •ë¦¬í•˜ëŠ” Compress ë…¸ë“œ"""
    
    findings = state.get("findings", [])
    read_contents = state.get("read_contents", [])
    search_results = state.get("search_results", [])
    
    print(f"\nğŸ“¦ Compress: Compressing {len(findings)} findings, {len(read_contents)} contents")
    
    # ì†ŒìŠ¤ URL ìˆ˜ì§‘
    source_urls = list(set([c.get("url", "") for c in read_contents if c.get("url")]))
    
    # ì••ì¶•í•  ë‚´ìš© ì¤€ë¹„
    content_to_compress = ""
    
    # Findings
    content_to_compress += "=== FINDINGS ===\n"
    for i, finding in enumerate(findings, 1):
        content_to_compress += f"[{i}] {finding}\n"
    
    # Read contents (ì¼ë¶€)
    content_to_compress += "\n=== SOURCE CONTENTS ===\n"
    for c in read_contents[:5]:
        url = c.get("url", "Unknown")
        text = c.get("content", "")[:800]
        content_to_compress += f"\n[Source: {url}]\n{text}\n"
    
    # ì†ŒìŠ¤ URL ëª©ë¡
    content_to_compress += "\n=== SOURCE URLS ===\n"
    for i, url in enumerate(source_urls, 1):
        content_to_compress += f"[{i}] {url}\n"
    
    try:
        prompt = f"""{COMPRESS_PROMPT}

Here is the raw research data to compress:

{content_to_compress[:8000]}

Now compress and organize this information with proper citations:
"""
        response = llm.invoke([HumanMessage(content=prompt)])
        compressed = response.content
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        print(f"   â””â”€ Compressed to {len(compressed)} chars (from ~{len(content_to_compress)} raw chars)")
        print(f"   â””â”€ Sources cited: {len(source_urls)}")
        preview = truncate_text(compressed, 400).replace('\n', '\n      ')
        print(f"   â””â”€ Preview:\n      {preview}")
        
        return {"compressed_notes": compressed}
        
    except Exception as e:
        print(f"âŒ Compress error: {e}")
        # ì—ëŸ¬ ì‹œ ì›ë³¸ findings ë°˜í™˜
        fallback = "\n".join(f"- {f}" for f in findings)
        return {"compressed_notes": fallback}


# ================================================================
# 6. Writer ë…¸ë“œ - ìµœì¢… ì‘ë‹µ ì‘ì„±
# ================================================================

WRITER_PROMPT = """You are a PROFESSIONAL WRITER. Write the FINAL RESPONSE based on research.

<Task>
Synthesize ALL research findings into a comprehensive, well-structured response.
</Task>

<Requirements>
1. Write in Korean (í•œêµ­ì–´)
2. Use proper markdown formatting
3. Include analysis and insights, not just copied findings
4. Reference key sources with inline citations
</Requirements>

<Output_Structure>
## í•µì‹¬ ìš”ì•½
(1-2 sentences overview of the main findings)

## ì£¼ìš” ë°œê²¬ ì‚¬í•­
(Key bullet points from research with citations [1], [2]...)

## ìƒì„¸ ë¶„ì„
(Detailed analysis organized by topic or theme)

## ê´€ë ¨ ìë£Œ ë° ì¶œì²˜
(List of sources with URLs and references)

## ê²°ë¡  ë° í‰ê°€
(Your synthesis, assessment, and recommendations)
</Output_Structure>

<Quality_Guidelines>
- Write clear, professional Korean
- DO NOT just copy findings - synthesize and analyze
- Provide valuable insights and actionable conclusions
- Ensure logical flow between sections
- Use proper citation format [1], [2]...
</Quality_Guidelines>
"""

def writer_node(state: DeepResearchState) -> dict:
    """ìµœì¢… ì‘ë‹µì„ ì‘ì„±í•˜ëŠ” Writer ë…¸ë“œ"""
    
    findings = state.get("findings", [])
    read_contents = state.get("read_contents", [])
    search_results = state.get("search_results", [])
    compressed_notes = state.get("compressed_notes", "")  # Phase 1: ì••ì¶•ëœ ë…¸íŠ¸ ì‚¬ìš©
    
    print(f"\nâœï¸ Writer: Composing response from {len(findings)} findings")
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
    user_query = ""
    messages = state.get("messages", [])
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage) or (hasattr(msg, 'type') and msg.type == 'human'):
            user_query = getattr(msg, 'content', str(msg))
            break
    
    # ì†ŒìŠ¤ URL ëª©ë¡
    source_urls = list(set([c.get("url", "") for c in read_contents if c.get("url")]))
    
    # Phase 1: compressed_notesê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹
    if compressed_notes:
        print(f"   â””â”€ Using compressed notes ({len(compressed_notes)} chars)")
        research_content = compressed_notes
    else:
        # fallback: ì›ë³¸ findings ì‚¬ìš©
        research_content = "\n".join(f"- {f}" for f in findings) if findings else "No findings available"
        for c in read_contents[:3]:
            research_content += f"\n\n[Source: {c.get('url', '')}]\n{c.get('content', '')[:500]}"
    
    # URLs ë¬¸ìì—´
    urls_str = "\n".join(f"- {url}" for url in source_urls) if source_urls else "- No source URLs"
    
    full_prompt = f"""{WRITER_PROMPT}

USER QUESTION: {user_query}

RESEARCH CONTENT (already organized with citations):
{research_content}

SOURCE URLs:
{urls_str}

Now write the final comprehensive response in Korean (í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”).
IMPORTANT: Preserve and include the citations [1], [2], etc. from the research content.
"""
    
    try:
        # HumanMessageë¡œ í˜¸ì¶œí•´ì•¼ Geminiê°€ ì œëŒ€ë¡œ ì‘ë‹µí•¨ (Phase 6: writer_llm ì‚¬ìš©)
        response = writer_llm.invoke([HumanMessage(content=full_prompt)])
        content = response.content
        
        if not content or len(content.strip()) < 50:
            # fallback ì‘ë‹µ ìƒì„±
            content = f"""## ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½

{findings_str}

### ì¶œì²˜
{urls_str}
"""
        
        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        print(f"\nâœï¸ Writer: Generated response ({len(content)} chars)")
        print("   â””â”€ Sources used:")
        for i, url in enumerate(source_urls[:3], 1):
            print(f"      [{i}] {truncate_text(url, 60)}")
        print(f"   â””â”€ Response preview:")
        preview = truncate_text(content, 500).replace('\n', '\n      ')
        print(f"      {preview}")
        
    except Exception as e:
        print(f"âŒ Writer error: {e}")
        # ì—ëŸ¬ ì‹œì—ë„ ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ë°˜í™˜
        content = f"""## ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½

{findings_str}

### ì°¸ê³  ìë£Œ
{urls_str}

> ìƒì„¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”.
"""
    
    return {
        "messages": [AIMessage(content=content, name="Writer")]
    }


# ================================================================
# 7. Critique ë…¸ë“œ - CARC ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€ (Phase 5 í™•ì¥)
# ================================================================

CRITIQUE_PROMPT = """You are a RESPONSE QUALITY EVALUATOR using the CARC Framework.

<Task>
Evaluate the research response quality using 4 dimensions, each scored 1-5.
</Task>

<CARC_Framework>
1. **Completeness** (1-5): Did the response answer ALL parts of the question?
   - 5: Fully complete, addresses every aspect
   - 3: Partially complete, some aspects missing
   - 1: Incomplete, major parts unanswered

2. **Accuracy** (1-5): Are the cited facts and sources correct?
   - 5: All citations accurate and verifiable
   - 3: Some inaccuracies or questionable sources
   - 1: Major factual errors or fabricated citations

3. **Relevance** (1-5): Is the response directly relevant to the question?
   - 5: Highly relevant, stays on topic throughout
   - 3: Somewhat relevant, includes tangential info
   - 1: Off-topic or irrelevant content

4. **Clarity** (1-5): Is the response well-structured and easy to understand?
   - 5: Excellent structure, clear language
   - 3: Decent structure, some unclear parts
   - 1: Disorganized, hard to follow
</CARC_Framework>

<Output_Format>
{
    "completeness": 4,
    "accuracy": 5,
    "relevance": 4,
    "clarity": 5,
    "total": 18,
    "feedback": "Brief overall assessment",
    "improvement_suggestions": ["suggestion1", "suggestion2"]
}
</Output_Format>

<Decision>
- Total >= 16: Excellent quality âœ…
- Total 12-15: Good quality, minor improvements possible
- Total < 12: Needs significant improvement âš ï¸
</Decision>
"""

def critique_node(state: DeepResearchState) -> dict:
    """CARC í”„ë ˆì„ì›Œí¬ë¡œ ì‘ë‹µ í’ˆì§ˆì„ ë‹¤ì°¨ì› í‰ê°€í•˜ëŠ” Critique ë…¸ë“œ"""
    
    # ë§ˆì§€ë§‰ Writer ì‘ë‹µ ì°¾ê¸°
    messages = state.get("messages", [])
    writer_response = ""
    for msg in reversed(messages):
        if hasattr(msg, 'name') and msg.name == "Writer":
            writer_response = msg.content
            break
    
    if not writer_response:
        print("ğŸ” Critique: No Writer response found, skipping...")
        return {
            "quality_completeness": None,
            "quality_accuracy": None,
            "quality_relevance": None,
            "quality_clarity": None,
            "quality_total": None,
            "critique_feedback": None,
            "needs_improvement": False
        }
    
    # ì›ë³¸ ì§ˆë¬¸ ì°¾ê¸°
    user_query = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    print(f"\nğŸ” Critique: CARC Quality Evaluation...")
    
    try:
        # í‰ê°€ ìš”ì²­
        evaluation_request = f"""
Original Question: {user_query}

Response to Evaluate:
{writer_response[:3000]}...

Please evaluate this response using the CARC Framework.
"""
        
        # JSON ì¶œë ¥ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ì‘ë‹µ ìš”ì²­
        from pydantic import BaseModel
        from typing import List
        
        class CARCResult(BaseModel):
            completeness: int
            accuracy: int
            relevance: int
            clarity: int
            total: int
            feedback: str
            improvement_suggestions: List[str]
        
        # Phase 6: critic_llm ì‚¬ìš©
        structured_critic = critic_llm.with_structured_output(CARCResult)
        
        result = structured_critic.invoke([
            SystemMessage(content=CRITIQUE_PROMPT),
            HumanMessage(content=evaluation_request)
        ])
        
        # ê²°ê³¼ ê³„ì‚° ë° ë¡œê¹…
        c, a, r, cl = result.completeness, result.accuracy, result.relevance, result.clarity
        total = c + a + r + cl
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if total >= 16:
            grade = "âœ… Excellent"
        elif total >= 12:
            grade = "ğŸ‘ Good"
        else:
            grade = "âš ï¸ Needs work"
        
        needs_improvement = total < 14
        
        print(f"   â””â”€ CARC Scores: C={c} A={a} R={r} C={cl}")
        print(f"   â””â”€ Total: {total}/20 {grade}")
        print(f"   â””â”€ Feedback: {truncate_text(result.feedback, 150)}")
        
        return {
            "quality_completeness": c,
            "quality_accuracy": a,
            "quality_relevance": r,
            "quality_clarity": cl,
            "quality_total": total,
            "critique_feedback": result.feedback,
            "needs_improvement": needs_improvement
        }
        
    except Exception as e:
        print(f"âŒ Critique error: {e}")
        return {
            "quality_completeness": None,
            "quality_accuracy": None,
            "quality_relevance": None,
            "quality_clarity": None,
            "quality_total": None,
            "critique_feedback": str(e),
            "needs_improvement": False
        }


# ================================================================
# ë¼ìš°íŒ… í•¨ìˆ˜
# ================================================================

def should_continue_research(state: DeepResearchState) -> str:
    """Analyzer í›„ ì¶”ê°€ ê²€ìƒ‰ ì—¬ë¶€ íŒë‹¨"""
    if state.get("needs_more_research", False):
        return "continue"
    return "finish"


def route_after_planner(state: DeepResearchState) -> str:
    """Planner í›„ Searcherë¡œ ì´ë™"""
    return "Searcher"
