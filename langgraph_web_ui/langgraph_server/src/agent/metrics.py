"""
metrics.py - Deep Research ì„±ëŠ¥ ì¸¡ì •
=====================================

ê° Phaseë³„ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•˜ê¸° ìœ„í•œ ë„êµ¬
"""

import time
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Optional, List
import json
import os


@dataclass
class ResearchMetrics:
    """ì—°êµ¬ ì„±ëŠ¥ ì§€í‘œ"""
    
    # ê¸°ë³¸ ì •ë³´
    phase: str                      # "Phase 0", "Phase 1", ...
    query: str                      # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    timestamp: str                  # ì¸¡ì • ì‹œê°„
    
    # ì‹œê°„ ì§€í‘œ
    total_time_sec: float           # ì´ ì†Œìš” ì‹œê°„
    
    # í˜¸ì¶œ ì§€í‘œ
    llm_calls: int                  # LLM í˜¸ì¶œ íšŸìˆ˜
    search_calls: int               # ê²€ìƒ‰ API í˜¸ì¶œ íšŸìˆ˜
    urls_read: int                  # ì½ì€ URL ìˆ˜
    research_iterations: int        # ë°˜ë³µ ê²€ìƒ‰ íšŸìˆ˜
    
    # í† í° ì§€í‘œ
    estimated_tokens: int           # ì¶”ì • í† í° ì‚¬ìš©ëŸ‰
    
    # CARC í’ˆì§ˆ ì§€í‘œ (ìë™ í‰ê°€)
    quality_completeness: Optional[int] = None  # ì™„ì „ì„± 1-5
    quality_accuracy: Optional[int] = None      # ì •í™•ì„± 1-5
    quality_relevance: Optional[int] = None     # ê´€ë ¨ì„± 1-5
    quality_clarity: Optional[int] = None       # ëª…í™•ì„± 1-5
    quality_total: Optional[int] = None         # ì´ì  4-20
    
    # ê¸°íƒ€ ì§€í‘œ
    has_citations: bool = False             # ì¸ìš© í¬í•¨ ì—¬ë¶€
    response_length: int = 0                # ì‘ë‹µ ê¸¸ì´ (ë¬¸ì)


def estimate_tokens(text: str) -> int:
    """ê°„ë‹¨í•œ í† í° ì¶”ì • (í•œê¸€ 2ì = 1í† í°, ì˜ì–´ 4ì = 1í† í°)"""
    korean_chars = sum(1 for c in text if 'ê°€' <= c <= 'í£')
    other_chars = len(text) - korean_chars
    return (korean_chars // 2) + (other_chars // 4)


def has_citations(text: str) -> bool:
    """ì¸ìš© í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    import re
    # [1], [2] ë˜ëŠ” [ì¶œì²˜] íŒ¨í„´ ê²€ìƒ‰
    return bool(re.search(r'\[\d+\]|\[ì¶œì²˜\]|### (Sources|ì¶œì²˜)', text))


class ResearchBenchmark:
    """ì—°êµ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ê¸°ë¡"""
    
    RESULTS_DIR = "benchmark_results"
    
    def __init__(self, phase: str, verbose: bool = False):
        self.phase = phase
        self.verbose = verbose
        self.results: List[ResearchMetrics] = []
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.RESULTS_DIR, exist_ok=True)
    
    def run_single(self, graph, query: str) -> ResearchMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        from langchain_core.messages import HumanMessage
        
        print(f"\nğŸ“Š Running benchmark: {query[:50]}...")
        
        start = time.time()
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        result = graph.invoke({
            "messages": [HumanMessage(content=query)]
        })
        
        elapsed = time.time() - start
        
        # ê²°ê³¼ ë¶„ì„
        messages = result.get("messages", [])
        final_response = messages[-1].content if messages else ""
        
        metrics = ResearchMetrics(
            phase=self.phase,
            query=query,
            timestamp=datetime.now().isoformat(),
            total_time_sec=round(elapsed, 2),
            llm_calls=self._count_ai_messages(messages),
            search_calls=result.get("current_query_index", 0),
            urls_read=len(result.get("read_contents", [])),
            research_iterations=result.get("research_iteration", 0),
            estimated_tokens=estimate_tokens(str(messages)),
            # CARC í’ˆì§ˆ ì ìˆ˜ (ìë™ í‰ê°€)
            quality_completeness=result.get("quality_completeness"),
            quality_accuracy=result.get("quality_accuracy"),
            quality_relevance=result.get("quality_relevance"),
            quality_clarity=result.get("quality_clarity"),
            quality_total=result.get("quality_total"),
            has_citations=has_citations(final_response),
            response_length=len(final_response)
        )
        
        self.results.append(metrics)
        self._print_metrics(metrics, final_response)
        
        return metrics
    
    def _count_ai_messages(self, messages) -> int:
        """AI ë©”ì‹œì§€ ìˆ˜ ì¹´ìš´íŠ¸"""
        count = 0
        for msg in messages:
            if hasattr(msg, 'name') or (hasattr(msg, 'type') and msg.type == 'ai'):
                count += 1
        return count
    
    def _print_metrics(self, m: ResearchMetrics, response: str = ""):
        """ì§€í‘œ ì¶œë ¥"""
        
        # CARC í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if m.quality_total:
            if m.quality_total >= 16:
                quality_grade = "âœ… Excellent"
            elif m.quality_total >= 12:
                quality_grade = "ğŸ‘ Good"
            else:
                quality_grade = "âš ï¸ Needs work"
            carc_line = f"â”‚ CARC Quality: C={m.quality_completeness} A={m.quality_accuracy} R={m.quality_relevance} C={m.quality_clarity} â†’ {m.quality_total}/20 {quality_grade}"
        else:
            carc_line = "â”‚ CARC Quality: N/A"
        
        print(f"""
â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ {m.phase} Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: {m.query[:60]}...
â”‚ Time: {m.total_time_sec}s
â”‚ LLM Calls: {m.llm_calls}
â”‚ Search Calls: {m.search_calls}
â”‚ URLs Read: {m.urls_read}
â”‚ Iterations: {m.research_iterations}
â”‚ Est. Tokens: {m.estimated_tokens}
â”‚ Response Length: {m.response_length} chars
â”‚ Has Citations: {'âœ…' if m.has_citations else 'âŒ'}
{carc_line}
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
""")
        
        # ì‘ë‹µ ë‚´ìš© ì¶œë ¥
        if self.verbose:
            # Verbose ëª¨ë“œ: ì „ì²´ ì‘ë‹µ ì¶œë ¥
            print("\n" + "="*70)
            print("ğŸ“„ FULL RESPONSE:")
            print("="*70)
            print(response)
            print("="*70 + "\n")
        else:
            # ê¸°ë³¸ ëª¨ë“œ: 500ì ë¯¸ë¦¬ë³´ê¸° (ì¤„ë°”ê¿ˆ ìœ ì§€)
            if response:
                preview = response[:500]
                if len(response) > 500:
                    preview += "\n... (truncated)\n"
                print("\nğŸ“ Response Preview (500 chars):")
                print("-"*50)
                print(preview)
                print("-"*50 + "\n")
    
    def run_all(self, graph, queries: List[str]) -> List[ResearchMetrics]:
        """ëª¨ë“  ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        for query in queries:
            self.run_single(graph, query)
        return self.results
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        filename = f"{self.RESULTS_DIR}/{self.phase.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        data = [asdict(r) for r in self.results]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ Results saved to: {filename}")
        return filename
    
    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        if not self.results:
            print("No results yet")
            return
        
        avg_time = sum(r.total_time_sec for r in self.results) / len(self.results)
        avg_tokens = sum(r.estimated_tokens for r in self.results) / len(self.results)
        citations_rate = sum(1 for r in self.results if r.has_citations) / len(self.results) * 100
        
        # CARC í‰ê·  ê³„ì‚°
        carc_results = [r for r in self.results if r.quality_total is not None]
        if carc_results:
            avg_carc = sum(r.quality_total for r in carc_results) / len(carc_results)
            avg_c = sum(r.quality_completeness for r in carc_results) / len(carc_results)
            avg_a = sum(r.quality_accuracy for r in carc_results) / len(carc_results)
            avg_r = sum(r.quality_relevance for r in carc_results) / len(carc_results)
            avg_cl = sum(r.quality_clarity for r in carc_results) / len(carc_results)
            carc_line = f"â•‘  CARC Quality: C={avg_c:.1f} A={avg_a:.1f} R={avg_r:.1f} C={avg_cl:.1f} â†’ {avg_carc:.1f}/20"
        else:
            carc_line = "â•‘  CARC Quality: N/A"
        
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  {self.phase} Summary ({len(self.results)} tests)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Average Time: {avg_time:.2f}s
â•‘  Average Tokens: {avg_tokens:.0f}
â•‘  Citation Rate: {citations_rate:.0f}%
{carc_line}
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")


# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸
TEST_QUERIES = [
    "LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ì¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜",
    "2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜",
    "RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ ê°ê° ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ì§€ ì„¤ëª…í•´ì¤˜"
]


def run_phase_benchmark(graph, phase: str = "Phase 0", verbose: bool = False):
    """Phase ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    Args:
        graph: LangGraph ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤
        phase: Phase ì´ë¦„ (ì˜ˆ: "Phase 0", "Phase 1")
        verbose: Trueë©´ ì „ì²´ ì‘ë‹µ ì¶œë ¥, Falseë©´ 500ì ë¯¸ë¦¬ë³´ê¸°
    """
    benchmark = ResearchBenchmark(phase, verbose=verbose)
    benchmark.run_all(graph, TEST_QUERIES)
    benchmark.print_summary()
    benchmark.save_results()
    return benchmark.results
