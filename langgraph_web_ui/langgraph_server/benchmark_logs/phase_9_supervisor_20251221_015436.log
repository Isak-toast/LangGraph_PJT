/home/isak/LangGraph_PJT/langgraph_web_ui/langgraph_server/src/agent/tools.py:36: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.
  tavily_tool = TavilySearchResults(

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Deep Research Benchmark - Phase 9 Supervisor
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Test Queries: 3
â•‘  Verbose: ON (full response)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ðŸ“Š Running benchmark: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜...

ðŸ”Ž Clarify: Analyzing query...
   â””â”€ Query: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜
   â””â”€ Status: ðŸŸ¢ Clear
   â””â”€ Analysis: The user is asking for a comparison between LangGraph and CrewAI in terms of multi-agent architecture, including their pros and cons. The query is clear about the intent, which is a comparative analysis.
   â””â”€ Topics: LangGraph, CrewAI, Multi-agent architecture
ðŸ“‹ Planner: Creating research plan for: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜

ðŸ“‹ Planner: Generated 8 queries
   â””â”€ Queries:
      [1] LangGraph vs CrewAI multi-agent architecture
      [2] LangGraph multi-agent system
      [3] CrewAI multi-agent system
      [4] LangGraph advantages and disadvantages
      [5] CrewAI advantages and disadvantages
      [6] Comparing LangGraph and CrewAI
      [7] LangGraph architecture
      [8] CrewAI architecture
   â””â”€ Focus: Architecture comparison, Advantages of LangGraph, Disadvantages of LangGraph, Advantages of CrewAI, Disadvantages of CrewAI

ðŸŽ¯ Supervisor [Phase 9]: Analyzing query complexity...
   â””â”€ Query: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜
   â””â”€ Planned queries: 8
   â””â”€ Complexity: ðŸŸ¡ MEDIUM
   â””â”€ Recommended: 2 iterations, depth 2
   â””â”€ Strategy: targeted
   â””â”€ Reasoning: The query requires comparing two frameworks, assessing their strengths and weaknesses. This involves more than simple fact retrieval, suggesting a medium complexity. Two iterations with a medium depth should be sufficient.

ðŸ”¬ Research Subgraph: Starting research loop...
   â””â”€ Supervisor config: MEDIUM, max 2 iterations, targeted strategy
ðŸ” Searcher [1]: Searching for: LangGraph vs CrewAI multi-agent architecture

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.truefoundry.com/blog/crewai-vs-langgraph
      [2] https://www.zenml.io/blog/langgraph-vs-crewai
      [3] https://www.zams.com/blog/crewai-vs-langgraph
      [4] https://blog.langchain.com/langgraph-multi-agent-workflows/
      [5] https://developer.ibm.com/articles/awb-comparing-ai-agent-frameworks-crewai-langgraph-and-beeai/
   â””â”€ Snippets:
      â€¢ While CrewAI focuses on collaborative agent teamsâ€”where each agent has a specific role, goal, and communication strategyâ€”LangGraph provides a graph-based workflow engine designed for building structured, resilient LLM applications. Both aim to simplify multi-agent development but approach the problem from different angles: CrewAI emphasizes team coordination, whereas LangGraph emphasizes stateful, production-ready execution. [...] CrewAI and LangGraph both bring powerful capabilities to AI application development, but they excel in different areas. CrewAI is ideal for structured, role-based multi-agent collaboration, while LangGraph is built for adaptive, stateful workflows that can branch, loop, and respond to changing inputs. Your choice depends on your projectâ€™s nature. If you need a coordinated team of specialized agents, CrewAI is a natural fit. If your focus is on flexible execution with robust state management, [...] When you build with CrewAI or LangGraph, you are working with powerful frameworks for orchestrating AI agents. CrewAI excels in structuring multi-agent collaboration, while LangGraph shines in managing complex, adaptive workflows. But once these systems move from development to production, the challenges shift. You need to ensure they run securely, efficiently, and with complete operational visibility. Thatâ€™s where TrueFoundry AI Gateway becomes the perfect companion.
      â€¢ What's the learning curve difference between these frameworks? CrewAI offers faster initial setup with its role-based, YAML-configurable approachâ€”teams can build working multi-agent systems in hours. LangGraph requires deeper understanding of graph structures, state management, and functional composition, typically taking days to weeks to master. However, LangGraph's complexity pays dividends in complex workflows requiring precise control, conditional routing, and advanced debugging. [...] Complex stateful workflows with branching logic: Choose LangGraph for its graph-based architecture, conditional routing, and time-travel debugging capabilities that handle non-linear agent interactions.  Rapid prototyping and POC development: Choose CrewAI for its intuitive role-based model and YAML configuration that enables working multi-agent systems in hours rather than days. [...] ## LangGraph vs CrewAI: Key Takeaways  ðŸ§‘â€ðŸ’» LangGraph: Itâ€™s a framework from LangChain that helps you build stateful, multi-agent applications as graphs. LangGraph provides low-level control over agent workflows with built-in persistence, streaming support, and the ability to create complex branching logic.
      â€¢ ## Final verdict  To sum up, Crew and Langgraph are both powerful frameworks, each with its unique strengths and weaknesses. Choosing the right framework ultimately depends on your specific needs and expertise.  For beginners or those looking for a simple multi-agent orchestration framework Crew is the way to go. Its user friendly and modular design makes it easy to get started. [...] While Crewai offers a beginner-friendly and is easy-to-use, it is limited in flexibility. On the other hand, LangGraph offers great control and flexibility but is not easy to quickly set up and get going.  > Both these platforms are primarily for software development teams.  When engineering teams want to research and quickly prototype, they go for Crew. And during production, they prefer LangGraph to develop agents for complex and detailed workflows. [...] LangGraph is an agent framework that takes a state-centric approach, providing a graph-based architecture to enable creation of complex workflows and applications. This approach is ideal for applications requiring sophisticated state management, knowledge retrieval, and distributed systems.  ## How to choose between Crewai and LangGraph?  Both are multi agent frameworks and they provide plenty of resources to get you started including tutorials, documentation, pre-built agents and tools.
ðŸ’­ Think: Query: LangGraph vs CrewAI multi-agent architecture | Found 5 results, 5 URLs. Key snippets: While CrewAI focuses on collaborative agent teamsâ€”where each agent has a specific role, goal, and co | What's the learning curve difference between these frameworks? CrewAI offers faster initial setup wi | ## Final verdict

To sum up, Crew and Langgraph are both powerful frameworks, each with its un. Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://www.truefoundry.com/blog/crewai-vs-langgra... (8015 chars)
ðŸ“– Read URL: https://www.zams.com/blog/crewai-vs-langgraph... (8015 chars)
ðŸ“– Read URL: https://www.zenml.io/blog/langgraph-vs-crewai... (8015 chars)
   âœ“ [https://www.truefoundry.com/blog/crewai-vs-langgraph]
      Preview: Crewai vs LangGraph: Know The Differences --> --> Join the AI Security Webinar with Palo Alto. Register here Product AI INFRA LLMOps Model Serving SECURE &amp; GOVERN AI Gateway MCP Gateway New! Product AI INFRA LLMOps Model Serving SECURE &amp; GOVERN AI Gateway MCP Gateway New! Why TrueFoundry CUSTOMERS Case Studies Wall of Love G2 Reviews DISCOVER For DS Leaders For IT Leaders For AI/ML Leaders Elevate for Enterprises What&#x27;s New Open Source Resources Newsletter Resource Center Trust Center Events &amp; Conferences Webinars Compare Newsletter Truefoundry vs Sagemaker Truefoundry vs Databricks Truefoundry vs Portkey Solutions BY INDUSTRY Banking and Investments Media and Communication Education Healthcare and Life Sciences Power and Utilities Insurance Retail Government Technology Oil and Gas BY FUNCTION For DS Leaders For IT Leaders Application Suites Customer Support and CRM Finance IT Operations Marketing Sales and Lead Management Security and Compliance HR and Recruiting Digital Workplace Why TrueFoundry CUSTOMERS Case Studies Wall of Love G2 Reviews DISCOVER For DS Leaders For IT Leaders For AI/ML Leaders Elevate for Enterprises What&#x27;s New Careers resources Newsletter Resource Center Trust Center Events &amp; Conferences Webinars Trust Center Compare Newsletter Truefoundry vs Sagemaker Truefoundry vs Databricks Truefoundry vs Portkey Solutions BY INDUSTRY Banking and Investments Media and Communication Education Healthcare and Life Sciences Power and Utilities CUSTOMERS For DS Leaders For IT Leaders Insurance Retail Government Technology Oil and Gas BY FUNCTION Newsletter Application Suites Customer Support and CRM Finance IT Operations Marketing Trust Center Compare Newsletter Sales and Lead Management Security and Compliance HR and Recruiting Digital Workplace Pricing Docs Blog Live Demo Start Building OPEN SOURCE Elasti Cognita LLM Benchmarking EXPLORE Live Demo Accelerators Start Building OPEN SOURCE Elasti Cognita LLM Benchmarking EXPLORE Live Demo Accelerators Login Book Demo Sign Up Login Sign Up Book Demo comparison Crewai vs LangGraph: Know The Differences August 21, 2025 | 9:30 min read TrueFoundry SHARE The rise of multi-agent AI systems has created a need for frameworks that go beyond simple prompt chaining. Developers now want tools that can orchestrate multiple agents, manage shared state, and support complex workflows with branching, looping, and retries. Two notable frameworks leading this space are CrewAI and LangGraph. While CrewAI focuses on collaborative agent teamsâ€”where each agent has a specific role, goal, and communication strategyâ€”LangGraph provides a graph-based workflow engine designed for building structured, resilient LLM applications. Both aim to simplify multi-agent development but approach the problem from different angles: CrewAI emphasizes team coordination, whereas LangGraph emphasizes stateful, production-ready execution. In this comparison, weâ€™ll break down their core philosophies, features, and use cases to help you decide which framework better fits your AI development needs. What Is CrewAI? CrewAI is an open-source, Python-based framework designed for orchestrating autonomous, collaborative AI agents, much like a digital team handling complex tasks. Each agent operates with a defined role, such as researcher, writer, or analyst, and works together within a structured crew to solve problems efficiently. CrewAI combines modularity with performance, offering both high-level simplicity and precise control over how agents interact. Through components like Crews and Flows, it supports dynamic collaboration while giving developers the ability to manage control flows, tasks, and environments with flexibility. Agents in CrewAI are set up with defined roles, goals, tools, and even personality through backstories. This is similar to how a human team organizes itself to divide work and minimize errors. The framework allows agents to work sequentially or in parallel, with coordination
   âœ“ [https://www.zams.com/blog/crewai-vs-langgraph]
      Preview: Crewai vs. LangGraph: Multi agent framework comparison | Zams Integrations Customers Pricing Blog Login Login Get Started Get Started Technology April 19, 2025 Crewai vs. LangGraph: Which multi agent framework should you use? Yaagneshwaran Ganesh Objective feature comparison to help you decide - based on features, benefits, and ideal use cases. While there are different ways to build an AI agent from scratch, itâ€™s great that you are taking the efficient approach of using multi agent frameworks. Youâ€™re probably here because youâ€™ve shortlisted Crew ai and LangGraph and want to decide which one is right for you. Youâ€™re in the right place. In this blog, we will compare the two in detail - on their features, benefits, and ideal use cases, including how agents connect to establish communication and interoperability. To get started, we need to have a basic understanding of multi agent systems. So, letâ€™s first get a few basics out of the way. Why do you need a multi-agent framework? As your AI systems scale and you add multiple agents with different capabilities, the complexity of these applications grow. As the complexity grows, you will need a structured environment that orchestrates the agent activities, including the technical steps and requirements involved in building agents. That is where agentic frameworks come in. Multi agent frameworks provide you with a foundational structure for developing autonomous systems, and define parameters and protocols to handle interactions between multiple specialized agents. These frameworks also incorporate agent actions, which are fundamental components within a node-based AI framework, facilitating the execution of complex tasks. An agentic application can significantly enhance user experience and system efficiency by streamlining user interaction through minimal input and adaptive responses. Single agent systems These systems are autonomous but rely on one agent to handle a wide range of tasks, like a jack of all trades. For example, hereâ€™s how it works when requested for a sales pipeline report: As you can see, one agent carries out a series of tasks to accomplish the requested outcome. Single agent systems are great for specialized tasks where the problem is well defined and the scope is limited. But as your environment and context evolves, they fall short. Multi agent systems Multi agent systems, on the contrary, consist of multiple AI agents working together to achieve common goals. Letâ€™s look at the same example of requesting to email the sales report, and see how the multi agent architecture manages specialized agents to execute it. Instead of one agent accomplishing all the tasks, the tasks are broken down into smaller components where each agent specializes in a specific task â€“ such as planning, integrating, analyzing, and more like a team of specialists working together, where each brings their unique expertise to the table. And because these AI agents can collaborate dynamically and run these tasks in parallel, they can tackle more complex problems where the environment is always changing and evolving. Multi agent interactions allow these AI agents to communicate within the system, monitor and debug in real-time, and handle handoffs efficiently, enabling dynamic workflows. The better you understand how these multi agents interact, the better you can optimize these systems and scale your operations, without worrying about bottlenecks or performance issues. And thatâ€™s exactly what multi agent frameworks help you with. Additionally, these frameworks give you a set of pre-packaged tools and features to help you quickly build any type of agent systems, be it knowledge oriented, process oriented or predictive. In short, agentic frameworks are the backbone of scalable, efficient and autonomous AI operations. With that said letâ€™s get to the comparison. What is Crewai? It is an open-source multi agent orchestration framework, that helps you build multi agent systems, and integrate them wit
   âœ“ [https://www.zenml.io/blog/langgraph-vs-crewai]
      Preview: LangGraph vs CrewAI: Letâ€™s Learn About the Differences - ZenML Blog Product DATA SCience Iterate at warp speed Accelerate your ML workflow seamlessly Auto-track everything Automatic logging and versioning Shared ML building blocks Boost team productivity with reusable components Infrastructure Backend flexibility, zero lock-in One framework for all your MLOps and LLMOps needs Limitless scaling Effortlessly deploy across clouds Streamline cloud expenses Gain clarity on resource usage and costs Organization ZenML Pro Our managed control plane for MLOps Open Source vs Pro Pick what works for your needs ZenML vs Other Tools Compare ZenML to other ML tools Solutions GENAI &amp; LLMS Finetuning LLMs Customize large language models for specific tasks Productionalizing a RAG application Deploy and scale RAG systems LLMOps Database A curated knowledge base of real-world implementations mlops Building Enterprise MLOps Platform architecture and best practices Abstract cloud compute Simplify management of cloud-based ML resources Track metrics and metadata Monitor and analyze ML model performance and data Success Stories JetBrains Software Adeo Leroy Merlin Retail Cross Screen Media Media View All Case Studies Learn more Developers Documentation Docs Comprehensive guides to use ZenML Deploying ZenML Understanding ZenML system architecture Tutorials Examples showing ZenML in action GUIDES Quickstart Quickly get your hands dirty Showcase Projects of ML use cases built with ZenML Starter Guide Get started with the basics COMMUNITY Slack Join our Slack Community Changelog Discover whatâ€™s new on ZenML Roadmap Join us on our MLOps journey Pricing Blog Case Studies Get Started Book a demo Software Engineering LangGraph vs CrewAI: Letâ€™s Learn About the Differences Hamza Tahir Jun 28, 2025 â€¢ 12 mins All posts LLMOps Contents Get started with ZenML today Begin with open source tools Works with any infrastructure Secure, metadata-only tracking Book a demo Related Posts Metaflow vs MLflow vs ZenML: Whatâ€™s the Difference? ZenML&#x27;s MCP Server Supports DXT: Making MLOps Conversations Frictionless This is also a heading This is a heading LangGraph and CrewAI are modern frameworks for orchestrating complex AI workflows with multiple LLM-driven agents. Both these intelligent systems are capable of sophisticated reasoning, planning, and autonomous action, and are becoming central to modern AI applications. However, they differ in abstraction, interfaces, and enterprise features. This LangGraph vs CrewAI article compares key attributes of these platforms, like: Workflow patterns Human-in-loop capabilities Parallelism and throttling Compliance and security Integration options Pricing We do this so you can exactly know when to use which one of these platforms. Recently Updated (November 2025) : This comparison has been refreshed with major 2025 developments including LangGraph 1.0&#x27;s stable release (October 2025), CrewAI&#x27;s multimodal support and agentic RAG capabilities, updated market adoption statistics showing 85% of organizations now using AI agents, and the emergence of new interoperability protocols like A2A and MCP. All framework comparisons and integration information reflect current capabilities as of November 2025. LangGraph vs CrewAI: Key Takeaways ðŸ§‘â€ðŸ’» LangGraph : Itâ€™s a framework from LangChain that helps you build stateful, multi-agent applications as graphs. LangGraph provides low-level control over agent workflows with built-in persistence, streaming support, and the ability to create complex branching logic. ðŸ§‘â€ðŸ’» CrewAI : Itâ€™s a high-level framework for orchestrating autonomous AI agents working together as a crew. The platform abstracts away complexity by providing pre-built patterns for agent collaboration, role assignment, and task delegation. Framework Maturity &amp; Lineage The table below compared the framework maturity of LangGraph and CrewAI: Metric CrewAI LangGraph First public release v0.1.0 â€” 14 Nov 2023 v0.0.9 â€” 8 Jan 20
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 0.18s
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] CrewAI is designed for collaborative agent teams with specific roles and goals, emphasizing team coordination.
      [2] LangGraph uses a graph-based workflow for structured, resilient LLM applications, emphasizing stateful execution.
      [3] CrewAI offers a faster initial setup and is easier for beginners, allowing teams to build multi-agent systems quickly.
      [4] LangGraph has a steeper learning curve, requiring a deeper understanding of graph structures and state management, but offers more control and flexibility.
      [5] LangGraph is integrated into the LangChain ecosystem, providing access to LangChain integrations and LangSmith observability.
   â””â”€ Decision: Research complete (7 total findings)
   â””â”€ âœ… Research Subgraph completed (execution #1)
   â””â”€ Findings: 7 items
   â””â”€ Contents: 3 URLs read

ðŸ“¦ Compress: Compressing 7 findings, 3 contents
   â””â”€ Compressed to 1921 chars (from ~3733 raw chars)
   â””â”€ Sources cited: 3
   â””â”€ Preview:
      ## Key Findings
      - CrewAI is designed for collaborative agent teams, emphasizing team coordination with specific roles and goals [1], [3].
      - LangGraph employs a graph-based workflow for structured, resilient LLM applications, focusing on stateful execution [2].
      - CrewAI offers a faster setup and is easier for beginners, enabling quick multi-agent system development [2].
      - LangGraph has a steeper learning curve but provides more control and flexibility [2].
      - The choice between CrewAI and LangGraph depends on project complexity and user expertise [2].
      
      ## Detailed Information
      
      CrewAI and LangGraph are both multi-agent frameworks, but they cater to different needs and expertise levels [2], [3]. CrewAI is designed for collaborative agent teams, emphasizing team coordination with specific roles and goals [1], [3]. Its architecture is modular, separating agent creation, task definition, and process orchestration for customization [1]. This makes CrewAI faster to set up and easier for beginners, allowing teams to quickly build multi-agent systems [2].
      
      LangGraph, on the other hand, uses a graph-based workflow for structured, resilient LLM applications, focusing on stateful execution [2]. It has a steeper learning curve, requiring a deeper understanding of graph structures and state management [2]. However, this complexity offers more control and flexibility [2]. LangGraph is integrated into the LangChain ecosystem, providing access to LangChain integrations and LangSmith observability [1].
      
      Ultimately, the choice between CrewAI and LangGraph depends on specific needs and expertise [2]. CrewAI is better for beginners and simpler projects, while LangGraph is suited for complex workflows requiring greater control and resilience [2].
      
      ## Sources
      [1] https://www.zenml.io/blog/langgraph-vs-crewai
      [2] https://www.truefoundry.com/blog/crewai-vs-langgraph
      [3] https://www.zams.com/blog/crewai-vs-langgraph

âœï¸ Writer: Composing response from 7 findings
   â””â”€ Using compressed notes (1921 chars)

âœï¸ Writer: Generated response (2480 chars)
   â””â”€ Sources used:
      [1] https://www.zenml.io/blog/langgraph-vs-crewai
      [2] https://www.truefoundry.com/blog/crewai-vs-langgraph
      [3] https://www.zams.com/blog/crewai-vs-langgraph
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      CrewAIì™€ LangGraphëŠ” ëª¨ë‘ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, CrewAIëŠ” íŒ€ í˜‘ì—…ì—, LangGraphëŠ” ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì— ë” ì í•©í•˜ë©° ì‚¬ìš©ìžì˜ ìˆ™ë ¨ë„ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§„ë‹¤ [2]. CrewAIëŠ” ë¹ ë¥¸ ì„¤ì •ê³¼ ì‰¬ìš´ ì‚¬ìš©ë²•ì„, LangGraphëŠ” ë” ë†’ì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2].
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      *   CrewAIëŠ” íŠ¹ì • ì—­í• ê³¼ ëª©í‘œë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ íŒ€ì˜ í˜‘ì—…ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, íŒ€ ì¡°ì •ì„ ê°•ì¡°í•œë‹¤ [1], [3].
      *   LangGraphëŠ” êµ¬ì¡°í™”ë˜ê³  íƒ„ë ¥ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ê·¸ëž˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ë©°, ìƒíƒœ ì €ìž¥ ì‹¤í–‰ì— ì¤‘ì ì„ ë‘”ë‹¤ [2].
      *   CrewAIëŠ” ë¹ ë¥¸ ì„¤ì •ì„ ì œê³µí•˜ë©° ì´ˆë³´ìžì—ê²Œ ë” ì‰¬ì›Œ ë¹ ë¥¸ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê°œë°œì´ ê°€ëŠ¥í•˜ë‹¤ [2].
      *   LangGraphëŠ” í•™ìŠµ ê³¡ì„ ì´ ë” ê°€íŒŒë¥´ì§€ë§Œ ë” ë§Žì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2].
      *   CrewAIì™€ LangGraph ì¤‘ ì„ íƒì€ í”„ë¡œì íŠ¸ ë³µìž¡ì„±ê³¼ ì‚¬ìš©ìž ì „ë¬¸ì„±ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ [2].
      
      ## ìƒì„¸ ë¶„ì„
      
      CrewAIì™€ LangGraphëŠ” ëª¨ë‘ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, ì‚¬ìš© ëª©ì ê³¼ ì‚¬ìš©ìž ìˆ˜ì¤€ì— ë”°ë¼ ë‹¤ë¥¸ íŠ¹ì§•ì„ ë³´ì¸ë‹¤ [2], [3]. CrewAIëŠ” í˜‘ì—… ì—ì´ì „íŠ¸ íŒ€ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, íŠ¹ì • ì—­í• ê³¼ ëª©í‘œë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ ê°„ì˜ íŒ€ì›Œí¬ë¥¼ ê°•ì¡°í•œë‹¤ [1], [3].  CrewAIì˜ ì•„í‚¤í…ì²˜ëŠ” ëª¨ë“ˆì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ì—ì´ì „íŠ¸ ìƒì„±, ìž‘ì—… ì •ì˜, í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©ìž ì •ì˜ê°€ ê°€ëŠ¥í•˜ë‹¤ [1]. ì´ëŸ¬í•œ íŠ¹ì§• ë•ë¶„ì— CrewAIëŠ” ì„¤ì •ì´ ë¹ ë¥´ê³  ì´ˆë³´ìžê°€ ì‚¬ìš©í•˜ê¸° ì‰¬ì›Œ ë¹ ë¥´ê²Œ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë‹¤ [2].  ì´ëŠ” í”„ë¡œí† íƒ€ìž… ì œìž‘ì´ë‚˜ ê°„ë‹¨í•œ í˜‘ì—… ì‹œë‚˜ë¦¬ì˜¤ì— ìœ ìš©í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë§ˆì¼€íŒ… íŒ€ì—ì„œ ì½˜í…ì¸  ìƒì„±, SEO ë¶„ì„, ì†Œì…œ ë¯¸ë””ì–´ ë°°í¬ë¥¼ ìœ„í•´ ê°ê¸° ë‹¤ë¥¸ ì—­í• ì„ ê°€ì§„ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•˜ì—¬ í˜‘ì—…í•˜ê²Œ í•  ìˆ˜ ìžˆë‹¤.
      
      ë°˜ë©´ LangGraphëŠ” êµ¬ì¡°í™”ë˜ê³  ë³µì›ë ¥ì´ ë›°ì–´ë‚œ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ê·¸ëž˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ë©°, ìƒíƒœ ì €ìž¥ ì‹¤í–‰ì— ì¤‘ì ì„ ë‘”ë‹¤ [2]. LangGraphëŠ” ê·¸ëž˜í”„ êµ¬ì¡°ì™€ ìƒíƒœ ê´€ë¦¬ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•˜ë¯€ë¡œ í•™ìŠµ ê³¡ì„ ì´ ë” ê°€íŒŒë¥´ë‹¤ [2]. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë³µìž¡ì„± ë•ë¶„ì— ë” ë§Žì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2]. LangGraphëŠ” LangChain ìƒíƒœê³„ì— í†µí•©ë˜ì–´ LangChain í†µí•© ê¸°ëŠ¥ê³¼ LangSmith ê´€ì°° ê¸°ëŠ¥ì„ ì´ìš©í•  ìˆ˜ ìžˆë‹¤ [1].  LangGraphì˜ ìƒíƒœ ì €ìž¥ ê¸°ëŠ¥ì€ ë³µìž¡í•œ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° íŠ¹ížˆ ìœ ìš©í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ì§€ì› ì±—ë´‡ì—ì„œ ì‚¬ìš©ìžì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìžˆë‹¤.
      
      CrewAIëŠ” ëª…í™•í•œ ì—­í•  ë¶„ë‹´ê³¼ ê°„ë‹¨í•œ ìž‘ì—… íë¦„ì— ì í•©í•˜ë©°, LangGraphëŠ” ë³µìž¡í•œ ì˜ì¡´ ê´€ê³„ì™€ ìƒíƒœ ê´€ë¦¬ê°€ í•„ìš”í•œ ìž‘ì—…ì— ë” ì í•©í•˜ë‹¤. CrewAIëŠ” 'ë¹ ë¥¸ ì‹œìž‘'ì„ ì§€í–¥í•˜ë©°, LangGraphëŠ” 'ì„¸ë°€í•œ ì œì–´'ë¥¼ ì§€í–¥í•œë‹¤ê³  ë³¼ ìˆ˜ ìžˆë‹¤.
      
      ê¶ê·¹ì ìœ¼ë¡œ CrewAIì™€ LangGraph ì¤‘ ì–´ë–¤ ê²ƒì„ ì„ íƒí• ì§€ëŠ” íŠ¹ì • ìš”êµ¬ ì‚¬í•­ê³¼ ì „ë¬¸ì„±ì— ë”°ë¼ ê²°ì •ëœë‹¤ [2]. CrewAIëŠ” ì´ˆë³´ìžì™€ ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ì— ì í•©í•˜ê³ , LangGraphëŠ” ë” ë§Žì€ ì œì–´ë ¥ê³¼ ë³µì›ë ¥ì´ í•„ìš”í•œ ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš°ì— ì í•©í•˜ë‹¤ [2]. í”„ë¡œì íŠ¸ì˜ ë³µìž¡ì„±, í•„ìš”í•œ ì œì–´ ìˆ˜ì¤€, íŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì í•©í•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ë¼ë©´ CrewAIê°€ ì í•©í•  ìˆ˜ ìžˆìœ¼ë©°, ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ìž¥í•´ì•¼ í•˜ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì´ë¼ë©´ LangGraphê°€ ë” ë‚˜ì€ ì„ íƒì¼ ìˆ˜ ìžˆë‹¤.
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      *   [1] https://www.zenml.io/blog/langgraph-vs-crewai
      *   [2] https://www.truefoundry.com/blog/crewai-vs-langgraph
      *   [3] https://www.zams.com/blog/crewai-vs-langgraph
      
      ## ê²°ë¡  ë° í‰ê°€
      
      CrewAIì™€ LangGraphëŠ” ê°ê° ëšœë ·í•œ ìž¥ë‹¨ì ì„ ê°€ì§„ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ë‹¤. CrewAIëŠ” ì ‘ê·¼ì„±ì´ ë›°ì–´ë‚˜ê³  ë¹ ë¥¸ ê°œë°œì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì´ ì œí•œì ì´ë‹¤. LangGraphëŠ” ë” ë†’ì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•˜ì§€ë§Œ, í•™ìŠµ ê³¡ì„ ì´ ê°€íŒŒë¥´ê³  ë³µìž¡í•˜ë‹¤. ë”°ë¼ì„œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ ì‚¬í•­ê³¼ ê°œë°œíŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ˆë³´ìž ë˜ëŠ” ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ì—ëŠ” CrewAIë¥¼, ë³µìž¡í•˜ê³  ì œì–´ë ¥ì´ í•„ìš”í•œ í”„ë¡œì íŠ¸ì—ëŠ” LangGraphë¥¼ ì¶”ì²œí•œë‹¤. ë˜í•œ, ë‘ í”„ë ˆìž„ì›Œí¬ì˜ ê°•ì ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, CrewAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ ê°œë°œí•˜ê³ , LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤. ì•žìœ¼ë¡œ ë‘ í”„ë ˆìž„ì›Œí¬ ëª¨ë‘ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ ì£¼ì‹œí•˜ê³  í”„ë¡œì íŠ¸ì— ì í•©í•œ ìµœì ì˜ ì„ íƒì„ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

ðŸ” Critique: CARC Quality Evaluation...
   â””â”€ CARC Scores: C=4.0 A=4.0 R=4.5 C=4.0
   â””â”€ Total: 16.5/20 âœ… Excellent
   â””â”€ Feedback: The response provides a good comparison of LangGraph and CrewAI, covering their key features, strengths, and weaknesses. It appropriately highlights the trade-offs between ease of use and control. However, it could benefit from more concrete examples illustrating specific use cases and a deeper dive into the architectural differences beyond the stated high-level characteristics. Some of the content is repetitive, such as restating the core differences in the intro, body, and conclusion.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 9 Supervisor Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜...
â”‚ Time: 20.27s
â”‚ LLM Calls: 2
â”‚ Search Calls: 0
â”‚ URLs Read: 3
â”‚ Iterations: 1
â”‚ Est. Tokens: 1034
â”‚ Response Length: 2480 chars
â”‚ Has Citations: âœ…
â”‚ CARC Quality: C=4.0 A=4.0 R=4.5 C=4.0 â†’ 16.5/20 ðŸ‘ Good
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

CrewAIì™€ LangGraphëŠ” ëª¨ë‘ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, CrewAIëŠ” íŒ€ í˜‘ì—…ì—, LangGraphëŠ” ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì— ë” ì í•©í•˜ë©° ì‚¬ìš©ìžì˜ ìˆ™ë ¨ë„ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§„ë‹¤ [2]. CrewAIëŠ” ë¹ ë¥¸ ì„¤ì •ê³¼ ì‰¬ìš´ ì‚¬ìš©ë²•ì„, LangGraphëŠ” ë” ë†’ì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2].

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

*   CrewAIëŠ” íŠ¹ì • ì—­í• ê³¼ ëª©í‘œë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ íŒ€ì˜ í˜‘ì—…ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, íŒ€ ì¡°ì •ì„ ê°•ì¡°í•œë‹¤ [1], [3].
*   LangGraphëŠ” êµ¬ì¡°í™”ë˜ê³  íƒ„ë ¥ì ì¸ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ê·¸ëž˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ë©°, ìƒíƒœ ì €ìž¥ ì‹¤í–‰ì— ì¤‘ì ì„ ë‘”ë‹¤ [2].
*   CrewAIëŠ” ë¹ ë¥¸ ì„¤ì •ì„ ì œê³µí•˜ë©° ì´ˆë³´ìžì—ê²Œ ë” ì‰¬ì›Œ ë¹ ë¥¸ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê°œë°œì´ ê°€ëŠ¥í•˜ë‹¤ [2].
*   LangGraphëŠ” í•™ìŠµ ê³¡ì„ ì´ ë” ê°€íŒŒë¥´ì§€ë§Œ ë” ë§Žì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2].
*   CrewAIì™€ LangGraph ì¤‘ ì„ íƒì€ í”„ë¡œì íŠ¸ ë³µìž¡ì„±ê³¼ ì‚¬ìš©ìž ì „ë¬¸ì„±ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤ [2].

## ìƒì„¸ ë¶„ì„

CrewAIì™€ LangGraphëŠ” ëª¨ë‘ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, ì‚¬ìš© ëª©ì ê³¼ ì‚¬ìš©ìž ìˆ˜ì¤€ì— ë”°ë¼ ë‹¤ë¥¸ íŠ¹ì§•ì„ ë³´ì¸ë‹¤ [2], [3]. CrewAIëŠ” í˜‘ì—… ì—ì´ì „íŠ¸ íŒ€ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, íŠ¹ì • ì—­í• ê³¼ ëª©í‘œë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ ê°„ì˜ íŒ€ì›Œí¬ë¥¼ ê°•ì¡°í•œë‹¤ [1], [3].  CrewAIì˜ ì•„í‚¤í…ì²˜ëŠ” ëª¨ë“ˆì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ì—ì´ì „íŠ¸ ìƒì„±, ìž‘ì—… ì •ì˜, í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë¶„ë¦¬í•˜ì—¬ ì‚¬ìš©ìž ì •ì˜ê°€ ê°€ëŠ¥í•˜ë‹¤ [1]. ì´ëŸ¬í•œ íŠ¹ì§• ë•ë¶„ì— CrewAIëŠ” ì„¤ì •ì´ ë¹ ë¥´ê³  ì´ˆë³´ìžê°€ ì‚¬ìš©í•˜ê¸° ì‰¬ì›Œ ë¹ ë¥´ê²Œ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë‹¤ [2].  ì´ëŠ” í”„ë¡œí† íƒ€ìž… ì œìž‘ì´ë‚˜ ê°„ë‹¨í•œ í˜‘ì—… ì‹œë‚˜ë¦¬ì˜¤ì— ìœ ìš©í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë§ˆì¼€íŒ… íŒ€ì—ì„œ ì½˜í…ì¸  ìƒì„±, SEO ë¶„ì„, ì†Œì…œ ë¯¸ë””ì–´ ë°°í¬ë¥¼ ìœ„í•´ ê°ê¸° ë‹¤ë¥¸ ì—­í• ì„ ê°€ì§„ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•˜ì—¬ í˜‘ì—…í•˜ê²Œ í•  ìˆ˜ ìžˆë‹¤.

ë°˜ë©´ LangGraphëŠ” êµ¬ì¡°í™”ë˜ê³  ë³µì›ë ¥ì´ ë›°ì–´ë‚œ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ê·¸ëž˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ë©°, ìƒíƒœ ì €ìž¥ ì‹¤í–‰ì— ì¤‘ì ì„ ë‘”ë‹¤ [2]. LangGraphëŠ” ê·¸ëž˜í”„ êµ¬ì¡°ì™€ ìƒíƒœ ê´€ë¦¬ì— ëŒ€í•œ ê¹Šì€ ì´í•´ê°€ í•„ìš”í•˜ë¯€ë¡œ í•™ìŠµ ê³¡ì„ ì´ ë” ê°€íŒŒë¥´ë‹¤ [2]. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë³µìž¡ì„± ë•ë¶„ì— ë” ë§Žì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•œë‹¤ [2]. LangGraphëŠ” LangChain ìƒíƒœê³„ì— í†µí•©ë˜ì–´ LangChain í†µí•© ê¸°ëŠ¥ê³¼ LangSmith ê´€ì°° ê¸°ëŠ¥ì„ ì´ìš©í•  ìˆ˜ ìžˆë‹¤ [1].  LangGraphì˜ ìƒíƒœ ì €ìž¥ ê¸°ëŠ¥ì€ ë³µìž¡í•œ ëŒ€í™”í˜• ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° íŠ¹ížˆ ìœ ìš©í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ì§€ì› ì±—ë´‡ì—ì„œ ì‚¬ìš©ìžì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìžˆë‹¤.

CrewAIëŠ” ëª…í™•í•œ ì—­í•  ë¶„ë‹´ê³¼ ê°„ë‹¨í•œ ìž‘ì—… íë¦„ì— ì í•©í•˜ë©°, LangGraphëŠ” ë³µìž¡í•œ ì˜ì¡´ ê´€ê³„ì™€ ìƒíƒœ ê´€ë¦¬ê°€ í•„ìš”í•œ ìž‘ì—…ì— ë” ì í•©í•˜ë‹¤. CrewAIëŠ” 'ë¹ ë¥¸ ì‹œìž‘'ì„ ì§€í–¥í•˜ë©°, LangGraphëŠ” 'ì„¸ë°€í•œ ì œì–´'ë¥¼ ì§€í–¥í•œë‹¤ê³  ë³¼ ìˆ˜ ìžˆë‹¤.

ê¶ê·¹ì ìœ¼ë¡œ CrewAIì™€ LangGraph ì¤‘ ì–´ë–¤ ê²ƒì„ ì„ íƒí• ì§€ëŠ” íŠ¹ì • ìš”êµ¬ ì‚¬í•­ê³¼ ì „ë¬¸ì„±ì— ë”°ë¼ ê²°ì •ëœë‹¤ [2]. CrewAIëŠ” ì´ˆë³´ìžì™€ ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ì— ì í•©í•˜ê³ , LangGraphëŠ” ë” ë§Žì€ ì œì–´ë ¥ê³¼ ë³µì›ë ¥ì´ í•„ìš”í•œ ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš°ì— ì í•©í•˜ë‹¤ [2]. í”„ë¡œì íŠ¸ì˜ ë³µìž¡ì„±, í•„ìš”í•œ ì œì–´ ìˆ˜ì¤€, íŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì í•©í•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ë¼ë©´ CrewAIê°€ ì í•©í•  ìˆ˜ ìžˆìœ¼ë©°, ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ë³´ìž¥í•´ì•¼ í•˜ëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì´ë¼ë©´ LangGraphê°€ ë” ë‚˜ì€ ì„ íƒì¼ ìˆ˜ ìžˆë‹¤.

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

*   [1] https://www.zenml.io/blog/langgraph-vs-crewai
*   [2] https://www.truefoundry.com/blog/crewai-vs-langgraph
*   [3] https://www.zams.com/blog/crewai-vs-langgraph

## ê²°ë¡  ë° í‰ê°€

CrewAIì™€ LangGraphëŠ” ê°ê° ëšœë ·í•œ ìž¥ë‹¨ì ì„ ê°€ì§„ ë©€í‹° ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì´ë‹¤. CrewAIëŠ” ì ‘ê·¼ì„±ì´ ë›°ì–´ë‚˜ê³  ë¹ ë¥¸ ê°œë°œì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì´ ì œí•œì ì´ë‹¤. LangGraphëŠ” ë” ë†’ì€ ì œì–´ë ¥ê³¼ ìœ ì—°ì„±ì„ ì œê³µí•˜ì§€ë§Œ, í•™ìŠµ ê³¡ì„ ì´ ê°€íŒŒë¥´ê³  ë³µìž¡í•˜ë‹¤. ë”°ë¼ì„œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ ì‚¬í•­ê³¼ ê°œë°œíŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ˆë³´ìž ë˜ëŠ” ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ì—ëŠ” CrewAIë¥¼, ë³µìž¡í•˜ê³  ì œì–´ë ¥ì´ í•„ìš”í•œ í”„ë¡œì íŠ¸ì—ëŠ” LangGraphë¥¼ ì¶”ì²œí•œë‹¤. ë˜í•œ, ë‘ í”„ë ˆìž„ì›Œí¬ì˜ ê°•ì ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, CrewAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ ê°œë°œí•˜ê³ , LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤. ì•žìœ¼ë¡œ ë‘ í”„ë ˆìž„ì›Œí¬ ëª¨ë‘ ì§€ì†ì ìœ¼ë¡œ ë°œì „í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ ì£¼ì‹œí•˜ê³  í”„ë¡œì íŠ¸ì— ì í•©í•œ ìµœì ì˜ ì„ íƒì„ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
======================================================================


ðŸ“Š Running benchmark: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´...

ðŸ”Ž Clarify: Analyzing query...
   â””â”€ Query: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜
   â””â”€ Status: ðŸŸ¢ Clear
   â””â”€ Analysis: The query asks for an analysis of research papers related to LLM-based agent systems published in 2024 and a description of the main trends. The query is reasonably specific and well-defined.
   â””â”€ Topics: LLM, Agent systems, Research papers, Trends
ðŸ“‹ Planner: Creating research plan for: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´

ðŸ“‹ Planner: Generated 4 queries
   â””â”€ Queries:
      [1] LLM based agent system papers 2024
      [2] Large language model agent architectures 2024
      [3] Trends in LLM agent research 2024
      [4] Applications of LLM agents 2024
   â””â”€ Focus: Architectures of LLM-based agents, Applications of LLM-based agents, Performance benchmarks and evaluations, Limitations and challenges of current LLM-based agents

ðŸŽ¯ Supervisor [Phase 9]: Analyzing query complexity...
   â””â”€ Query: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜
   â””â”€ Planned queries: 4
   â””â”€ Complexity: ðŸŸ¡ MEDIUM
   â””â”€ Recommended: 2 iterations, depth 2
   â””â”€ Strategy: targeted
   â””â”€ Reasoning: The query requires analyzing multiple research papers to identify trends, indicating a medium level of complexity. Two iterations should be sufficient to cover the key aspects, and a depth of 2 will allow for a more focused investigation.

ðŸ”¬ Research Subgraph: Starting research loop...
   â””â”€ Supervisor config: MEDIUM, max 2 iterations, targeted strategy
ðŸ” Searcher [1]: Searching for: LLM based agent system papers 2024

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.nature.com/articles/s41598-025-92337-6
      [2] https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers
      [3] https://github.com/AGI-Edgerunners/LLM-Agents-Papers
      [4] https://www.sciencedirect.com/science/article/pii/S1571064524001386
      [5] https://arxiv.org/abs/2402.01680
   â””â”€ Snippets:
      â€¢ Article  MATH   Google Scholar 22. Zhang, H., Song, Y., Hou, Z., Miret, S. & Liu, B. Honeycomb: A Flexible LLM-based Agent System for Materials Science.  (2024). 23. Zhang, Q. et al. Large-language-model-based ai agent for organic semiconductor device research. Adv. Mater. 36, 2405163.  (2024). [...] MATH   Google Scholar 15. Wu, Q. et al. Autogen: Enabling Next-gen llm Applications Via Multi-agent Conversation (2024). 16. Jung, H., Covino, R. & Hummer, G. Artificial Intelligence Assists Discovery of Reaction Coordinates and Mechanisms from Molecular Dynamics Simulations.  (2019). 17. Ni, Z. et al. Matpilot: An llm-Enabled AI Materials Scientist Under the Framework of Humanâ€“Machine Collaboration.  (2024). [...] material design. ChemCrow21.") leverages API tools to tackle chemistry-related problems, and HoneyComb22.") explores LLM-based agent systems for materials science, albeit without fine-tuning. Further, AI applications have expanded to specialized domains, such as organic semiconductor device research, with LLM-based AI agents for organic semiconductor device research23.") demonstrating promising results. Similarly, ChatMOF24.") uses AI systems to predict and generate metalâ€“organic frameworks.
      â€¢ [2024/03] Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems. Lingjiao Chen et al. (  [2024/02] Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?. Qineng Wang et al. (  [2024/02] AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System. Zhiwei Liu et al. ( [...] Because the LLM-based Multi-Agents is a fast-growing research field, we may miss some important related papers. Very welcome contributions to this repository! Please feel free to submit a pull request or open an issue if you have anything to add or comment.  Taicheng Guo   Email: tguo2@nd.edu  Twitter:   ## About  Large Language Model based Multi-Agents: A Survey of Progress and Challenges (In IJCAI 2024)  arxiv.org/abs/2402.01680  ### Topics [...] Our summarized LLM-based Multi-Agents architecture is:      The Overview table is as follows. More details can be seen in our paper. Very appreciate any suggestions.  # ðŸ†• News  [2024/02] We will update our paper list every two weeks and include all the following papers in the next version of our paper. Please Feel free to contact me in case we have missed any papers!  [2024/01] This repo is created to maintain LLM-based Multi-Agents papers. We categorized these papers into five main streams:
      â€¢ [2024/08/12] Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models | ( | [code]     [2024/08/01] AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation | ( | [code]     [2024/07/04] Controllable Conversations: Planning-Based Dialogue Agent with Large Language Models | ( | [code]     [2024/06/17] RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents | ( | [code] [...] [2024/10/17] MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling | ( | [code]     [2024/10/16] MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration | ( | [code]     [2024/10/02] Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics | ( | [code]     [2024/08/28] Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions | ( | [code] [...] [2024/03/11] Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation | ( | [code]     [2024/03/10] TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision | ( | [code]     [2024/03/05] KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents | ( | [code]     [2024/02/29] PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval | ( | [code]
ðŸ’­ Think: Query: LLM based agent system papers 2024 | Found 5 results, 5 URLs. Key snippets: Article  MATH   Google Scholar
22. Zhang, H., Song, Y., Hou, Z., Miret, S. & Liu, B. Honeycomb: A Fl | [2024/03] Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems. Lingj | [2024/08/12] Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language M. Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://github.com/taichengguo/LLM_MultiAgents_Sur... (8015 chars)
ðŸ“– Read URL: https://github.com/AGI-Edgerunners/LLM-Agents-Pape... (8015 chars)
ðŸ“– Read URL: https://www.nature.com/articles/s41598-025-92337-6... (8015 chars)
   âœ“ [https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers]
      Preview: GitHub - taichengguo/LLM_MultiAgents_Survey_Papers: Large Language Model based Multi-Agents: A Survey of Progress and Challenges (In IJCAI 2024) Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform AI CODE CREATION GitHub Copilot Write better code with AI GitHub Spark Build and deploy intelligent apps GitHub Models Manage and compare prompts MCP Registry New Integrate external tools DEVELOPER WORKFLOWS Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes APPLICATION SECURITY GitHub Advanced Security Find and fix vulnerabilities Code security Secure your code as you build Secret protection Stop leaks before they start EXPLORE Why GitHub Documentation Blog Changelog Marketplace View all features Solutions BY COMPANY SIZE Enterprises Small and medium teams Startups Nonprofits BY USE CASE App Modernization DevSecOps DevOps CI/CD View all use cases BY INDUSTRY Healthcare Financial services Manufacturing Government View all industries View all solutions Resources EXPLORE BY TOPIC AI Software Development DevOps Security View all topics EXPLORE BY TYPE Customer stories Events &amp; webinars Ebooks &amp; reports Business insights GitHub Skills SUPPORT &amp; SERVICES Documentation Customer support Community forum Trust center Partners Open Source COMMUNITY GitHub Sponsors Fund open source developers PROGRAMS Security Lab Maintainer Community Accelerator Archive Program REPOSITORIES Topics Trending Collections Enterprise ENTERPRISE SOLUTIONS Enterprise platform AI-powered developer platform AVAILABLE ADD-ONS GitHub Advanced Security Enterprise-grade security features Copilot for Business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} taichengguo / LLM_MultiAgents_Survey_Papers Public Notifications You must be signed in to change notification settings Fork 64 Star 1.2k Large Language Model based Multi-Agents: A Survey of Progress and Challenges (In IJCAI 2024) arxiv.org/abs/2402.01680 1.2k stars 64 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 2 Pull requests 0 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights taichengguo/LLM_MultiAgents_Survey_Papers main Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 54 Commits LLM-MA.png LLM-MA.png README.md README.md image.png image.png overview.png overview.png trend.png trend.png View all files Repository files navigation README ðŸ¤– Awesome LLM-based Multi-Agents Papers ðŸ”¥ Paper ðŸ”¥ ðŸ”¥Our Survey Paper Our survey about LLM based Multi-Agents is available at: https://arxiv.org/abs/2402.01680 Our summarized LLM-based Multi-Agents architecture is: The Overview table is as follows. More details can be seen in our paper. Very appreciate any suggestions. ðŸ†• News [2024/02] We will update our paper list every two weeks and include all the following papers in the next version of our paper. Please Feel free to contact me in case we have mis
   âœ“ [https://github.com/AGI-Edgerunners/LLM-Agents-Papers]
      Preview: GitHub - AGI-Edgerunners/LLM-Agents-Papers: A repo lists papers related to LLM based agent Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform AI CODE CREATION GitHub Copilot Write better code with AI GitHub Spark Build and deploy intelligent apps GitHub Models Manage and compare prompts MCP Registry New Integrate external tools DEVELOPER WORKFLOWS Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes APPLICATION SECURITY GitHub Advanced Security Find and fix vulnerabilities Code security Secure your code as you build Secret protection Stop leaks before they start EXPLORE Why GitHub Documentation Blog Changelog Marketplace View all features Solutions BY COMPANY SIZE Enterprises Small and medium teams Startups Nonprofits BY USE CASE App Modernization DevSecOps DevOps CI/CD View all use cases BY INDUSTRY Healthcare Financial services Manufacturing Government View all industries View all solutions Resources EXPLORE BY TOPIC AI Software Development DevOps Security View all topics EXPLORE BY TYPE Customer stories Events &amp; webinars Ebooks &amp; reports Business insights GitHub Skills SUPPORT &amp; SERVICES Documentation Customer support Community forum Trust center Partners Open Source COMMUNITY GitHub Sponsors Fund open source developers PROGRAMS Security Lab Maintainer Community Accelerator Archive Program REPOSITORIES Topics Trending Collections Enterprise ENTERPRISE SOLUTIONS Enterprise platform AI-powered developer platform AVAILABLE ADD-ONS GitHub Advanced Security Enterprise-grade security features Copilot for Business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} AGI-Edgerunners / LLM-Agents-Papers Public Notifications You must be signed in to change notification settings Fork 133 Star 2.2k A repo lists papers related to LLM based agent 2.2k stars 133 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 1 Pull requests 4 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights AGI-Edgerunners/LLM-Agents-Papers main Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 164 Commits parsed_v5 parsed_v5 .gitignore .gitignore README.md README.md config_v5.json config_v5.json download_pdf.py download_pdf.py papers_v5.json papers_v5.json script_v5_step1.py script_v5_step1.py script_v5_step2.py script_v5_step2.py View all files Repository files navigation README LLM-Agents-Papers âœï¸ Description Last Updated Time: 2025/7/12 A repo lists papers related to LLM based agent. Includes Survey Technique For Enhancement Planning Memory Mechanism Feedback&amp;Reflection RAG Search Interaction Role Playing Conversation Game Playing Human-Agent Interaction Tool Usage Simulation Application Math Chemistry Biology Physics Geography Art Medicine Finance Software Engineering Research Automation Workflow Automatic Evaluation Training Fine tuning RL DPO Scaling Single-Agent Framework Multi
   âœ“ [https://www.nature.com/articles/s41598-025-92337-6]
      Preview: A fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters | Scientific Reports Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement View all journals Search Log in Content Explore content About the journal Publish with us Sign up for alerts RSS feed nature scientific reports articles article A fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters Download PDF Download PDF Article Open access Published: 25 March 2025 A fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters Zhuofan Shi 1 , 2 , 3 na1 , Chunxiao Xin 1 , 2 , 3 na1 , Tong Huo 2 , 3 , 4 na1 , Yuntao Jiang 1 , 2 na1 , Bowen Wu 1 na1 , Xingyue Chen 1 , Wei Qin 3 , Xinjian Ma 2 , 3 , Gang Huang 1 , 2 , 3 , Zhenyu Wang 1 &amp; â€¦ Xiang Jing 1 , 2 , 3 Show authors Scientific Reports volume 15 , Article number: 10295 ( 2025 ) Cite this article 6579 Accesses 3 Citations 1 Altmetric Metrics details Subjects Scientific data Theory and computation Abstract In the field of materials science, addressing the complex relationship between the material structure and properties has increasingly involved leveraging the text generation capabilities of AI-generated content (AIGC) models for tasks that include literature mining and data analysis. However, theoretical calculations and code development remain labor-intensive challenges. This paper proposes a novel approach based on text-to-code generation, utilizing large language models to automate the implementation of simulation programs in materials science. The effectiveness of automated code generation and review is validated with thermodynamics simulations based on the LAMMPS software as a foundation. This study introduces Molecular Dynamics Agent (MDAgent), a framework designed to guide large models in automatically generating, executing, and refining simulation code. In addition, a thermodynamic simulation code dataset for LAMMPS was constructed to fine-tune the language model. Expert evaluation scores demonstrate that MDAgent significantly improves the code generation and review capabilities. The proposed approach reduces the average task time by 42.22%, as compared to traditional models, thus highlighting its potential applications in the field of materials science. Similar content being viewed by others Large language models for reticular chemistry Article 01 February 2025 1.5 million materials narratives generated by chatbots Article Open access 28 September 2024 DPA-2: a large atomic model as a multi-task learner Article Open access 19 December 2024 Introduction Calculating thermodynamic properties is a common task in materials computation that provides important insights into material characteristics and informing design and optimization strategies in materials science. For instance, understanding phase transitions in two-dimensional materials is crucial for their applications in electronics and energy storage 1 . Accurately predicting and controlling specific thermodynamic properties, including stability and thermal expansion coefficients, is essential for ensuring the material performance in extreme environments, such as those in aerospace applications 2 . Molecular dynamics (MD) simulations have emerged as an indispensable tool to address these challenges at the atomic scale. Using platforms such as LAMMPS, which is specialized software for MD simulations, researchers can model thermodynamic phenomena, analyze atomic interactions, and investigate material behaviors under varying conditions 3 , 4 . MD 
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 1.68s
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] Several papers on LLM-based agents were published in 2024, covering topics like multi-agent conversation (AutoGen), applications in materials science (Honeycomb), and organic semiconductor device research.
      [2] Surveys and lists of papers related to LLM agents are available on GitHub, such as 'LLM_MultiAgents_Survey_Papers' and 'LLM-Agents-Papers'.
      [3] Research explores the use of LLMs in complex systems research, including generative agent-based models.
      [4] Specific applications include molecular dynamics code generation and planning-based dialogue agents.
   â””â”€ Decision: More research needed
   â””â”€ Next query: trends in LLM agent research 2024
ðŸ” Searcher [2]: Follow-up search for: trends in LLM agent research 2024

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://arxiv.org/html/2508.17281v1
      [2] https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6
      [3] https://blog.langchain.com/langchain-state-of-ai-2024/
      [4] https://www.linkedin.com/posts/sebastianraschka_noteworthy-llm-research-papers-of-2024-activity-7288592444613439488-2LwF
      [5] https://newsletter.victordibia.com/p/ai-agents-2024-rewind-a-year-of-building
   â””â”€ Snippets:
      â€¢ The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 [...] In LLM agent research, Googleâ€™s proprietary models are frequently featured, particularly in frameworks involving multiple model evaluations. The Gemini series is the most prominent [94, 83], with Gemini Pro emerging as the version most often used in agent development and evaluation [91, 109, 39, 107]. Gemini 1.5 Pro, known for its large context window, has been used in LLM-based agent reasoning under distraction tasks , and Gemini 1.0 Pro can be found in specific LLM agent benchmark studies [...] The landscape of open-source LLMs used in agent research and development is rapidly diversifying, offering researchers and developers powerful alternatives to proprietary models [87, 46]. With models like Metaâ€™s LLaMA, Mistral, Googleâ€™s Gemma, and the Qwen model from Alibaba gaining traction, open models now support a wide spectrum of capabilities, from code generation and dialogue to visual reasoning and collaborative decision making [102, 53, 57, 113]. An overview of all open source LLMs
      â€¢ driven by their capability in automating tasks intelligently. [...] Niche Know-How: The Rise of Domain-Specific Fine Tuned LLMs [...] AI agents in 2023 made notable progress. They formulate action plans, store data, select appropriate tools, and execute these plans. Consider a research-focused AI agent. When presented with a query, it independently navigates the web, aggregating and synthesizing data from varied sources. It then crafts a clear, informed response, delivering it via email. This process enhances research efficiency, providing timely and thorough information. The number of AI agents is projected to surge in 2024,
      â€¢ In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation. As more LLM apps are created, weâ€™re excited to see how folks dig into smarter workflows, better performance, and stronger reliability. [...] As we did last year, we want to share some product usage patterns that showcase how the AI ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, weâ€™ve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows. [...] As companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction â€” with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.
ðŸ’­ Think: Query: trends in LLM agent research 2024 | Found 5 results, 5 URLs. Key snippets: The pursuit of human-level artificial intelligence (AI) has significantly advanced the development o | driven by their capability in automating tasks intelligently. [...] Niche Know-How: The Rise of Doma | In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doi. Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://blog.langchain.com/langchain-state-of-ai-2... (8015 chars)
ðŸ“– Read URL: https://arxiv.org/html/2508.17281v1... (8015 chars)
   âœ“ [https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6]
      Preview: Error: HTTP 403 for URL: https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6
   âœ“ [https://blog.langchain.com/langchain-state-of-ai-2024/]
      Preview: LangChain State of AI 2024 Report Skip to content Case Studies In the Loop Web Home Try LangSmith Docs Sign in Subscribe LangChain State of AI 2024 Report Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving. By LangChain 6 min read Dec 19, 2024 Another year of building with LLMs is coming to an end â€”&nbsp;and 2024 didnâ€™t disappoint. With nearly 30k users signing up for LangSmith every month, weâ€™re lucky to have front row seats to whatâ€™s happening in the industry.&nbsp; As we did last year , we want to share some product usage patterns that showcase how the AI&nbsp;ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, weâ€™ve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows.&nbsp; Dive into the stats below to learn exactly what developers are building, testing, and prioritizing. Infrastructure usage With Large Language Models (LLMs) eating the world, everyoneâ€™s asking the mirror-mirror-on-the-wall question: â€œWhich model is the most utilized of them all?â€ Letâ€™s unpack what weâ€™ve seen. Top LLM providers Like last yearâ€™s results , OpenAI reigns as the most used LLM provider among LangSmith users â€”&nbsp;used more than 6x as much as Ollama, the next-most popular provider (counted by LangSmith organization usage). Interestingly, Ollama and Groq (which both allow users to run open source models, with the former focusing on local execution and the latter on cloud deployment) have accelerated in momentum this year, breaking into the top 5. This shows a growing interest in more flexible deployment options and customizable AI infrastructure. When it comes to providers that offer open-source models, the top providers have stayed relatively consistent compared to last year - Ollama, Mistral, and Hugging Face have made it easy for developers to run open source models on their platforms. These OSS providersâ€™ collective usage represents 20% of the top 20 LLM providers (by the number of organizations using them).&nbsp; Top Retrievers / Vector Stores Performing retrieval is still critical for many GenAI workflows. The top 3 vector stores have remained the same as last year, with Chroma and FAISS as the most popular choices. This year, Milvus, MongoDB, and Elasticâ€™s vector databases have also entered the top 10.&nbsp; Building with LangChain products As developers have gained more experience utilizing generative AI, they are also building more dynamic applications. From the growing sophistication of workflows, to the rise of AI agents â€” weâ€™re seeing a few trends that point to an evolving ecosystem of innovation. Observability isnâ€™t limited to LangChain applications While langchain (our open source framework) is central to many folksâ€™ LLM app development journeys, 15.7% of LangSmith traces this year come from non- langchain frameworks. This reflects a broader trend where observability is needed regardless of what framework youâ€™re using to build the LLM app â€” and that interoperability is supported by LangSmith. Python remains dominant, while JavaScript usage grows Debugging, testing, and monitoring certainly has a special place in our Python developersâ€™ hearts, with 84.7% usage coming from the Python SDK. But there is a notable and growing interest in JavaScript as developers pursue web-first applications â€”&nbsp;the JavaScript SDK accounts for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.&nbsp; AI agents are gaining traction As companies are getting more serious about incorporating AI agents across various industries , adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction â€”&nbsp;with 43% of LangSmith organizations are now 
   âœ“ [https://arxiv.org/html/2508.17281v1]
      Preview: From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users 1 Introduction 2 Related works 3 Methodology 3.1 Research questions (RQs) 3.2 Search strategies 3.3 Selection criteria 3.4 Article selection 4 Baseline Large Language Models for agent frameworks 4.1 Proprietary LLMs for agentic applications 4.2 Open-sourced LLMs for agentic applications 5 External tool integration across LLM agent workflows 5.1 Usage of tools across knowledge grounding, web search, and structured retrieval 5.2 Usage of tools across code generation, API use, and system-level integration 5.3 Usage of tools across interactive and embodied environments 6 Frameworks for building LLM agents 6.1 Basic architecture of an LLM agent 6.1.1 Single-agent LLM system 6.1.2 Multi-agent LLM systems 6.2 Common LLM agent frameworks 6.3 Domain-Specific Frameworks 6.3.1 Single-Agent LLM Systemsâ€™ Application Domain 6.3.2 Multi-Agent LLM systemsâ€™ application domain 7 Reasoning, planning, and memory of LLM agents 7.1 Reasoning in LLM-based agents 7.1.1 Application-specific reasoning techniques 7.1.2 Widely-used reasoning techniques 7.2 Planning in LLM-based agents 7.2.1 Application-specific reasoning techniques 7.2.2 Widely-used planning techniques 7.3 Memory mechanisms in LLM agents 7.3.1 Application-specific memory techniques 7.3.2 Widely-used memory techniques 8 Impact of prompting, fine-tuning and memory augmentation 8.1 Prompt engineering: a non-parametric approach to dynamic control and role delegation 8.2 Fine-tuning: embedding domain expertise and core behavioral traits 8.3 Memory augmentation: enabling grounded reasoning and experiential learning 8.4 The synergistic integration of prompting, fine-tuning, and memory 9 Evaluation benchmarks and datasets 9.1 Task-oriented and interactive benchmarks 9.2 Methodologies and metrics for evaluation 9.3 Datasets for agent training and grounding 10 Discussion 11 Future directions 11.1 Towards verifiable reasoning and robust self-improvement 11.2 Toward scalable, adaptive, and collaborative LLM-based agent systems 11.3 Deepening the human-agent symbiosis: personalization, proactivity, and trust 12 Conclusion From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users Sadia Sultana Chowa 1 Riasad Alvi 2 Subhey Sadi Rahman 2 Md Abdur Rahman 2 Mohaimenul Azam Khan Raiaan 2 Md Rafiqul Islam 3 Mukhtar Hussain 3 Sami Azam 3,* 1 Department of Computer Science and Engineering Daffodil International University Dhaka-1341 Bangladesh 2 Department of Computer Science and Engineering United International University Dhaka 1212 Bangladesh 3 Faculty of Science and Technology Charles Darwin University Casuarina NT 0909 Australia * Corresponding Author: sami.azam@cdu.edu.au Abstract The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals. A structured analysis of the LLM agentsâ€™ architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLM, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on 
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 0.13s
ðŸ”¬ Analyzer [2]: Analyzing 5 results, 6 contents

ðŸ”¬ Analyzer [2]: Analyzed 5 results, 6 contents
   â””â”€ New findings:
      [1] LLMs are being used as decision-making agents capable of interpreting instructions, managing tasks, and adapting through feedback.
      [2] Trends in LLM agents for 2024 include multi-step agents, increased efficiency with fewer LLM calls, and the implementation of quality checks through feedback and evaluation.
      [3] LLMs are evolving from solely language modeling to incorporating built-in capabilities for multi-step task decomposition, planning, tool use, and multimodal interactions.
      [4] GitHub repositories such as 'LLM_MultiAgents_Survey_Papers' and 'LLM-Agents-Papers' offer collections of research papers related to LLM-based agents.
      [5] Specific applications of LLM agents include molecular dynamics code generation for material thermodynamic parameter determination.
   â””â”€ Decision: More research needed
   â””â”€ Next query: specific examples of LLM agent architectures 2024
ðŸ” Searcher [3]: Follow-up search for: specific examples of LLM agent architectures 2024

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react
      [2] https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/
      [3] https://juteq.ca/biggest-ai-agent-paper-releases-2024/
      [4] https://medium.com/@gianluca.mondillo/how-ai-agents-really-work-beyond-the-hype-inside-the-architecture-aa6c892a5c2a
      [5] https://www.promptingguide.ai/research/llm-agents
   â””â”€ Snippets:
      â€¢ Planning phase: The agent  generates a list of steps or sub-tasks needed to achieve the goal. This plan is formulated before taking any action. For example, given our question, a planning agent might produce: (1) Identify the 2024 Australian Open menâ€™s winner; (2) Find that winnerâ€™s hometown; (3) Check if this person has collaborations with University X; (4) Compile the findings. This plan gives a high-level roadmap.     â€ [...] In our tennis question example, a multi-agent solution might deploy one agent to handle sports queries (â€œWho won the 2024 Australian Open?â€) and another to handle academic queries (â€œDid that person collaborate with University X?â€). A supervising agent could orchestrate the workflow: first activating the sports agent to get the winner and hometown, then passing that info to the academic agent. This modular approach keeps each agentâ€™s context focused, reducing confusion and errors.  â€ [...] Real-world projects often require an AI to perform diverse sub-tasks or handle large context, which can overwhelm a single monolithic agent. Multi-agent architectures tackle this by splitting the problem among multiple smaller, specialized agents that work together. Each agent can be designed for a specific role or expertise â€“ for example, one agent focused on web research, another on data analysis, and another on summarization.  â€
      â€¢ If 2023 was the year of retrieval augmented generation, 2024 has been the year of agents. Companies all over the world are experimenting with chatbot agents, tools like MultiOn are growing by connecting agents to outside websites, and frameworks like LangGraph and LlamaIndex Workflows are helping developers around the world build structured agents. [...] Hereâ€™s an example of a LangGraph agent that can either respond to a userâ€™s greeting, or perform some sort of RAG lookup of information:  Here, the routing logic instead lives within nodes and conditional edges that choose to move the user between different nodes depending on a function response. In this case, is\_greeting and check\_rag\_response are conditional edges. Defining one of these edges looks like this: [...] Finally, many agents will track a shared state or memory as they execute. This allows agents to more easily pass context between various components.  # Examples of Agent Architectures  There are some common patterns we see across agent deployments today. Weâ€™ll walk through an overview of all of those architectures in the following pieces but the below examples are probably the most common.
      â€¢ 1. Dynamic action creation and composition 2. Online learning and adaptation 3. Enhanced flexibility for LLM agents  While the specific architecture is not detailed in the search results, we can infer that DynaSaur likely includes: [...] ðŸ”— Click here to learn more  ### 4. Harvardâ€™s FINCON  FINCON is an LLM-based multi-agent framework developed by Harvard researchers, specifically designed for diverse financial tasks. It employs conversational verbal reinforcement to enhance agent performance.  Key features:   Specialized in financial domain tasks  Multi-agent architecture  Conversational verbal reinforcement learning [...] Graph-based representation of long texts  Agent-driven exploration of the graph  Enhanced long-context understanding for LLMs  The Graph Reader architecture includes:  1. Text-to-Graph Converter: Transforms long texts into graph structures 2. Graph Explorer Agent: Navigates the graph to extract relevant information 3. LLM Interface: Integrates graph-based knowledge with LLM capabilities 4. Answer Generator: Produces responses based on graph exploration and LLM processing
ðŸ’­ Think: Query: specific examples of LLM agent architectures 2024 | Found 5 results, 5 URLs. Key snippets: Planning phase: The agent  generates a list of steps or sub-tasks needed to achieve the goal. This p | If 2023 was the year of retrieval augmented generation, 2024 has been the year of agents. Companies  | 1. Dynamic action creation and composition
2. Online learning and adaptation
3. Enhanced flexi. Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://www.wollenlabs.com/blog-posts/navigating-m... (8015 chars)
ðŸ“– Read URL: https://towardsdatascience.com/navigating-the-new-... (8015 chars)
ðŸ“– Read URL: https://juteq.ca/biggest-ai-agent-paper-releases-2... (8015 chars)
   âœ“ [https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react]
      Preview: Wollen Labs Services Work Team Blog Community Contact July 17, 2025 Navigating Modern LLM Agent Architectures A guide to understanding commonly used LLM Agent architectures and their distinct advantages and disadvantages: Multiâ€‘Agents, Plan-and-Execute, ReWOO, Tree-of-Thoughts, and ReAct Published by: Jack Spolski Santiago Pourteau â€Building an AI agent to tackle complex tasks requires more than just prompting a single large language model (LLM) for an answer. Recent research and frameworks (like LangChainâ€™s LangGraph ) have introduced various architectures to extend an LLMâ€™s capabilities. These architectures help break down problems, integrate external knowledge, and coordinate reasoning steps. In this post, weâ€™ll explore several prominent agent patterns, ReAct (Reason + Act), Multi-Agent systems , Plan-and-Execute , ReWOO (Reasoning Without Observation) and Tree-of-Thoughts and discuss when to use each approach. â€ Imagine youâ€™re building an AI assistant to answer a complex research question : â€œWho won the 2024 Australian Open, and what is their hometown? Also, did that person collaborate with any researchers from University X?â€ ). Answering this involves multiple steps: finding the tennis winner, finding their hometown, then checking for academic collaborations â€“ a mix of factual lookup and multi-step reasoning. Letâ€™s see how each architecture could approach such a task, and what unique strengths they bring. â€ ReAct (Reason + Act): Think Step-by-Step, Act When Needed â€ ReAct is one of the earliest and most influential agent patterns for LLMs. ReAct stands for â€œReasoning and Actingâ€ and it marries the idea of chain-of-thought reasoning with tool use . A ReAct agent operates in a loop that goes: Thought â†’ Action â†’ Observation , and then repeats this cycle until the task is complete. In simpler terms, the agent thinks out loud , decides on an action (like calling a tool or giving a final answer), observes the result of that action, and then uses that new information to inform the next thought . â€ To illustrate, letâ€™s apply ReAct to our example question (winner and hometown and collaboration): â€ Thought 1: â€œThe question asks for the 2024 Australian Open winner and their hometown, then about collaborations with University X. I should first find out who won the 2024 Australian Open menâ€™s title.â€ â€ Action 1: Call the search tool with query â€œ2024 Australian Open menâ€™s winnerâ€. â€ Observation 1: The search returns: â€œThe menâ€™s singles champion in 2024 was Jannik Sinner .â€ â€ Thought 2: â€œThe winner is Jannik Sinner. Now I need his hometown. Let me find that.â€ â€ Action 2: Search for â€œJannik Sinner hometownâ€. â€ Observation 2: The results say: â€œJannik Sinnerâ€™s hometown is Sexten, in northern Italy.â€ â€ Thought 3: â€œSo Sinner is from Sexten, Italy. Next, did he collaborate with anyone at University X? Heâ€™s a tennis player, likely not an academic, but maybe University X did a study with him or he gave a talk there. I should search for Sinner and University X together.â€ â€ Action 3: Search for â€œJannik Sinner University X collaborationâ€. â€ Observation 3: No obvious results about collaborations (likely none exist if itâ€™s a random university). â€ Thought 4: â€œIt appears he has no direct collaborations with University X. I should answer with the info I have: he won the 2024 Australian Open, hometown Sexten, and presumably no notable collaboration with that university.â€ â€ Action 4: Finish with the compiled answer. â€ This sequence shows how ReAct allows the agent to adapt at each step based on observations . If the first search had already included hometown info, the agent might have skipped a step. If the collaboration search turned up something unexpected (say he did participate in an event at University X), the agent would incorporate that into the answer. This dynamic looping is what gives ReAct its robustness: it closely mimics how a human problem-solver alternates between thinking and doing , adjusting plans on the fly. â€ Key characteristics of ReA
   âœ“ [https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/]
      Preview: Navigating the New Types of LLM Agents and Architectures | Towards Data Science Publish AI, ML &amp; data-science insights to a global community of data professionals. Sign in Submit an Article Latest Editorâ€™s Picks Deep Dives Newsletter Write For TDS Toggle Mobile Navigation LinkedIn X Toggle Search Search Navigating the New Types of LLM Agents and Architectures The failure of ReAct agents gives way to a new generation of agents â€” and possibilities Aparna Dhinakaran Aug 30, 2024 12 min read Share Image created by author using Dall-E My thanks to John Gilhuly for his contributions to this piece If 2023 was the year of retrieval augmented generation, 2024 has been the year of agents. Companies all over the world are experimenting with chatbot agents, tools like MultiOn are growing by connecting agents to outside websites, and frameworks like LangGraph and LlamaIndex Workflows are helping developers around the world build structured agents. However, despite their popularity, agents have yet to make a strong splash outside of the AI ecosystem. Few agents are taking off among either consumer or enterprise users. How can teams navigate the new frameworks and new agent directions? What tools are available, and which should you use to build your next application? As a leader at a company that recently built our own complex agent to act as a copilot within our product, we have some insights on this topic. Defining Agents First, it helps to define what we mean by an agent. LLM-based agents are software systems that string together multiple processing steps, including calls to LLMs, in order to achieve a desired end result. Agents typically have some amount of conditional logic or decision-making capabilities, as well as a working memory they can access between steps. Letâ€™s dive into how agents are built today, the current problems with modern agents, and some initial solutions. The Failure of ReAct Agents Letâ€™s be honest, the idea of an agent isnâ€™t new. There were countless agents launched on AI Twitter over the last year claiming amazing feats of intelligence. This first generation were mainly&nbsp; ReAct &nbsp;(reason, act) agents. They were designed to abstract as much as possible, and promised a wide set of outcomes. Unfortunately, this first generation of agent architectures really struggled. Their heavy abstraction made them hard to use, and despite their lofty promises, they turned out to&nbsp; not do much of anything . In reaction to this, many people began to rethink how agents should be structured. In the past year weâ€™ve seen great advances, now leading us into the next generation of agents. What is the Second Generation of Agents? This new generation of agents is built on the principle of defining the possible paths an agent can take in a much more rigid fashion, instead of the open-ended nature of ReAct. Whether agents use a framework or not, we have seen a trend towards smaller solution spaces â€” aka a reduction in the possible things each agent can do. A smaller solution space means an easier-to-define agent, which often leads to a more powerful agent. This second generation covers many different types of agents, however itâ€™s worth noting that&nbsp; most of the agents or assistants we see today are written in code without frameworks, have an LLM router stage, and process data in iterative loops. What Makes Up An Agent? Many agents have a node or component called a&nbsp; router,&nbsp; that decides which step the agent should take next. The term router normally refers to an LLM or classifier making an intent decision of what path to take. An agent may return to this router continuously as they progress through their execution, each time bringing some updated information. The router will take that information, combine it with its existing knowledge of the possible next steps, and choose the next action to take. The router itself is sometimes powered by a call to an LLM. Most popular LLMs at this point support&nbsp; function c
   âœ“ [https://juteq.ca/biggest-ai-agent-paper-releases-2024/]
      Preview: 2024â€™s Most Powerful AI Agent Papers - JUTEQ Inc Skip to content Home Services GenAI Cloud DevOps Kubernetes Solutions Cloud Architect Agent Insights Contact Us About Schedule A Call Home Services GenAI Cloud DevOps Kubernetes Solutions Cloud Architect Agent Insights Contact Us About Schedule A Call Menu 2024's Most Powerful AI Agent Papers AI Agents have become one of the most popular tech trends of 2024, but they are still pretty new and require a lot of improvement. This year we saw an outstanding research and improvements in agents that even made us rethink the overall concept. From Frameworks to Surveys, we have categorized the list this year as there was a great number of novel research papers coming from top companies and Universities. Letâ€™s take a look at our collection of research papers that made AI Agents even more interesting this year: Frameworks: 1. Magentic-One by Microsoft Magentic-One is an updated version of Microsoft&#8217;s Autogen framework, designed to create a generalist multi-agent system for solving open-ended web and file-based tasks across various domains. Key features: Multi-agent architecture for handling complex tasks Ability to process web and file-based inputs Generalist approach for versatility across domains The system likely employs a coordinated set of specialized agents, each focusing on different aspects of task solving, such as information retrieval, reasoning, and output generation. We can infer that Magentic-One probably uses a hierarchical structure where a central coordinator manages task distribution among various specialized agents. ðŸ”— Click here to learn more 2. Agent-oriented planning in a Multi-Agent system This framework introduces a novel approach to planning in multi-agent systems using a Meta-agent architecture. The system is designed to enhance coordination and decision-making among multiple AI agents. Key features: Meta-agent architecture for overseeing planning Improved coordination among multiple agents Enhanced decision-making capabilities We can envision a system where a meta-agent oversees and coordinates the planning activities of individual agents. This meta-agent has a global view of the task and can optimize the overall planning strategy by considering the strengths and limitations of each agent in the system. ðŸ”— Click here to learn more 3. KGLA by Amazon Amazon&#8217;s KGLA (Knowledge Graph-enhanced Agent) framework is designed to improve knowledge retrieval across various domains. This system leverages knowledge graphs to enhance the capabilities of AI agents. Key features: Integration of knowledge graphs with AI agents Improved knowledge retrieval capabilities Applicability across multiple domains The KGLA architecture likely consists of several key components: Knowledge Graph: A structured representation of domain knowledge Agent Interface: Allows agents to query and interact with the knowledge graph Retrieval Mechanism: Efficiently extracts relevant information from the graph Reasoning Module: Combines retrieved knowledge with agent capabilities This integration allows agents to access and utilize structured knowledge more effectively, potentially improving their performance on complex tasks that require extensive domain knowledge. ðŸ”— Click here to learn more 4. Harvard&#8217;s FINCON FINCON is an LLM-based multi-agent framework developed by Harvard researchers, specifically designed for diverse financial tasks. It employs conversational verbal reinforcement to enhance agent performance. Key features: Specialized in financial domain tasks Multi-agent architecture Conversational verbal reinforcement learning The FINCON architecture likely includes: 1. Multiple Specialized Agents: Each focusing on different aspects of financial tasks 2. Conversational Interface: Allows agents to communicate and learn from each other 3. Verbal Reinforcement Module: Provides feedback to improve agent performance 4. Task Coordinator: Manages the distribution and integration of subtask
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 0.61s
ðŸ”¬ Analyzer [3]: Analyzing 5 results, 9 contents

ðŸ”¬ Analyzer: Max iterations reached, proceeding to Writer

ðŸ”¬ Analyzer [3]: Analyzed 5 results, 9 contents
   â””â”€ New findings:
      [1] Several papers on LLM-based agents were published in 2024, covering topics like multi-agent conversation (AutoGen), applications in materials science (Honeycomb), and organic semiconductor device research.
      [2] Surveys and lists of papers related to LLM agents are available on GitHub, such as 'LLM_MultiAgents_Survey_Papers' and 'LLM-Agents-Papers'.
      [3] Research explores the use of LLMs in complex systems research, including generative agent-based models.
      [4] Specific applications include molecular dynamics code generation and planning-based dialogue agents.
      [5] LLMs are being used as decision-making agents capable of interpreting instructions, managing tasks, and adapting through feedback.
   â””â”€ Decision: Research complete (19 total findings)
   â””â”€ âœ… Research Subgraph completed (execution #1)
   â””â”€ Findings: 19 items
   â””â”€ Contents: 9 URLs read

ðŸ“¦ Compress: Compressing 19 findings, 9 contents
   â””â”€ Compressed to 2589 chars (from ~7297 raw chars)
   â””â”€ Sources cited: 9
   â””â”€ Preview:
      ## Key Findings
      - 2024 has seen a surge in research and applications of LLM-based agents [1], [2], [7].
      - LLMs are evolving to incorporate multi-step task decomposition, planning, tool use, and multimodal interactions [2], [5].
      - Trends include multi-step agents, increased efficiency, and quality checks through feedback [2].
      - LLM agents are being applied in diverse fields like materials science and complex systems research [7], [3].
      
      ## Detailed Information
      
      **General Overview:**
      
      *   2024 is considered a key year for the development and application of LLM agents [1]. Several research papers on LLM-based agents have been published [1], [7].
      *   LLMs are transitioning from purely language modeling to incorporating capabilities for multi-step task decomposition, planning, tool use, and multimodal interactions [2], [5].
      
      **Trends in LLM Agents (2024):**
      
      *   Key trends include the development of multi-step agents [2].
      *   Researchers are focused on increasing efficiency by reducing the number of LLM calls [2].
      *   Quality checks are being implemented through feedback and evaluation mechanisms [2].
      
      **Applications of LLM Agents:**
      
      *   LLMs are being used in complex systems research, including generative agent-based models [3].
      *   Specific applications include molecular dynamics code generation for material thermodynamic parameter determination [7].
      *   LLMs are used as decision-making agents, interpreting instructions, managing tasks, and adapting through feedback [5].
      *   LLMs facilitate multi-agent conversation (AutoGen), applications in materials science (Honeycomb), and organic semiconductor device research [7].
      *   Planning-based dialogue agents are another area of application [5].
      
      **Resources:**
      
      *   GitHub repositories such as 'LLM\_MultiAgents\_Survey\_Papers' [9] and 'LLM-Agents-Papers' [8] provide collections of research papers related to LLM-based agents.
      
      ## Sources
      [1] https://blog.langchain.com/langchain-state-of-ai-2024/
      [2] https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6
      [3] https://arxiv.org/html/2508.17281v1
      [4] https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/
      [5] https://juteq.ca/biggest-ai-agent-paper-releases-2024/
      [6] https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react
      [7] https://www.nature.com/articles/s41598-025-92337-6
      [8] https://github.com/AGI-Edgerunners/LLM-Agents-Papers
      [9] https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers

âœï¸ Writer: Composing response from 19 findings
   â””â”€ Using compressed notes (2589 chars)

âœï¸ Writer: Generated response (4379 chars)
   â””â”€ Sources used:
      [1] https://blog.langchain.com/langchain-state-of-ai-2024/
      [2] https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6
      [3] https://arxiv.org/html/2508.17281v1
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      2024ë…„ì€ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì—°êµ¬ì™€ í™œìš©ì´ ê¸‰ì¦í•œ í•´ì´ë©° [1], [2], [7], íŠ¹ížˆ ë‹¤ë‹¨ê³„ ìž‘ì—… ë¶„í•´, ê³„íš ìˆ˜ë¦½, ë„êµ¬ ì‚¬ìš©, ë©€í‹°ëª¨ë‹¬ ìƒí˜¸ìž‘ìš© í†µí•©ì— ì´ˆì ì„ ë§žì¶˜ LLMì˜ ì§„í™”ê°€ ë‘ë“œëŸ¬ì§„ë‹¤ [2], [5].  ì£¼ìš” íŠ¸ë Œë“œëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ, íš¨ìœ¨ì„± í–¥ìƒ, í”¼ë“œë°±ì„ í†µí•œ í’ˆì§ˆ ê²€ì¦ì´ë©°, ìž¬ë£Œ ê³¼í•™ ë° ë³µìž¡ê³„ ì—°êµ¬ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ê³  ìžˆë‹¤ [2], [7], [3].
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      *   2024ë…„ì€ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê°œë°œ ë° ì‘ìš©ì— ìžˆì–´ ì¤‘ìš”í•œ í•´ë¡œ ê°„ì£¼ëœë‹¤ [1].
      *   LLMì€ ë‹¨ìˆœí•œ ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ë²—ì–´ë‚˜ ë‹¤ë‹¨ê³„ ìž‘ì—… ë¶„í•´, ê³„íš, ë„êµ¬ ì‚¬ìš© ë° ë©€í‹°ëª¨ë‹¬ ìƒí˜¸ ìž‘ìš© ê¸°ëŠ¥ì„ í†µí•©í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í™”í•˜ê³  ìžˆë‹¤ [2], [5].
      *   ì£¼ìš” íŠ¸ë Œë“œë¡œëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ [2], LLM í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œë¥¼ í†µí•œ íš¨ìœ¨ì„± í–¥ìƒ [2], í”¼ë“œë°± ë° í‰ê°€ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ í’ˆì§ˆ ê²€ì¦ êµ¬í˜„ ë“±ì´ ìžˆë‹¤ [2].
      *   LLMì€ ë³µìž¡ê³„ ì—°êµ¬, íŠ¹ížˆ ìƒì„±ì  ì—ì´ì „íŠ¸ ê¸°ë°˜ ëª¨ë¸ì— ì‚¬ìš©ë˜ê³  ìžˆë‹¤ [3].
      *   êµ¬ì²´ì ì¸ ì‘ìš© ë¶„ì•¼ë¡œëŠ” ìž¬ë£Œ ì—´ì—­í•™ íŒŒë¼ë¯¸í„° ê²°ì •ì„ ìœ„í•œ ë¶„ìž ì—­í•™ ì½”ë“œ ìƒì„± [7]ì´ ìžˆë‹¤.
      *   LLMì€ ì§€ì‹œ í•´ì„, ìž‘ì—… ê´€ë¦¬, í”¼ë“œë°±ì„ í†µí•œ ì ì‘ ë“± ì˜ì‚¬ ê²°ì • ì—ì´ì „íŠ¸ë¡œ í™œìš©ëœë‹¤ [5].
      *   LLMì€ ë©€í‹° ì—ì´ì „íŠ¸ ëŒ€í™” (AutoGen), ìž¬ë£Œ ê³¼í•™ (Honeycomb), ìœ ê¸° ë°˜ë„ì²´ ìž¥ì¹˜ ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•œë‹¤ [7].
      *   ê³„íš ê¸°ë°˜ ëŒ€í™” ì—ì´ì „íŠ¸ ë˜í•œ ì‘ìš© ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë‹¤ [5].
      *   'LLM\_MultiAgents\_Survey\_Papers' [9] ë° 'LLM-Agents-Papers' [8]ì™€ ê°™ì€ GitHub ì €ìž¥ì†ŒëŠ” LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ ëª¨ìŒì„ ì œê³µí•œë‹¤.
      
      ## ìƒì„¸ ë¶„ì„
      
      LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ 2024ë…„ ë“¤ì–´ ë¹„ì•½ì ì¸ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤.  ê¸°ì¡´ì˜ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì—ì„œ ë²—ì–´ë‚˜, ë³µìž¡í•œ ìž‘ì—…ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ê³„íší•˜ê³ , í•„ìš”í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ë©°, ì´ë¯¸ì§€, ìŒì„± ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸ë¡œ ì§„í™”í•˜ê³  ìžˆë‹¤ëŠ” ì ì´ ê°€ìž¥ í° íŠ¹ì§•ì´ë‹¤ [2], [5].  ì´ëŠ” LLMì´ ë‹¨ìˆœí•œ 'ì±—ë´‡' ìˆ˜ì¤€ì„ ë„˜ì–´, ì‹¤ì œ ì„¸ê³„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” 'ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸'ë¡œ ë°œë‹ì›€í•˜ê³  ìžˆìŒì„ ì‹œì‚¬í•œë‹¤.
      
      **ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ (Multi-Step Agents):**  í•˜ë‚˜ì˜ ë³µìž¡í•œ ìž‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´, LLM ì—ì´ì „íŠ¸ê°€ ì—¬ëŸ¬ ë‹¨ê³„ì˜ í•˜ìœ„ ìž‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ê° ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ ì£¼ëª©ë°›ê³  ìžˆë‹¤ [2]. ì´ëŠ” ë§ˆì¹˜ ì‚¬ëžŒì´ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ, ë¬¸ì œ í•´ê²° ê³¼ì •ì„ ì„¸ë¶„í™”í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ë‹¤.  ì´ëŸ¬í•œ ë‹¤ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì€ ë¬¸ì œ í•´ê²°ì˜ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.
      
      **íš¨ìœ¨ì„± í–¥ìƒ:**  LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì—°êµ¬ë„ í™œë°œí•˜ê²Œ ì§„í–‰ë˜ê³  ìžˆë‹¤ [2]. íŠ¹ížˆ, LLM API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ë¹„ìš© ì ˆê°ê³¼ ì‘ë‹µ ì†ë„ í–¥ìƒì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•œ ê³¼ì œì´ë‹¤.  ì—°êµ¬ìžë“¤ì€ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ í†µí•´ LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„, ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë° ì£¼ë ¥í•˜ê³  ìžˆë‹¤.
      
      **í’ˆì§ˆ ê²€ì¦:**  LLM ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¬¼ì„ ì‹ ë¢°í•  ìˆ˜ ìžˆë„ë¡ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ë„ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ê³  ìžˆë‹¤ [2].  í”¼ë“œë°± ë£¨í”„ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ê°œì„ í•˜ê±°ë‚˜, ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ë°©ë²• ë“±ì´ ì—°êµ¬ë˜ê³  ìžˆë‹¤.  ì‚¬ëžŒì˜ ê°œìž… ì—†ì´ ìžë™ìœ¼ë¡œ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ê¸°ìˆ ì€ LLM ì—ì´ì „íŠ¸ì˜ ì‹¤ìš©ì„±ì„ ë†’ì´ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.
      
      **ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼:**  LLM ì—ì´ì „íŠ¸ëŠ” ìž¬ë£Œ ê³¼í•™ [7], ë³µìž¡ê³„ ì—°êµ¬ [3], ìœ ê¸° ë°˜ë„ì²´ ìž¥ì¹˜ ì—°êµ¬ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ê³  ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìž¬ë£Œ ê³¼í•™ ë¶„ì•¼ì—ì„œëŠ” LLM ì—ì´ì „íŠ¸ê°€ ë¶„ìž ì—­í•™ ì½”ë“œë¥¼ ìƒì„±í•˜ì—¬ ìƒˆë¡œìš´ ìž¬ë£Œì˜ ì—´ì—­í•™ì  íŒŒë¼ë¯¸í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìžˆë‹¤ [7].  ì´ëŠ” LLM ì—ì´ì „íŠ¸ê°€ ê³¼í•™ ì—°êµ¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìžˆìŒì„ ë³´ì—¬ì¤€ë‹¤.  ë˜í•œ, AutoGen [7] ê³¼ ê°™ì€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ LLM ì—ì´ì „íŠ¸ ê°„ì˜ í˜‘ì—…ì„ í†µí•´ ë”ìš± ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆë„ë¡ í•œë‹¤.
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      *   [1] Langchain State of AI 2024: [https://blog.langchain.com/langchain-state-of-ai-2024/](https://blog.langchain.com/langchain-state-of-ai-2024/)
      *   [2] LLM Trends in the Making: What to Expect in 2024: [https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6](https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6)
      *   [3] arXiv - Generative Agent-Based Models: [https://arxiv.org/html/2508.17281v1](https://arxiv.org/html/2508.17281v1)
      *   [4] Navigating the New Types of LLM Agents and Architectures: [https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/](https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/)
      *   [5] Biggest AI Agent Paper Releases 2024: [https://juteq.ca/biggest-ai-agent-paper-releases-2024/](https://juteq.ca/biggest-ai-agent-paper-releases-2024/)
      *   [6] Navigating Modern LLM Agent Architectures: [https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react](https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react)
      *   [7] LLM accelerated molecular dynamics code generation: [https://www.nature.com/articles/s41598-025-92337-6](https://www.nature.com/articles/s41598-025-92337-6)
      *   [8] LLM-Agents-Papers (GitHub): [https://github.com/AGI-Edgerunners/LLM-Agents-Papers](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)
      *   [9] LLM\_MultiAgents\_Survey\_Papers (GitHub): [https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers](https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers)
      
      ## ê²°ë¡  ë° í‰ê°€
      
      2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ì–‘ì , ì§ˆì ìœ¼ë¡œ ê´„ëª©í•  ë§Œí•œ ì„±ìž¥ì„ ì´ë£¨ì—ˆë‹¤. íŠ¹ížˆ, ë‹¤ë‹¨ê³„ ìž‘ì—… ì²˜ë¦¬, íš¨ìœ¨ì„± ê°œì„ , í’ˆì§ˆ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ ë„ìž…ì€ LLM ì—ì´ì „íŠ¸ì˜ ì‹¤ìš©ì„±ì„ ë†’ì´ëŠ” ë° í¬ê²Œ ê¸°ì—¬í–ˆë‹¤. ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ë©´ì„œ LLM ì—ì´ì „íŠ¸ì˜ ìž ìž¬ë ¥ì´ ìž…ì¦ë˜ì—ˆì§€ë§Œ, ì—¬ì „ížˆ í•´ê²°í•´ì•¼ í•  ê³¼ì œë„ ë‚¨ì•„ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ì˜ ì•ˆì „ì„±, ìœ¤ë¦¬ì  ë¬¸ì œ, ê·¸ë¦¬ê³  ì„¤ëª…ê°€ëŠ¥ì„± (Explainability)ì€ ì•žìœ¼ë¡œ ë”ìš± ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì ¸ì•¼ í•  ê²ƒì´ë‹¤.  ë˜í•œ, íŠ¹ì • ë¶„ì•¼ì— íŠ¹í™”ëœ LLM ì—ì´ì „íŠ¸ë¥¼ ê°œë°œí•˜ê³ , ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ì´ ë  ê²ƒì´ë‹¤.  GitHub ì €ìž¥ì†Œ [8], [9] ì™€ ê°™ì€ ìžë£Œë¥¼ í†µí•´ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ì§€ì†ì ìœ¼ë¡œ íŒŒì•…í•˜ê³ , ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— ëŒ€í•œ ì‹¤í—˜ê³¼ ê²€ì¦ì„ í†µí•´ LLM ì—ì´ì „íŠ¸ ê¸°ìˆ ì„ ë°œì „ì‹œì¼œ ë‚˜ê°€ì•¼ í•  ê²ƒì´ë‹¤.  ì¢…í•©ì ìœ¼ë¡œ ë³¼ ë•Œ, 2024ë…„ì€ LLM ì—ì´ì „íŠ¸ì˜ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê³ , ë¯¸ëž˜ ë°œì „ì„ ìœ„í•œ í† ëŒ€ë¥¼ ë‹¤ì§„ í•´ë¡œ í‰ê°€í•  ìˆ˜ ìžˆë‹¤.

ðŸ” Critique: CARC Quality Evaluation...
   â””â”€ CARC Scores: C=4.0 A=3.5 R=4.5 C=4.0
   â””â”€ Total: 16.0/20 âœ… Excellent
   â””â”€ Feedback: The response provides a good overview of LLM-based agent systems in 2024, covering key trends and applications. However, it could benefit from including a wider range of sources, especially academic papers, to enhance accuracy. While the relevance and clarity are strong, some claims need more robust backing.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 9 Supervisor Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜...
â”‚ Time: 43.78s
â”‚ LLM Calls: 2
â”‚ Search Calls: 0
â”‚ URLs Read: 9
â”‚ Iterations: 3
â”‚ Est. Tokens: 1562
â”‚ Response Length: 4379 chars
â”‚ Has Citations: âœ…
â”‚ CARC Quality: C=4.0 A=3.5 R=4.5 C=4.0 â†’ 16.0/20 ðŸ‘ Good
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

2024ë…„ì€ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì—°êµ¬ì™€ í™œìš©ì´ ê¸‰ì¦í•œ í•´ì´ë©° [1], [2], [7], íŠ¹ížˆ ë‹¤ë‹¨ê³„ ìž‘ì—… ë¶„í•´, ê³„íš ìˆ˜ë¦½, ë„êµ¬ ì‚¬ìš©, ë©€í‹°ëª¨ë‹¬ ìƒí˜¸ìž‘ìš© í†µí•©ì— ì´ˆì ì„ ë§žì¶˜ LLMì˜ ì§„í™”ê°€ ë‘ë“œëŸ¬ì§„ë‹¤ [2], [5].  ì£¼ìš” íŠ¸ë Œë“œëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ, íš¨ìœ¨ì„± í–¥ìƒ, í”¼ë“œë°±ì„ í†µí•œ í’ˆì§ˆ ê²€ì¦ì´ë©°, ìž¬ë£Œ ê³¼í•™ ë° ë³µìž¡ê³„ ì—°êµ¬ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ê³  ìžˆë‹¤ [2], [7], [3].

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

*   2024ë…„ì€ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê°œë°œ ë° ì‘ìš©ì— ìžˆì–´ ì¤‘ìš”í•œ í•´ë¡œ ê°„ì£¼ëœë‹¤ [1].
*   LLMì€ ë‹¨ìˆœí•œ ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ë²—ì–´ë‚˜ ë‹¤ë‹¨ê³„ ìž‘ì—… ë¶„í•´, ê³„íš, ë„êµ¬ ì‚¬ìš© ë° ë©€í‹°ëª¨ë‹¬ ìƒí˜¸ ìž‘ìš© ê¸°ëŠ¥ì„ í†µí•©í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í™”í•˜ê³  ìžˆë‹¤ [2], [5].
*   ì£¼ìš” íŠ¸ë Œë“œë¡œëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ [2], LLM í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œë¥¼ í†µí•œ íš¨ìœ¨ì„± í–¥ìƒ [2], í”¼ë“œë°± ë° í‰ê°€ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ í’ˆì§ˆ ê²€ì¦ êµ¬í˜„ ë“±ì´ ìžˆë‹¤ [2].
*   LLMì€ ë³µìž¡ê³„ ì—°êµ¬, íŠ¹ížˆ ìƒì„±ì  ì—ì´ì „íŠ¸ ê¸°ë°˜ ëª¨ë¸ì— ì‚¬ìš©ë˜ê³  ìžˆë‹¤ [3].
*   êµ¬ì²´ì ì¸ ì‘ìš© ë¶„ì•¼ë¡œëŠ” ìž¬ë£Œ ì—´ì—­í•™ íŒŒë¼ë¯¸í„° ê²°ì •ì„ ìœ„í•œ ë¶„ìž ì—­í•™ ì½”ë“œ ìƒì„± [7]ì´ ìžˆë‹¤.
*   LLMì€ ì§€ì‹œ í•´ì„, ìž‘ì—… ê´€ë¦¬, í”¼ë“œë°±ì„ í†µí•œ ì ì‘ ë“± ì˜ì‚¬ ê²°ì • ì—ì´ì „íŠ¸ë¡œ í™œìš©ëœë‹¤ [5].
*   LLMì€ ë©€í‹° ì—ì´ì „íŠ¸ ëŒ€í™” (AutoGen), ìž¬ë£Œ ê³¼í•™ (Honeycomb), ìœ ê¸° ë°˜ë„ì²´ ìž¥ì¹˜ ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•œë‹¤ [7].
*   ê³„íš ê¸°ë°˜ ëŒ€í™” ì—ì´ì „íŠ¸ ë˜í•œ ì‘ìš© ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë‹¤ [5].
*   'LLM\_MultiAgents\_Survey\_Papers' [9] ë° 'LLM-Agents-Papers' [8]ì™€ ê°™ì€ GitHub ì €ìž¥ì†ŒëŠ” LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ ëª¨ìŒì„ ì œê³µí•œë‹¤.

## ìƒì„¸ ë¶„ì„

LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ 2024ë…„ ë“¤ì–´ ë¹„ì•½ì ì¸ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤.  ê¸°ì¡´ì˜ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì—ì„œ ë²—ì–´ë‚˜, ë³µìž¡í•œ ìž‘ì—…ì„ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ê³„íší•˜ê³ , í•„ìš”í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ë©°, ì´ë¯¸ì§€, ìŒì„± ë“± ë‹¤ì–‘í•œ í˜•íƒœì˜ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìžˆëŠ” ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸ë¡œ ì§„í™”í•˜ê³  ìžˆë‹¤ëŠ” ì ì´ ê°€ìž¥ í° íŠ¹ì§•ì´ë‹¤ [2], [5].  ì´ëŠ” LLMì´ ë‹¨ìˆœí•œ 'ì±—ë´‡' ìˆ˜ì¤€ì„ ë„˜ì–´, ì‹¤ì œ ì„¸ê³„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” 'ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸'ë¡œ ë°œë‹ì›€í•˜ê³  ìžˆìŒì„ ì‹œì‚¬í•œë‹¤.

**ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ (Multi-Step Agents):**  í•˜ë‚˜ì˜ ë³µìž¡í•œ ìž‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´, LLM ì—ì´ì „íŠ¸ê°€ ì—¬ëŸ¬ ë‹¨ê³„ì˜ í•˜ìœ„ ìž‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ê° ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ ì£¼ëª©ë°›ê³  ìžˆë‹¤ [2]. ì´ëŠ” ë§ˆì¹˜ ì‚¬ëžŒì´ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ, ë¬¸ì œ í•´ê²° ê³¼ì •ì„ ì„¸ë¶„í™”í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ë‹¤.  ì´ëŸ¬í•œ ë‹¤ë‹¨ê³„ ì ‘ê·¼ ë°©ì‹ì€ ë¬¸ì œ í•´ê²°ì˜ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.

**íš¨ìœ¨ì„± í–¥ìƒ:**  LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ì—°êµ¬ë„ í™œë°œí•˜ê²Œ ì§„í–‰ë˜ê³  ìžˆë‹¤ [2]. íŠ¹ížˆ, LLM API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ë¹„ìš© ì ˆê°ê³¼ ì‘ë‹µ ì†ë„ í–¥ìƒì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•œ ê³¼ì œì´ë‹¤.  ì—°êµ¬ìžë“¤ì€ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì„ í†µí•´ LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œë„, ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” ë° ì£¼ë ¥í•˜ê³  ìžˆë‹¤.

**í’ˆì§ˆ ê²€ì¦:**  LLM ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¬¼ì„ ì‹ ë¢°í•  ìˆ˜ ìžˆë„ë¡ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ë„ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ê³  ìžˆë‹¤ [2].  í”¼ë“œë°± ë£¨í”„ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ê°œì„ í•˜ê±°ë‚˜, ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ë°©ë²• ë“±ì´ ì—°êµ¬ë˜ê³  ìžˆë‹¤.  ì‚¬ëžŒì˜ ê°œìž… ì—†ì´ ìžë™ìœ¼ë¡œ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ê¸°ìˆ ì€ LLM ì—ì´ì „íŠ¸ì˜ ì‹¤ìš©ì„±ì„ ë†’ì´ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.

**ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼:**  LLM ì—ì´ì „íŠ¸ëŠ” ìž¬ë£Œ ê³¼í•™ [7], ë³µìž¡ê³„ ì—°êµ¬ [3], ìœ ê¸° ë°˜ë„ì²´ ìž¥ì¹˜ ì—°êµ¬ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ê³  ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìž¬ë£Œ ê³¼í•™ ë¶„ì•¼ì—ì„œëŠ” LLM ì—ì´ì „íŠ¸ê°€ ë¶„ìž ì—­í•™ ì½”ë“œë¥¼ ìƒì„±í•˜ì—¬ ìƒˆë¡œìš´ ìž¬ë£Œì˜ ì—´ì—­í•™ì  íŒŒë¼ë¯¸í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìžˆë‹¤ [7].  ì´ëŠ” LLM ì—ì´ì „íŠ¸ê°€ ê³¼í•™ ì—°êµ¬ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìžˆìŒì„ ë³´ì—¬ì¤€ë‹¤.  ë˜í•œ, AutoGen [7] ê³¼ ê°™ì€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ LLM ì—ì´ì „íŠ¸ ê°„ì˜ í˜‘ì—…ì„ í†µí•´ ë”ìš± ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆë„ë¡ í•œë‹¤.

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

*   [1] Langchain State of AI 2024: [https://blog.langchain.com/langchain-state-of-ai-2024/](https://blog.langchain.com/langchain-state-of-ai-2024/)
*   [2] LLM Trends in the Making: What to Expect in 2024: [https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6](https://medium.com/@ai_hi/llm-trends-in-the-making-what-to-expect-in-2024-915029dfd9b6)
*   [3] arXiv - Generative Agent-Based Models: [https://arxiv.org/html/2508.17281v1](https://arxiv.org/html/2508.17281v1)
*   [4] Navigating the New Types of LLM Agents and Architectures: [https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/](https://towardsdatascience.com/navigating-the-new-types-of-llm-agents-and-architectures-309382ce9f88/)
*   [5] Biggest AI Agent Paper Releases 2024: [https://juteq.ca/biggest-ai-agent-paper-releases-2024/](https://juteq.ca/biggest-ai-agent-paper-releases-2024/)
*   [6] Navigating Modern LLM Agent Architectures: [https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react](https://www.wollenlabs.com/blog-posts/navigating-modern-llm-agent-architectures-multi-agents-plan-and-execute-rewoo-tree-of-thoughts-and-react)
*   [7] LLM accelerated molecular dynamics code generation: [https://www.nature.com/articles/s41598-025-92337-6](https://www.nature.com/articles/s41598-025-92337-6)
*   [8] LLM-Agents-Papers (GitHub): [https://github.com/AGI-Edgerunners/LLM-Agents-Papers](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)
*   [9] LLM\_MultiAgents\_Survey\_Papers (GitHub): [https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers](https://github.com/taichengguo/LLM_MultiAgents_Survey_Papers)

## ê²°ë¡  ë° í‰ê°€

2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ì–‘ì , ì§ˆì ìœ¼ë¡œ ê´„ëª©í•  ë§Œí•œ ì„±ìž¥ì„ ì´ë£¨ì—ˆë‹¤. íŠ¹ížˆ, ë‹¤ë‹¨ê³„ ìž‘ì—… ì²˜ë¦¬, íš¨ìœ¨ì„± ê°œì„ , í’ˆì§ˆ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ ë„ìž…ì€ LLM ì—ì´ì „íŠ¸ì˜ ì‹¤ìš©ì„±ì„ ë†’ì´ëŠ” ë° í¬ê²Œ ê¸°ì—¬í–ˆë‹¤. ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë˜ë©´ì„œ LLM ì—ì´ì „íŠ¸ì˜ ìž ìž¬ë ¥ì´ ìž…ì¦ë˜ì—ˆì§€ë§Œ, ì—¬ì „ížˆ í•´ê²°í•´ì•¼ í•  ê³¼ì œë„ ë‚¨ì•„ìžˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ì˜ ì•ˆì „ì„±, ìœ¤ë¦¬ì  ë¬¸ì œ, ê·¸ë¦¬ê³  ì„¤ëª…ê°€ëŠ¥ì„± (Explainability)ì€ ì•žìœ¼ë¡œ ë”ìš± ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì ¸ì•¼ í•  ê²ƒì´ë‹¤.  ë˜í•œ, íŠ¹ì • ë¶„ì•¼ì— íŠ¹í™”ëœ LLM ì—ì´ì „íŠ¸ë¥¼ ê°œë°œí•˜ê³ , ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ì—°êµ¬ ë°©í–¥ì´ ë  ê²ƒì´ë‹¤.  GitHub ì €ìž¥ì†Œ [8], [9] ì™€ ê°™ì€ ìžë£Œë¥¼ í†µí•´ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ì§€ì†ì ìœ¼ë¡œ íŒŒì•…í•˜ê³ , ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— ëŒ€í•œ ì‹¤í—˜ê³¼ ê²€ì¦ì„ í†µí•´ LLM ì—ì´ì „íŠ¸ ê¸°ìˆ ì„ ë°œì „ì‹œì¼œ ë‚˜ê°€ì•¼ í•  ê²ƒì´ë‹¤.  ì¢…í•©ì ìœ¼ë¡œ ë³¼ ë•Œ, 2024ë…„ì€ LLM ì—ì´ì „íŠ¸ì˜ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê³ , ë¯¸ëž˜ ë°œì „ì„ ìœ„í•œ í† ëŒ€ë¥¼ ë‹¤ì§„ í•´ë¡œ í‰ê°€í•  ìˆ˜ ìžˆë‹¤.
======================================================================


ðŸ“Š Running benchmark: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜...

ðŸ”Ž Clarify: Analyzing query...
   â””â”€ Query: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ ê°ê° ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ì§€ ì„¤ëª…í•´ì¤˜
   â””â”€ Status: ðŸŸ¢ Clear
   â””â”€ Analysis: The user is asking for a comparison between RAG (Retrieval-Augmented Generation) and Agent-based approaches, including when each is best used. The query is clear and specific.
   â””â”€ Topics: Retrieval-Augmented Generation, Agent-based approach
ðŸ“‹ Planner: Creating research plan for: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜

ðŸ“‹ Planner: Generated 4 queries
   â””â”€ Queries:
      [1] RAG (Retrieval-Augmented Generation) vs Agent-based approach
      [2] When to use RAG vs Agent-based approach
      [3] Advantages and disadvantages of RAG
      [4] Advantages and disadvantages of Agent-based approach
   â””â”€ Focus: Key differences between RAG and Agent-based systems, Use cases for RAG, Use cases for Agent-based systems, Performance comparison of RAG and Agent-based systems

ðŸŽ¯ Supervisor [Phase 9]: Analyzing query complexity...
   â””â”€ Query: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ ê°ê° ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ì§€ ì„¤ëª…í•´ì¤˜
   â””â”€ Planned queries: 4
   â””â”€ Complexity: ðŸŸ¡ MEDIUM
   â””â”€ Recommended: 2 iterations, depth 2
   â””â”€ Strategy: targeted
   â””â”€ Reasoning: The query requires comparing two different approaches and providing guidance on when to use each. This involves understanding the nuances of both RAG and Agent-based methods, making it a medium complexity task. Two iterations with a depth of 2 are sufficient to cover the main differences and use cases.

ðŸ”¬ Research Subgraph: Starting research loop...
   â””â”€ Supervisor config: MEDIUM, max 2 iterations, targeted strategy
ðŸ” Searcher [1]: Searching for: RAG (Retrieval-Augmented Generation) vs Agent-based approach

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
      [2] https://testrigor.com/blog/rag-vs-ai-agents/
      [3] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [4] https://medium.com/olarry/understanding-the-difference-between-rag-and-ai-agents-10df56b35e02
      [5] https://developer.nvidia.com/blog/traditional-rag-vs-agentic-rag-why-ai-agents-need-dynamic-knowledge-to-get-smarter/
   â””â”€ Snippets:
      â€¢ RAG vs Agentic RAG RAG (Retrieval Augmented Generation) is a method that combines information retrieval with large language models to generate answers. Hereâ€™s how RAG works on a high level: 1 - The model retrieves relevant data from data sources and then extracts it to a vector database from the pre-indexed model. 2 - Augment the prompts by retrieving information and merging it with the query prompt. 3 - A Large Language Model (like GPT, Claude, or Gemini) understands the combined query and [...] generates the final response. A traditional RAG has a simple retrieval, limited adaptability, and relies on static knowledge, making it less flexible for dynamic and real-time information. Agentic RAG improves on this by introducing AI agents that can make decisions, select tools, and even refine queries for more accurate and flexible responses. Hereâ€™s how Agentic RAG works on a high level: 1 - The user query is directed to an AI Agent for processing. 2 - The agent uses short-term and long-term
      â€¢ As we dive deeper into this AI-driven world, we can expect these technologies to get better. From RAG, we can expect more intelligent and adaptive retrievals, enhanced context integration and processing capabilities, improved evaluation and feedback loops, and wider adoption across industries. AI agents, on the other hand, will get better at being autonomous with better perception and learning abilities. Read: When to Use AI in Test Automation: Insights from QA Experts. [...] Whatâ€™s more exciting is to see how these two converge to give us more powerful and versatile intelligent systems. We can look forward to agentic RAG, where AI agents will strategically use RAG to access and incorporate relevant information into their decision-making and action processes. These systems will be able to handle much more complex tasks, have dynamic knowledge updation techniques, and give us a more reliable, general-purpose AI. [...] Artificial intelligence (AI) has come a long way, leaving us at a position where we tend to see debates about which AI method is better. On one hand, we have the Retrieval Augmented Generation (RAG), the trusty librarian providing your AI with the right books at the right time so that your Large Language Model (LLM) doesnâ€™t confidently invent historical facts. On the other hand, we have AI Agents, the ambitious interns eager to not just read the books but also order them, alphabetize them, and
      â€¢ Before diving into comparisons, letâ€™s define each approach in clear, business-relevant terms: what they are, what they do, and where they fit in real-world AI use cases.  ### RAG (Retrieval-Augmented Generation)  RAG enhances large language models (LLMs) by letting them â€œlook things up.â€ Instead of relying only on what the model was trained on, RAG retrieves relevant documents or knowledge from an external source (like a database or vector store) before generating a response.  Use it for: [...] The table below makes it easy for you to see the trade-offs:   RAG is best for quick wins with static content.  Agentic RAG is a smart bridge when goal-based reasoning is needed.  MCP is for building durable, intelligent systems with real decision-making power. [...] What are these AI frameworks?  How do they differ in capabilities and complexity?  Which is best suited for your goals?  Whether youâ€™re leading a digital transformation initiative or simply looking to make smarter decisions with AI, this side-by-side comparison of Retrieval-Augmented Generation (RAG), Agentic RAG, and the Model Context Protocol (MCP) will help you confidently choose the right approach.  Letâ€™s break it down.  Contents  hide  1 What Are RAG, Agentic RAG, and MCP?
ðŸ’­ Think: Query: RAG (Retrieval-Augmented Generation) vs Agent-based approach | Found 5 results, 5 URLs. Key snippets: RAG vs Agentic RAG RAG (Retrieval Augmented Generation) is a method that combines information retrie | As we dive deeper into this AI-driven world, we can expect these technologies to get better. From RA | Before diving into comparisons, letâ€™s define each approach in clear, business-relevant terms: . Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://testrigor.com/blog/rag-vs-ai-agents/... (8015 chars)
ðŸ“– Read URL: https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/... (8015 chars)
ðŸ“– Read URL: https://www.linkedin.com/posts/alexxubyte_systemde... (8015 chars)
   âœ“ [https://testrigor.com/blog/rag-vs-ai-agents/]
      Preview: Retrieval Augmented Generation (RAG) vs. AI Agents - testRigor AI-Based Automated Testing Tool --> --> Youâ€™re 15 minutes away from fewer bugs and almost no test maintenance Request a Demo Now Why testRigor? AI Testing AI In Software Testing Generative AI in Software Testing Prompt Engineering in Software Testing AI Agents in Software Testing AI Features Testing AI-Based Self-Healing How Does It All Work? Features Top Features AI Features Testing No Code / Codeless Testing Web Testing Mobile Testing Desktop Testing Mainframe Testing Accessibility Testing 21 CFR Part 11 Compliance Resources Documentation Generative AI-Based Testing Certification Integrations Whitepaper Test Automation Blueprint Benefits Blog FAQ Tutorials Events Become a Partner View Recent Public Tests Training ERP Testing Acumatica Testing Infor Testing Salesforce Testing SAP ERP Testing ServiceNow Testing Workday Testing Case Studies Login Sign Up Request a Demo Turn your manual testers into automation experts! Request a Demo Table of contents: Retrieval Augmented Generation (RAG) vs. AI Agents Anushree Chatterjee AI in Testing Artificial intelligence (AI) has come a long way, leaving us at a position where we tend to see debates about which AI method is better. On one hand, we have the Retrieval Augmented Generation (RAG), the trusty librarian providing your AI with the right books at the right time so that your Large Language Model (LLM) doesn&#8217;t confidently invent historical facts. On the other hand, we have AI Agents, the ambitious interns eager to not just read the books but also order them, alphabetize them, and maybe even write their own sequels. Let&#8217;s look at these two concepts and see how they differ from each other. What is Retrieval Augmented Generation (RAG)? This technology combines two powerful AI techniques: retrieval and generation . Here&#8217;s how it works: Retrieval: First, it searches for information from a large pool of data (like documents, websites, or a knowledge base). It&#8217;s like searching for the right answer in a big library. Generation: Once it finds the relevant information, it uses that data to create a response, just like a person summarizing and explaining what they found. This is similar to how you might read an article and then explain it in your own words. So, instead of just guessing or pulling from limited knowledge, RAG gives AI systems access to a wide range of information to generate more accurate, relevant, and informed answers. Read: Impact of AI on Engineering: What the Future Holds for Engineers . Types of RAG Naive RAG (Basic RAG): This is the most straightforward implementation of RAG. It follows a simple three-step process: Step 1 &#8211; Retrieval: The user&#8217;s query is used to fetch relevant documents or chunks from an external knowledge base (e.g., a vector database) using a basic retrieval method like vector similarity search. Step 2 &#8211; Augmentation: The retrieved AI context (the relevant documents/chunks) is directly concatenated with the original user query to form an augmented prompt. Read: Prompt Engineering in QA and Software Testing . Step 3 &#8211; Generation: This augmented prompt is fed into a Large Language Model (LLM), which then generates a response based on both the retrieved information and its pre-trained knowledge. Modular RAG: Modular RAG represents an evolution of the basic RAG pipeline by breaking it down into more specialized and independent modules or components. You get greater flexibility and the ability to optimize each stage of the process individually. Advanced RAG: Advanced RAG builds upon the foundations of Naive and Modular RAG by incorporating more sophisticated techniques and often iterative processes to further enhance the quality of retrieval and generation. It aims to address the limitations of simpler RAG approaches more comprehensively. Real-Life Examples of RAG Customer Support Chatbots: Imagine you&#8217;re chatting with a support bot on a websit
   âœ“ [https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/]
      Preview: RAG vs Agentic RAG vs MCP: A 2025 Comparison Guide for Business Leaders Skip to main content Get awesome and let's build together! +1 858-683-3692 Hit enter to search or ESC to close Close Search search Menu Services Web Development Full Stack Development SaaS Development Ruby on Rails Development Node.js Development React Development Angular Development Vue.js Development eCommerce Development PHP Development Firebase Development WordPress Development WooCommerce Development Shopify Development Shopify Apps Development Mobile App Development iOS App Development Android App Development Progressive Web Application React Native App Development Hybrid App Development Flutter App Development Roku App Development Apple tvOS App Development Apple WatchOS App Development Tablet App Development Google Tv App Development UX UI Services Enterprise Digital Transformation Web Data Extraction Scraping Big Data and Business Intelligence AWS Serverless Computing Robotic Process Automation AWS Cloud Computing AI &#038; Machine Learning Development DevOps Services Cloud Computing Services Business Software Development Software Development Partner Contentful Digital Agency Chia Blockchain farming Automation &#038; AI AI Development Services Power Platform Consulting Chatbot Development GenAI Integration Services AI Consulting Services AI Development with v0 &#038; Cursor Healthcare AI Agents AI Agent Development Workflow automation Services CMS &#038; eCommerce Identity &#038; Access Management Modern Data Stack Low Code Development Industries Healthcare Wound Care Software Development EHR/EMR Software Development HMO Automation Software Advance Imaging Software Solutions Telemedicine Software Development Services RPM Software Development Healthcare Mobile App Development Medical Device Software Development Life Sciences Software Development Healthcare Web Design &#038; Development Manufacturing Fitness FinTech Non-Profit eCommerce Retail Startups Our Work Solutions Cloud Solutions Cloud Application Development Custom Solutions Digital SMS/IVR Will Call with Twilio Digital Inspection Platform Digital Lending Platform Custom Software Development Marketplace Platform IoT Development Appointment Scheduling Software HME Software Development Business Software Solutions No-Code Solutions RPA Solution CRM Integrations Integrations Payment Gateway Google Calendar Integration Google Maps Integration OAuth Authentication Twilio API Integration OKTA Integration NetSuite Integration Salesforce API Integration SugarCRM &#038; SuiteCRM Integration VGS Integration Partner Hire Us Hire Mobile App Developer Hire Ruby on Rails Developer Hire Swift Developer Hire Full Stack Developer Hire Xamarin Developer Hire PHP Developer Hire Vue.js Developer Hire Angular JS Developer Hire React Developers Hire Node.js Developer Hire DevOps Engineer Blog Jumpstart My Project search RAG vs Agentic RAG vs MCP: A 2025 Comparison Guide for Business Leaders By Raj Sanghvi June 24, 2025 AI , Automation ChatGPT Perplexity Claude Social Facebook Twitter LinkedIn WhatsApp Email No Comments For business leaders exploring AI-powered solutions, terms like RAG, Agentic RAG, and MCP are now at the center of strategy discussions. But what do they actually mean, and which one is right for your organization? This guide cuts through the technical jargon and gets straight to the point: What are these AI frameworks? How do they differ in capabilities and complexity? Which is best suited for your goals? Whether youâ€™re leading a digital transformation initiative or simply looking to make smarter decisions with AI, this side-by-side comparison of Retrieval-Augmented Generation (RAG), Agentic RAG, and the Model Context Protocol (MCP) will help you confidently choose the right approach. Letâ€™s break it down. Contents hide 1 What Are RAG, Agentic RAG, and MCP? 1.1 RAG (Retrieval-Augmented Generation) 1.2 Agentic RAG 1.3 MCP (Model Context Protocol) 2 How RAG, Agentic RAG, and MCP Differ: Side-by-Side C
   âœ“ [https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J]
      Preview: RAG vs Agentic RAG: How AI Agents Improve Answers | Alex Xu posted on the topic | LinkedIn Agree & Join LinkedIn By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement , Privacy Policy , and Cookie Policy . Skip to main content LinkedIn Top Content People Learning Jobs Games Get the app Join now Sign in RAG vs Agentic RAG: How AI Agents Improve Answers This title was summarized by AI from the post below. Alex Xu 5mo Report this post RAG vs Agentic RAG RAG (Retrieval Augmented Generation) is a method that combines information retrieval with large language models to generate answers. Hereâ€™s how RAG works on a high level: 1 - The model retrieves relevant data from data sources and then extracts it to a vector database from the pre-indexed model. 2 - Augment the prompts by retrieving information and merging it with the query prompt. 3 - A Large Language Model (like GPT, Claude, or Gemini) understands the combined query and generates the final response. A traditional RAG has a simple retrieval, limited adaptability, and relies on static knowledge, making it less flexible for dynamic and real-time information. Agentic RAG improves on this by introducing AI agents that can make decisions, select tools, and even refine queries for more accurate and flexible responses. Hereâ€™s how Agentic RAG works on a high level: 1 - The user query is directed to an AI Agent for processing. 2 - The agent uses short-term and long-term memory to track query context. It also formulates a retrieval strategy and selects appropriate tools for the job. 3 - The data fetching process can use tools such as vector search, multiple agents, and MCP servers to gather relevant data from the knowledge base. 4 - The agent then combines retrieved data with a query and system prompt. It passes this data to the LLM. 5 - LLM processes the optimized input to answer the userâ€™s query. Over to you: What else will you add to better understand RAG vs Agentic RAG? -- Subscribe to our weekly newsletter to get a Free System Design PDF (158 pages): https://bit.ly/bbg-social #systemdesign #coding #interviewtips . 9,249 377 Comments Like Comment Share Copy LinkedIn Facebook X Ritheesh T 5mo Report this comment Agentic RAG is going to be huge. Traditional RAG was already a game changer, but giving AI agents memory, decision-making, and tool orchestration takes it to the next level. It opens doors to dynamic multi-hop reasoning, real-time adaptability, and truly autonomous data workflows. Like Reply 31&nbsp;Reactions 32&nbsp;Reactions Aishwarya Srinivasan 5mo Report this comment Always great resources! CFBR Like Reply 9&nbsp;Reactions 10&nbsp;Reactions Mohit Anand 5mo Report this comment Fully agree but i think RAG is messy. Fine-tuning is expensive. And both are outdated â€” too slow, too complex, too resource-heavy. TOXO is the new hero. A smart behavior layer that sits on top of any LLM or API and converts them into C.A.L.M. (Context Augmented Language Model) No GPUs. No ML expertise. Just train it with plain English or your own data. It learns how you think â€” and feels fine-tuned without the pain. No infra. No code. Just alignment â€” in minutes. Like Reply 2&nbsp;Reactions 3&nbsp;Reactions Bajer Lin 5mo Report this comment Anyone know how these graphics are created. Its very impressive. Like Reply 1&nbsp;Reaction LEONARDO SANCHEZ NIÃ‘O 5mo Report this comment The approach you describe about RAG vs Agentic RAG is well structured and captures the key differences between both methods. Traditional RAG, while useful, has limitations in adaptability and handling dynamic information. Agentic RAG represents a natural evolution toward more intelligent systems. However, it's worth considering other modern alternatives like the Model Context Protocol (MCP). MCP offers a complementary approach. Instead of relying on static vector databases, MCP allows models to connect directly with applications and services in real-time - accessing APIs, databases, and tools more dynamica
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 1.39s
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] RAG enhances LLMs by retrieving relevant documents or knowledge from external sources (like a database or vector store) before generating a response.
      [2] AI agents can access and use dynamic knowledge through retrieval-augmented generation (RAG).
      [3] RAG combines information retrieval with large language models to generate answers by retrieving data from sources, extracting it to a vector database, and augmenting prompts.
      [4] RAG and AI Agents are different approaches to enhancing LLMs, each with its own strengths and weaknesses suitable for different tasks.
   â””â”€ Decision: More research needed
   â””â”€ Next query: RAG vs Agent detailed comparison use cases examples
ðŸ” Searcher [2]: Follow-up search for: RAG vs Agent detailed comparison use cases examples

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [2] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis
      [3] https://www.youtube.com/watch?v=Ts42JTye-AI
      [4] https://www.merge.dev/blog/rag-vs-ai-agent
      [5] https://www.multimodal.dev/post/ai-agents-vs-rag-vs-agentic-ai
   â””â”€ Snippets:
      â€¢ Of course, traditional RAG still shines in simpler use cases: a documentation Q&A bot, a single-turn FAQ answer, or anytime you have a well-defined static knowledge base and straightforward queries. The overhead of an agent might not be worth it for a question like â€œWhat are the store hours?â€ â€” a single retrieval from an FAQ vector index and answer generation is sufficient. As developers, the goal is to match the solution to the complexity of the problem. [...] user for clarification if needed, look up the userâ€™s account info via an API, and even proactively follow up. IBMâ€™s example highlights that a traditional RAG is like an employee waiting for instructions, whereas an agentic system is more like a proactive team that â€œtake initiative and solve challenges on their ownâ€. In a multi-turn workflow, the agent can decide at each turn what the best course of action is â€” whether thatâ€™s fetching more data or executing a step. This leads to more intelligent [...] Figure: An example Agentic RAG architecture (single-agent router). The agent (LLM with an â€œAgentâ€ role) sits in the loop between the user query and the LLMâ€™s final answer. It can decide among various tools or knowledge sources: e.g., performing a vector DB search on Database A vs. Database B, calling a web search API, or even invoking a calculator. The agent reasons about which tool best fits the query, uses it to retrieve or compute intermediate results, and feeds that into the generation of
      â€¢ The motivation behind agentic RAG development stems from use cases that require context-aware generation and real-time actions. Examples encompass advanced robotics, legal advisory services, healthcare diagnostics, and ongoing customer service engagements.    In these contexts, merely retrieving relevant information is insufficient. The agent must analyze the information, assess its importance, determine a response, and potentially execute an action in a continuous feedback loop. [...] For example, a robot vacuum cleaner represents a model-based reflex agent. It uses sensors to identify its position and detect obstacles while keeping an internal room map. This map helps the vacuum recall areas it has already cleaned and navigate obstacles more effectively. This way, the agent prevents unnecessary actions and enhances performance compared to a simple reflex system.    Letâ€™s consider the following image:  source [...] For example, a GPS navigation system acts as a goal-oriented agent. Users set a destination, and the agent assesses the best route based on distance, traffic, and road conditions. After selecting a path, the system provides step-by-step guidance to reach the destination.    We will consider the following diagram:  Source
      â€¢ students. Now, here the planner might use tool access to search for current solar energy statistics, recent case studies, relevant research papers and stuff like that. So, the agent here is performing document loading. It uses a search tool to find the most relevant articles. And this is where retrieval augmented generation or RAG comes in. The agent splits these documents into chunks. And those chunks get embedded into a vector database, which becomes the agent's memory for this specific task.
ðŸ’­ Think: Query: RAG vs Agent detailed comparison use cases examples | Found 5 results, 5 URLs. Key snippets: Of course, traditional RAG still shines in simpler use cases: a documentation Q&A bot, a single-turn | The motivation behind agentic RAG development stems from use cases that require context-aware genera | students. Now, here the planner might use tool access to search for current solar energy stati. Assessment: Is this sufficient or need more specific search?

ðŸ“– ContentReader: Reading 3 URLs in parallel ðŸš€
ðŸ“– Read URL: https://www.digitalocean.com/community/conceptual-... (8015 chars)
ðŸ“– Read URL: https://www.youtube.com/watch?v=Ts42JTye-AI... (645 chars)
   âœ“ [https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167]
      Preview: Error: HTTP 403 for URL: https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
   âœ“ [https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis]
      Preview: RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean Blog Docs Get Support Contact Sales DigitalOcean Products Featured Products Droplets Scalable virtual machines Kubernetes Scale more effectively Gradientâ„¢ AI Agentic Cloud Build and scale with AI Cloudways Managed cloud hosting App Platform Get apps to market faster Managed Databases Fully-managed database hosting Compute Droplets Kubernetes CPU-Optimized Droplets Functions App Platform Gradientâ„¢ AI Agentic Cloud GPU Droplets 1-Click Models Platform Bare Metal GPUs Backups &amp; Snapshots Backups Snapshots SnapShooter Networking Virtual Private Cloud (VPC) Partner Network Connect Cloud Firewalls Load Balancers DNS DDoS Protection Managed Databases MongoDB Kafka MySQL PostgreSQL Valkey OpenSearch Storage Spaces Object Storage Volume Block Storage Network File Storage Developer Tools API CLI Support Plans Monitoring Uptime Identity and Access Management Marketplace Droplet 1-Click Kubernetes 1-Click AI 1-Click Models Add-Ons Cloud Website Hosting Cloudways See all products Solutions AI and Machine Learning Develop, train, and deploy AI apps GPUs Platform 1-Click Models HR Knowledge Assistant Code Copilot Support Ticket Triage Recommendation Engine Blockchain Infrastructure for decentralized apps Blogs, Forums and Content Websites Lightning-fast, reliable CMS hosting Wordpress Ghost Mastodon Data Analytics Real-time data processing at scale Data Streaming AdTech &amp; Martech Kafka Developer Tools DevOps and CI/CD solutions CI/CD Prototyping Digital Marketing Agencies Power your clientsâ€™ websites and campaigns Freelancer IT Consulting Ecommerce Build beautiful online storefronts Dropshipping WooCommerce Magento Game Development Low-latency multiplayer servers Minecraft Hosting IoT Connect to the power of the cloud Kafka ISVs Streamlined ISV application development Secure Web Hosting Powerful protection from DDoS and more Private VPN Startup Cloud Hosting Scalable, cost-effective infrastructure Small Business Video Streaming High-bandwidth, low-latency delivery Kafka Web and Mobile Apps Simple cross-platform app hosting cPanel Docker Next.js Node.js Website Hosting Fast page loads and reliable site uptime VPS Hosting Virtual Machines Get help Migration Assistance Talk to an expert See all solutions Developers Our Community Community Home DevOps and development guides CSS-Tricks All things web design The Wave Content to level up your business. Resources Tutorials Questions and Answers Marketplace Tools Write for DOnations Cloud Chats Customer Stories DigitalOcean Blog Pricing Calculator Get Involved DigitalOcean Startups Open Source Sponsorships Hacktoberfest Deploy 2025 Wavemakers Program Documentation Quickstart Compute Gradientâ„¢ AI Platform Storage Managed Databases Containers Billing API Reference Partners DigitalOcean Partner Programs Become a Partner Partner Services Program DigitalOcean AI Partner Program Marketplace DigitalOcean Startups Connect with a Partner Partner Programs Resources Customer Stories DigitalOcean Onboarding Series Training for Agencies and Freelancers Price Estimate Calculator Featured Partner Articles Cloud cost optimization best practices Read more How to choose a cloud provider Read more DigitalOcean vs. AWS Lightsail: Which Cloud Platform is Right for You? Read more Questions? Talk to an expert Pricing Log in Log in to: Community DigitalOcean Sign up Sign up for: Community DigitalOcean Blog Docs Get Support Contact Sales Log in Log in to: Community DigitalOcean Sign up Sign up for: Community DigitalOcean Tutorials Questions Product Docs Cloud Chats Search Community Report this What is the reason for this report? This undefined is spam This undefined is offensive This undefined is off-topic This undefined is other Submit Table of contents Introduction Prerequisites Definition and Conceptual Overview of RAG Key Paradigms of RAG Modular RAG AI Agents Autonomy and Adaptability How ModelBased Reflex Agents Work Typ
   âœ“ [https://www.youtube.com/watch?v=Ts42JTye-AI]
      Preview: 10 Use Cases for AI Agents: IoT, RAG, &amp; Disaster Response Explained - YouTube About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp; Safety How YouTube works Test new features &copy; 2025 Google LLC, Sundar Pichai, 1600 Amphitheatre Parkway, Mountain View CA 94043, USA, 0807-882-594 (free), yt-support-solutions-kr@google.com, Hosted by Google LLC, Business Information , Report illegally filmed content Products shown, tagged or featured on YouTube by creators are sold by merchants and are subject to merchant's terms and conditions. YouTube does not sell these products and is not responsible for them.
   â””â”€ â±ï¸ Parallel read: 3/3 URLs in 0.86s
ðŸ”¬ Analyzer [2]: Analyzing 5 results, 6 contents

ðŸ”¬ Analyzer [2]: Analyzed 5 results, 6 contents
   â””â”€ New findings:
      [1] RAG is suitable for simpler use cases like documentation Q&A, single-turn FAQ, or when there's a well-defined static knowledge base.
      [2] Agentic RAG is better for context-aware generation and real-time actions, such as advanced robotics, legal advisory services, healthcare diagnostics, and ongoing customer service.
      [3] AI agents can incorporate RAG, but RAG itself doesnâ€™t use an AI agent.
      [4] RAG allows users to ask questions and receive helpful, accurate answers by combining relevant context with generative AI. AI agents go a step further by also taking actions on behalf of users.
   â””â”€ Decision: Research complete (8 total findings)
   â””â”€ âœ… Research Subgraph completed (execution #1)
   â””â”€ Findings: 8 items
   â””â”€ Contents: 6 URLs read

ðŸ“¦ Compress: Compressing 8 findings, 6 contents
   â””â”€ Compressed to 2221 chars (from ~5448 raw chars)
   â””â”€ Sources cited: 6
   â””â”€ Preview:
      ## Key Findings
      - Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant information from external sources [3], [4].
      - RAG combines information retrieval with LLMs to generate answers by retrieving data and augmenting prompts [4].
      - AI Agents can access dynamic knowledge through RAG [3].
      - RAG and AI Agents are distinct approaches, each suited to different tasks [3].
      - Agentic RAG is better for context-aware generation and real-time actions [6].
      
      ## Detailed Information
      - RAG enhances LLMs by retrieving relevant documents or knowledge from external sources such as databases or vector stores before generating a response [3], [4]. This allows AI agents to access and utilize dynamic knowledge [3].
      - RAG functions by combining information retrieval with LLMs to generate answers. This involves retrieving data from various sources, extracting it into a vector database, and using it to augment prompts [4].
      - While AI Agents can incorporate RAG, RAG itself does not utilize an AI agent [3]. RAG allows users to ask questions and receive accurate answers by combining relevant context with generative AI [3]. AI agents go a step further by also taking actions on behalf of users [3].
      - RAG and AI Agents represent different approaches to enhancing LLMs, each possessing unique strengths and weaknesses making them suitable for different applications [3]. RAG is well-suited for simpler use cases like documentation question answering, single-turn FAQs, or when working with a well-defined static knowledge base [3]. Agentic RAG is better for context-aware generation and real-time actions, such as advanced robotics, legal advisory services, healthcare diagnostics, and ongoing customer service [6].
      
      ## Sources
      [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [2] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis
      [3] https://testrigor.com/blog/rag-vs-ai-agents/
      [4] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
      [5] https://www.youtube.com/watch?v=Ts42JTye-AI
      [6] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/

âœï¸ Writer: Composing response from 8 findings
   â””â”€ Using compressed notes (2221 chars)

âœï¸ Writer: Generated response (2579 chars)
   â””â”€ Sources used:
      [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [2] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis
      [3] https://testrigor.com/blog/rag-vs-ai-agents/
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì •ë³´ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°˜ë©´, Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ LLMì´ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ ì¶”ê°€ì ì¸ í–‰ë™ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤ [3]. RAGëŠ” ë¹„êµì  ë‹¨ìˆœí•œ ì§ˆì˜ì‘ë‹µì— ì í•©í•˜ê³ , AgentëŠ” ë” ë³µìž¡í•˜ê³  ìƒí™© ì¸ì‹ì ì¸ ìž‘ì—…ì— ìœ ìš©í•©ë‹ˆë‹¤ [3], [6].
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      *   RAGëŠ” ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤ [3], [4].
      *   RAGëŠ” ì •ë³´ ê²€ìƒ‰ê³¼ LLMì„ ê²°í•©í•˜ì—¬ ë°ì´í„° ê²€ìƒ‰ ë° í”„ë¡¬í”„íŠ¸ ì¦ê°•ì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ [4].
      *   AI AgentëŠ” RAGë¥¼ í†µí•´ ë™ì  ì§€ì‹ì— ì ‘ê·¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [3].
      *   RAGì™€ AI AgentëŠ” LLMì„ ê°•í™”í•˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì´ë©°, ê°ìž ë‹¤ë¥¸ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤ [3].
      *   Agentic RAGëŠ” ìƒí™© ì¸ì‹ì ì¸ ìƒì„± ë° ì‹¤ì‹œê°„ ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤ [6].
      
      ## ìƒì„¸ ë¶„ì„
      
      RAG(Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ìž…ë‹ˆë‹¤. RAGëŠ” LLMì´ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ì „ì— ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë²¡í„° ìŠ¤í† ì–´ì™€ ê°™ì€ ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ë˜ëŠ” ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ [3], [4]. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ AI agentëŠ” ë™ì  ì§€ì‹ì— ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [3]. RAGëŠ” ì •ë³´ ê²€ìƒ‰ê³¼ LLMì„ ê²°í•©í•˜ì—¬ ìž‘ë™í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì¦ê°•í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤ [4]. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ RAGëŠ” ì œí’ˆ ì„¤ëª…ì„œ, ì‚¬ìš©ìž ë¦¬ë·°, FAQ ë“±ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
      
      ë°˜ë©´ AI AgentëŠ” ë‹¨ìˆœížˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ íŠ¹ì • ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ì‹œìŠ¤í…œìž…ë‹ˆë‹¤. RAGê°€ LLMì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§žì¶”ëŠ” ë°˜ë©´, AI AgentëŠ” ìžìœ¨ì ìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. RAGë¥¼ AI Agentì˜ êµ¬ì„± ìš”ì†Œë¡œ í¬í•¨í•  ìˆ˜ëŠ” ìžˆì§€ë§Œ, RAG ìžì²´ê°€ AI AgentëŠ” ì•„ë‹™ë‹ˆë‹¤ [3]. RAGëŠ” ì‚¬ìš©ìžê°€ ì§ˆë¬¸ì„ í•˜ê³  ê´€ë ¨ ë§¥ë½ê³¼ ìƒì„± AIë¥¼ ê²°í•©í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìžˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤ [3]. AI agentëŠ” ì—¬ê¸°ì„œ í•œ ê±¸ìŒ ë” ë‚˜ì•„ê°€ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ í–‰ë™ì„ ì·¨í•©ë‹ˆë‹¤ [3]. ì˜ˆë¥¼ ë“¤ì–´, AI AgentëŠ” ì‚¬ìš©ìžì˜ ì¼ì •ì— ë”°ë¼ íšŒì˜ë¥¼ ì˜ˆì•½í•˜ê±°ë‚˜, íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ ìžë™ìœ¼ë¡œ ì´ë©”ì¼ì„ ë³´ë‚¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
      
      RAGì™€ AI AgentëŠ” LLMì„ í–¥ìƒì‹œí‚¤ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ë‚˜íƒ€ë‚´ë©°, ê°ê° ê³ ìœ í•œ ê°•ì ê³¼ ì•½ì ì„ ê°€ì§€ê³  ìžˆì–´ ì„œë¡œ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•©ë‹ˆë‹¤ [3]. RAGëŠ” ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ, ë‹¨ì¼ í„´ FAQ ë˜ëŠ” ìž˜ ì •ì˜ëœ ì •ì  ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìž‘ì—…í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ê°„ë‹¨í•œ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•©ë‹ˆë‹¤ [3]. Agentic RAGëŠ” ê³ ê¸‰ ë¡œë´‡ ê³µí•™, ë²•ë¥  ìžë¬¸ ì„œë¹„ìŠ¤, ì˜ë£Œ ì§„ë‹¨ ë° ì§€ì†ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ì™€ ê°™ì€ ìƒí™© ì¸ì‹ ìƒì„± ë° ì‹¤ì‹œê°„ ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤ [6]. Agentic RAGëŠ” RAGë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì˜ì‚¬ ê²°ì • ë£¨í”„ì— ì •ë³´ ê²€ìƒ‰ ë° ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ì„ ë” ìž˜ ì´í•´í•˜ê³  ìƒí™©ì— ë§žëŠ” ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [1], [2].
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      *   [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      *   [2] https://www.digitalocean.com/community/conceptual_articles/rag-ai-agents-agentic-rag-comparative_analysis
      *   [3] https://testrigor.com/blog/rag-vs-ai-agents/
      *   [4] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
      *   [5] https://www.youtube.com/watch?v=Ts42JTye-AI
      *   [6] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      
      ## ê²°ë¡  ë° í‰ê°€
      
      RAGì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ LLMì˜ í™œìš©ë„ë¥¼ ë†’ì´ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì´ì§€ë§Œ, ì ìš© ë¶„ì•¼ì™€ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œì´ë‚˜ ì •ì ì¸ ì§€ì‹ ê¸°ë°˜ì„ í™œìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” RAGê°€ íš¨ê³¼ì ì´ë©°, ë³µìž¡í•˜ê³  ìƒí™© ì¸ì‹ì ì¸ ìž‘ì—…ì—ëŠ” Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ë” ì í•©í•©ë‹ˆë‹¤. íŠ¹ížˆ Agentic RAGëŠ” RAGì˜ ì •ë³´ ê²€ìƒ‰ ëŠ¥ë ¥ê³¼ Agentì˜ ì˜ì‚¬ ê²°ì • ëŠ¥ë ¥ì„ ê²°í•©í•˜ì—¬ ë”ìš± ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, í”„ë¡œì íŠ¸ì˜ ëª©í‘œì™€ ìš”êµ¬ ì‚¬í•­ì„ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì•žìœ¼ë¡œëŠ” RAGì™€ Agent ê¸°ìˆ ì´ ë”ìš± ë°œì „í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ LLMì˜ í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

ðŸ” Critique: CARC Quality Evaluation...
   â””â”€ CARC Scores: C=4.0 A=4.0 R=4.5 C=4.0
   â””â”€ Total: 16.5/20 âœ… Excellent
   â””â”€ Feedback: The response provides a good overview of the differences between RAG and Agent-based approaches and when to use each. It covers the main aspects of the question, including definitions, comparisons, and use cases. However, it could benefit from a slightly deeper dive into the edge cases or specific scenarios where one approach might be significantly superior to the other. Also, while the sources seem generally reliable, a more critical evaluation of their authority and potential biases could enhance the accuracy score.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 9 Supervisor Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ ê°ê° ì–¸...
â”‚ Time: 28.72s
â”‚ LLM Calls: 2
â”‚ Search Calls: 0
â”‚ URLs Read: 6
â”‚ Iterations: 2
â”‚ Est. Tokens: 1025
â”‚ Response Length: 2579 chars
â”‚ Has Citations: âœ…
â”‚ CARC Quality: C=4.0 A=4.0 R=4.5 C=4.0 â†’ 16.5/20 ðŸ‘ Good
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì •ë³´ ì†ŒìŠ¤ë¥¼ í™œìš©í•˜ì—¬ LLMì˜ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°˜ë©´, Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ LLMì´ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ ì¶”ê°€ì ì¸ í–‰ë™ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤ [3]. RAGëŠ” ë¹„êµì  ë‹¨ìˆœí•œ ì§ˆì˜ì‘ë‹µì— ì í•©í•˜ê³ , AgentëŠ” ë” ë³µìž¡í•˜ê³  ìƒí™© ì¸ì‹ì ì¸ ìž‘ì—…ì— ìœ ìš©í•©ë‹ˆë‹¤ [3], [6].

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

*   RAGëŠ” ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤ [3], [4].
*   RAGëŠ” ì •ë³´ ê²€ìƒ‰ê³¼ LLMì„ ê²°í•©í•˜ì—¬ ë°ì´í„° ê²€ìƒ‰ ë° í”„ë¡¬í”„íŠ¸ ì¦ê°•ì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ [4].
*   AI AgentëŠ” RAGë¥¼ í†µí•´ ë™ì  ì§€ì‹ì— ì ‘ê·¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [3].
*   RAGì™€ AI AgentëŠ” LLMì„ ê°•í™”í•˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì´ë©°, ê°ìž ë‹¤ë¥¸ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤ [3].
*   Agentic RAGëŠ” ìƒí™© ì¸ì‹ì ì¸ ìƒì„± ë° ì‹¤ì‹œê°„ ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤ [6].

## ìƒì„¸ ë¶„ì„

RAG(Retrieval-Augmented Generation)ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ìž…ë‹ˆë‹¤. RAGëŠ” LLMì´ ì‘ë‹µì„ ìƒì„±í•˜ê¸° ì „ì— ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë²¡í„° ìŠ¤í† ì–´ì™€ ê°™ì€ ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ë˜ëŠ” ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ [3], [4]. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ AI agentëŠ” ë™ì  ì§€ì‹ì— ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [3]. RAGëŠ” ì •ë³´ ê²€ìƒ‰ê³¼ LLMì„ ê²°í•©í•˜ì—¬ ìž‘ë™í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì¶”ì¶œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì¦ê°•í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤ [4]. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ RAGëŠ” ì œí’ˆ ì„¤ëª…ì„œ, ì‚¬ìš©ìž ë¦¬ë·°, FAQ ë“±ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„ë¥¼ ë†’ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë°˜ë©´ AI AgentëŠ” ë‹¨ìˆœížˆ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒ ì™¸ì—ë„ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ íŠ¹ì • ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ì‹œìŠ¤í…œìž…ë‹ˆë‹¤. RAGê°€ LLMì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì´ˆì ì„ ë§žì¶”ëŠ” ë°˜ë©´, AI AgentëŠ” ìžìœ¨ì ìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. RAGë¥¼ AI Agentì˜ êµ¬ì„± ìš”ì†Œë¡œ í¬í•¨í•  ìˆ˜ëŠ” ìžˆì§€ë§Œ, RAG ìžì²´ê°€ AI AgentëŠ” ì•„ë‹™ë‹ˆë‹¤ [3]. RAGëŠ” ì‚¬ìš©ìžê°€ ì§ˆë¬¸ì„ í•˜ê³  ê´€ë ¨ ë§¥ë½ê³¼ ìƒì„± AIë¥¼ ê²°í•©í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìžˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤ [3]. AI agentëŠ” ì—¬ê¸°ì„œ í•œ ê±¸ìŒ ë” ë‚˜ì•„ê°€ ì‚¬ìš©ìžë¥¼ ëŒ€ì‹ í•˜ì—¬ í–‰ë™ì„ ì·¨í•©ë‹ˆë‹¤ [3]. ì˜ˆë¥¼ ë“¤ì–´, AI AgentëŠ” ì‚¬ìš©ìžì˜ ì¼ì •ì— ë”°ë¼ íšŒì˜ë¥¼ ì˜ˆì•½í•˜ê±°ë‚˜, íŠ¹ì • ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ ìžë™ìœ¼ë¡œ ì´ë©”ì¼ì„ ë³´ë‚¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

RAGì™€ AI AgentëŠ” LLMì„ í–¥ìƒì‹œí‚¤ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ë‚˜íƒ€ë‚´ë©°, ê°ê° ê³ ìœ í•œ ê°•ì ê³¼ ì•½ì ì„ ê°€ì§€ê³  ìžˆì–´ ì„œë¡œ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•©ë‹ˆë‹¤ [3]. RAGëŠ” ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ, ë‹¨ì¼ í„´ FAQ ë˜ëŠ” ìž˜ ì •ì˜ëœ ì •ì  ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìž‘ì—…í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ê°„ë‹¨í•œ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•©ë‹ˆë‹¤ [3]. Agentic RAGëŠ” ê³ ê¸‰ ë¡œë´‡ ê³µí•™, ë²•ë¥  ìžë¬¸ ì„œë¹„ìŠ¤, ì˜ë£Œ ì§„ë‹¨ ë° ì§€ì†ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ì™€ ê°™ì€ ìƒí™© ì¸ì‹ ìƒì„± ë° ì‹¤ì‹œê°„ ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤ [6]. Agentic RAGëŠ” RAGë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì˜ì‚¬ ê²°ì • ë£¨í”„ì— ì •ë³´ ê²€ìƒ‰ ë° ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” í™˜ê²½ì„ ë” ìž˜ ì´í•´í•˜ê³  ìƒí™©ì— ë§žëŠ” ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤ [1], [2].

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

*   [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
*   [2] https://www.digitalocean.com/community/conceptual_articles/rag-ai-agents-agentic-rag-comparative_analysis
*   [3] https://testrigor.com/blog/rag-vs-ai-agents/
*   [4] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
*   [5] https://www.youtube.com/watch?v=Ts42JTye-AI
*   [6] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/

## ê²°ë¡  ë° í‰ê°€

RAGì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ LLMì˜ í™œìš©ë„ë¥¼ ë†’ì´ëŠ” íš¨ê³¼ì ì¸ ë°©ë²•ì´ì§€ë§Œ, ì ìš© ë¶„ì•¼ì™€ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì§ˆì˜ ì‘ë‹µ ì‹œìŠ¤í…œì´ë‚˜ ì •ì ì¸ ì§€ì‹ ê¸°ë°˜ì„ í™œìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” RAGê°€ íš¨ê³¼ì ì´ë©°, ë³µìž¡í•˜ê³  ìƒí™© ì¸ì‹ì ì¸ ìž‘ì—…ì—ëŠ” Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì´ ë” ì í•©í•©ë‹ˆë‹¤. íŠ¹ížˆ Agentic RAGëŠ” RAGì˜ ì •ë³´ ê²€ìƒ‰ ëŠ¥ë ¥ê³¼ Agentì˜ ì˜ì‚¬ ê²°ì • ëŠ¥ë ¥ì„ ê²°í•©í•˜ì—¬ ë”ìš± ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, í”„ë¡œì íŠ¸ì˜ ëª©í‘œì™€ ìš”êµ¬ ì‚¬í•­ì„ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì•žìœ¼ë¡œëŠ” RAGì™€ Agent ê¸°ìˆ ì´ ë”ìš± ë°œì „í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ LLMì˜ í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.
======================================================================


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Phase 9 Supervisor Summary (3 tests)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Average Time: 30.92s
â•‘  Average Tokens: 1207
â•‘  Citation Rate: 100%
â•‘  CARC Quality: C=4.0 A=3.8 R=4.5 C=4.0 â†’ 16.3/20
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ Results saved to: benchmark_results/phase_9_supervisor_20251221_015610.json
