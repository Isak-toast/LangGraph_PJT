"""
config.py - Deep Research ì„¤ì • ê´€ë¦¬
=====================================

Phase 6: Multi-LLM ì§€ì›ì„ ìœ„í•œ ì—­í• ë³„ ëª¨ë¸ ì„¤ì •

ì—­í• :
- Planner: ë¦¬ì„œì¹˜ ê³„íš ìˆ˜ë¦½ (ë¹ ë¥¸ ëª¨ë¸)
- Searcher: ê²€ìƒ‰ ì „ëµ (ê¸°ë³¸ ëª¨ë¸)
- Analyzer: ì •ë³´ ë¶„ì„ (ë¶„ì„ íŠ¹í™”)
- Writer: ìµœì¢… ì‘ë‹µ (ê³ í’ˆì§ˆ ëª¨ë¸)
- Critic: í’ˆì§ˆ í‰ê°€ (ë¶„ì„ íŠ¹í™”)
"""

import os
from dataclasses import dataclass, field
from typing import Optional, Dict
from langchain_google_genai import ChatGoogleGenerativeAI


@dataclass
class ModelConfig:
    """ê°œë³„ ëª¨ë¸ ì„¤ì •"""
    model_name: str
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    
    def to_dict(self) -> dict:
        config = {
            "model": self.model_name,
            "temperature": self.temperature,
        }
        if self.max_tokens:
            config["max_tokens"] = self.max_tokens
        return config


@dataclass
class ResearchConfig:
    """
    ì—­í• ë³„ LLM ì„¤ì •
    
    í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥:
    - PLANNER_MODEL
    - SEARCHER_MODEL
    - ANALYZER_MODEL
    - WRITER_MODEL
    - CRITIC_MODEL
    """
    
    # ê¸°ë³¸ ëª¨ë¸ (ëª¨ë“  ì—­í• ì—ì„œ fallbackìœ¼ë¡œ ì‚¬ìš©)
    default_model: str = "gemini-2.0-flash"
    
    # ì—­í• ë³„ ëª¨ë¸ ì„¤ì •
    planner: ModelConfig = field(default_factory=lambda: ModelConfig(
        model_name=os.getenv("PLANNER_MODEL", "gemini-2.0-flash"),
        temperature=0.3  # ê³„íšì€ ì¼ê´€ì„± ìˆê²Œ
    ))
    
    searcher: ModelConfig = field(default_factory=lambda: ModelConfig(
        model_name=os.getenv("SEARCHER_MODEL", "gemini-2.0-flash"),
        temperature=0.5
    ))
    
    analyzer: ModelConfig = field(default_factory=lambda: ModelConfig(
        model_name=os.getenv("ANALYZER_MODEL", "gemini-2.0-flash"),
        temperature=0.3  # ë¶„ì„ì€ ì¼ê´€ì„± ìˆê²Œ
    ))
    
    writer: ModelConfig = field(default_factory=lambda: ModelConfig(
        model_name=os.getenv("WRITER_MODEL", "gemini-2.0-flash"),
        temperature=0.7  # ê¸€ì“°ê¸°ëŠ” ì°½ì˜ì ìœ¼ë¡œ
    ))
    
    critic: ModelConfig = field(default_factory=lambda: ModelConfig(
        model_name=os.getenv("CRITIC_MODEL", "gemini-2.0-flash"),
        temperature=0.2  # ë¹„í‰ì€ ê°ê´€ì ìœ¼ë¡œ
    ))
    
    # ì—°êµ¬ ì„¤ì •
    max_research_iterations: int = 3
    max_urls_per_search: int = 5
    max_content_length: int = 4000
    
    # ë¡œê¹… ì„¤ì •
    verbose_mode: bool = field(default_factory=lambda: 
        os.getenv("VERBOSE_LOGGING", "false").lower() == "true"
    )
    
    def get_llm(self, role: str) -> ChatGoogleGenerativeAI:
        """ì—­í• ì— ë§ëŠ” LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        
        role_configs = {
            "planner": self.planner,
            "searcher": self.searcher,
            "analyzer": self.analyzer,
            "writer": self.writer,
            "critic": self.critic,
        }
        
        config = role_configs.get(role.lower(), self.planner)
        
        return ChatGoogleGenerativeAI(
            model=config.model_name,
            temperature=config.temperature,
        )
    
    def get_model_info(self) -> Dict[str, str]:
        """í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "planner": self.planner.model_name,
            "searcher": self.searcher.model_name,
            "analyzer": self.analyzer.model_name,
            "writer": self.writer.model_name,
            "critic": self.critic.model_name,
        }
    
    def print_config(self):
        """ì„¤ì • ì •ë³´ ì¶œë ¥"""
        print("\nğŸ“‹ Research Config:")
        print(f"   â””â”€ Default Model: {self.default_model}")
        print("   â””â”€ Role Models:")
        for role, model in self.get_model_info().items():
            temp = getattr(self, role).temperature
            print(f"      â€¢ {role.capitalize()}: {model} (temp={temp})")
        print(f"   â””â”€ Max Iterations: {self.max_research_iterations}")
        print(f"   â””â”€ Verbose Mode: {self.verbose_mode}")


# ê¸€ë¡œë²Œ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
research_config = ResearchConfig()
