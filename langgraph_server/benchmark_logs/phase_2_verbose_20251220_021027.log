/home/isak/LangGraph_PJT/langgraph_web_ui/langgraph_server/src/agent/tools.py:36: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.
  tavily_tool = TavilySearchResults(

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Deep Research Benchmark - Phase 2
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Test Queries: 3
â•‘  Verbose: ON (full response)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


ðŸ“Š Running benchmark: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜...
ðŸ“‹ Planner: Creating research plan for: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜...

ðŸ“‹ Planner: Generated 4 queries
   â””â”€ Queries:
      [1] LangGraph vs CrewAI multi-agent architecture
      [2] LangGraph multi-agent pros and cons
      [3] CrewAI multi-agent pros and cons
      [4] Comparison of LangGraph and CrewAI
   â””â”€ Focus: Multi-agent system architecture, LangGraph framework, CrewAI framework, Pros and cons of each framework, Scalability, Flexibility, Ease of use, Cost-effectiveness
ðŸ” Searcher [1]: Searching for: LangGraph vs CrewAI multi-agent architecture

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen
      [2] https://medium.com/@aydinKerem/which-ai-agent-framework-i-should-use-crewai-langgraph-majestic-one-and-pure-code-e16a6e4d9252
      [3] https://medium.com/@saeedhajebi/multiagent-orchestration-showdown-comparing-crewai-smolagents-and-langgraph-0e169b6a293d
      [4] https://www.3pillarglobal.com/insights/blog/comparison-crewai-langgraph-n8n/
      [5] https://developer.ibm.com/articles/awb-comparing-ai-agent-frameworks-crewai-langgraph-and-beeai/
   â””â”€ Snippets:
      â€¢ Architecture is the foundation of each framework. CrewAI follows a role-based model where agents behave like employees with specific responsibilities. This makes it easy to visualize workflows in terms of teamwork.   LangGraph, by contrast, focuses on graph-based orchestration, where workflows are represented as nodes and edges, enabling highly modular and conditional execution. [...] Each framework approaches multi-agent orchestration from a unique angle. CrewAI emphasizes role assignment, LangGraph emphasizes workflow structure, and AutoGen emphasizes conversation.   These differences affect how developers design, manage, and scale their systems, and understanding them is essential before making a choice.   Letâ€™s break down these differences across several important dimensions, starting with architecture.  ### Architectural differences [...] The significance of this comparison lies in how each framework approaches the challenge of multi-agent coordination. CrewAI adopts a role-based model inspired by real-world organizational structures, LangGraph embraces a graph-based workflow approach, and AutoGen focuses on conversational collaboration. Each framework offers unique design philosophies, strengths, and trade-offs.
      â€¢ From various sources it seems all of the frameworks have their own strong suits:  Â· For Software Development: LangGraph â€” Best suited for tasks involving code generation and complex multi-agent coding workflows.  Â· Best for Newbies: CrewAI â€” User-friendly, making it ideal for those new to multi-agent AI without complex setup requirements.  Â· Best for Complex Tasks: LangGraph â€” Offers high flexibility and is built for advanced users, allowing custom logic and orchestration. [...] Compared to crewAI, LangGraph is fairly a low-level framework. Therefore it is a bit harder to implement than crewAI. But low-level structure allows us to tweak the process and generate much more complex flows required for our work. Even though the framework is known for its ease of use, scalability, and integration with popular AI libraries like TensorFlow, PyTorch, and Keras, it has limited support for distributed systems such as Amazon or Azure.  Get Kerem AydÄ±nâ€™s stories in your inbox [...] ### 2.LangGraph  LangGraph is a framework built upon the Langchain library and uses its many functions and tools. LangGraph utilizes graphs to create a multi-agent or single agent structure. Graph represents the general flow of the architecture. LangGraph is a tool designed to visualize and manage complex relationships and workflows involving language models by creating a graph-like representation of component interactions.  Image 6  Figure 3 Example Graph of an Agent
      â€¢ For tasks requiring stable, trackable flows â€” especially in an enterprise setting â€” I found LangGraphâ€™s approach the most robust. However, it doesnâ€™t generate plans or code on the fly like SmolAgents, nor does it focus on role-based prompts like CrewAI. Itâ€™s a classic â€œorchestratorâ€ for well-defined processes. It doesnâ€™t feel like a true multi-agent system, with dynamic planning, execution, and revision of the execution plan.  ## Comparison Table [...] For workflows that require stable, trackable orchestration â€” especially with complex branching â€” I found LangGraph the most reliable. Its time-travel debugging is a major plus, though the outputs can be less verbose than CrewAIâ€™s. Additionally, if the notion of the job requires dynamic planning, itâ€™s not the right choice.  ## Final Thoughts [...] In my journey with these three frameworks, Iâ€™ve come to appreciate that no single tool fits every scenario. CrewAI excels at role-based collaboration and verbose outputs, SmolAgents shines in dynamic planning and code generation, and LangGraph offers top-notch reliability for conditional workflows and debugging, when no dynamic planning is needed.

ðŸ“– ContentReader: Reading 3 URLs...
   â””â”€ [https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen]
      Preview: Error: HTTP 403 for URL: https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen
   â””â”€ [https://medium.com/@aydinKerem/which-ai-agent-framework-i-should-use-crewai-langgraph-majestic-one-and-pure-code-e16a6e4d9252]
      Preview: Error: HTTP 403 for URL: https://medium.com/@aydinKerem/which-ai-agent-framework-i-should-use-crewai-langgraph-majestic-one-and-pure-code-e16a6e4d9252
   â””â”€ [https://medium.com/@saeedhajebi/multiagent-orchestration-showdown-comparing-crewai-smolagents-and-langgraph-0e169b6a293d]
      Preview: Error: HTTP 403 for URL: https://medium.com/@saeedhajebi/multiagent-orchestration-showdown-comparing-crewai-smolagents-and-langgraph-0e169b6a293d
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] CrewAI uses a role-based model, simplifying workflow visualization through teamwork simulation.
      [2] LangGraph uses a graph-based orchestration, which represents workflows as nodes and edges to enable modular and conditional execution.
      [3] LangGraph is suitable for code generation and complex multi-agent coding workflows.
      [4] CrewAI is user-friendly and ideal for those new to multi-agent AI.
      [5] LangGraph offers flexibility for advanced users, allowing custom logic and orchestration.
   â””â”€ Decision: More research needed
   â””â”€ Next query: LangGraph vs CrewAI architecture detailed comparison and benchmark
ðŸ” Searcher [2]: Follow-up search for: LangGraph vs CrewAI architecture detailed comparison and benchmark

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen
      [2] https://www.zams.com/blog/crewai-vs-langgraph
      [3] https://www.3pillarglobal.com/insights/blog/comparison-crewai-langgraph-n8n/
      [4] https://developer.ibm.com/articles/awb-comparing-ai-agent-frameworks-crewai-langgraph-and-beeai/
      [5] https://aaronyuqi.medium.com/first-hand-comparison-of-langgraph-crewai-and-autogen-30026e60b563
   â””â”€ Snippets:
      â€¢ Architecture is the foundation of each framework. CrewAI follows a role-based model where agents behave like employees with specific responsibilities. This makes it easy to visualize workflows in terms of teamwork.   LangGraph, by contrast, focuses on graph-based orchestration, where workflows are represented as nodes and edges, enabling highly modular and conditional execution. [...] Each framework approaches multi-agent orchestration from a unique angle. CrewAI emphasizes role assignment, LangGraph emphasizes workflow structure, and AutoGen emphasizes conversation.   These differences affect how developers design, manage, and scale their systems, and understanding them is essential before making a choice.   Letâ€™s break down these differences across several important dimensions, starting with architecture.  ### Architectural differences [...] The significance of this comparison lies in how each framework approaches the challenge of multi-agent coordination. CrewAI adopts a role-based model inspired by real-world organizational structures, LangGraph embraces a graph-based workflow approach, and AutoGen focuses on conversational collaboration. Each framework offers unique design philosophies, strengths, and trade-offs.
      â€¢ While Crewai offers a beginner-friendly and is easy-to-use, it is limited in flexibility. On the other hand, LangGraph offers great control and flexibility but is not easy to quickly set up and get going.  > Both these platforms are primarily for software development teams.  When engineering teams want to research and quickly prototype, they go for Crew. And during production, they prefer LangGraph to develop agents for complex and detailed workflows. [...] LangGraph is an open-source AI framework to develop, deploy and manage advanced workflows for generative AI agents. Using graph-based architecture LangGraph handles the relationships between different components of the AI workflow.  LangGraph applications offer robust features like scalable infrastructure and self-hosted deployment options that enhance workflow management and processing without adding overhead. [...] With Crew, you can start by creating a simple agent and gradually add more complexity and functionality as you get more comfortable with the framework.  LangGraph has a different approach where you can design graph based workflows that can be deployed in a scalable and secure environment. This visual interface simplifies the process of building and managing complex workflows.  But before pulling the trigger and making a choice, letâ€™s see how to evaluate these two.  ## Key evaluation factors
      â€¢ Below is a detailed comparison of frameworks we have evaluated through hands-on development: CrewAI, LangGraph, and n8n. To help you prioritize your projectâ€™s needs, we have evaluated each framework across a number of key criteria. (The ratings on a 1-10 scale are based on an arbitrary system where 1 signifies a feature that is badly implemented or non-existent, 5 indicates a standard or unremarkable implementation, and 10 represents a truly exceptional capability in its given category.) [...] CrewAI offers a highly flexible and modular architecture centered around role-based â€œcrewsâ€ and â€œflowsâ€. The addition of â€œflows,â€ CrewAIâ€™s programmable workflow structures, allows its core agent crew mechanic to shine while keeping the application structured and rigid. The created crews can also function on their own, sending information between themselves and acting based on predefined behaviors. It supports both sequential and hierarchical processes, with the latter leveraging an LLM-powered [...] ## CrewAI: Orchestrating Collaborative Teams  CrewAI is an open-source as well as an enterprise framework designed to create and manage teams of AI agents that collaborate to complete complex tasks efficiently. It allows different AI agents to take on specialized roles such as researchers, writers, coders, or analysts, each contributing with their expertise within a pre-defined â€œcrewâ€ structure where they can communicate.  ### Framework Flexibility and Architecture

ðŸ“– ContentReader: Reading 3 URLs...
   â””â”€ [https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen]
      Preview: Error: HTTP 403 for URL: https://www.datacamp.com/tutorial/crewai-vs-langgraph-vs-autogen
ðŸ“– Read URL: https://www.zams.com/blog/crewai-vs-langgraph... (8015 chars)
   â””â”€ [https://www.zams.com/blog/crewai-vs-langgraph]
      Preview: Crewai vs. LangGraph: Multi agent framework comparison | Zams Integrations Customers Pricing Blog Login Login Get Started Get Started Technology April 19, 2025 Crewai vs. LangGraph: Which multi agent framework should you use? Yaagneshwaran Ganesh Objective feature comparison to help you decide - based on features, benefits, and ideal use cases. While there are different ways to build an AI agent from scratch, itâ€™s great that you are taking the efficient approach of using multi agent frameworks. Youâ€™re probably here because youâ€™ve shortlisted Crew ai and LangGraph and want to decide which one is right for you. Youâ€™re in the right place. In this blog, we will compare the two in detail - on their features, benefits, and ideal use cases, including how agents connect to establish communication and interoperability. To get started, we need to have a basic understanding of multi agent systems. So, letâ€™s first get a few basics out of the way. Why do you need a multi-agent framework? As your AI systems scale and you add multiple agents with different capabilities, the complexity of these applications grow. As the complexity grows, you will need a structured environment that orchestrates the agent activities, including the technical steps and requirements involved in building agents. That is where agentic frameworks come in. Multi agent frameworks provide you with a foundational structure for developing autonomous systems, and define parameters and protocols to handle interactions between multiple specialized agents. These frameworks also incorporate agent actions, which are fundamental components within a node-based AI framework, facilitating the execution of complex tasks. An agentic application can significantly enhance user experience and system efficiency by streamlining user interaction through minimal input and adaptive responses. Single agent systems These systems are autonomous but rely on one agent to handle a wide range of tasks, like a jack of all trades. For example, hereâ€™s how it works when requested for a sales pipeline report: As you can see, one agent carries out a series of tasks to accomplish the requested outcome. Single agent systems are great for specialized tasks where the problem is well defined and the scope is limited. But as your environment and context evolves, they fall short. Multi agent systems Multi agent systems, on the contrary, consist of multiple AI agents working together to achieve common goals. Letâ€™s look at the same example of requesting to email the sales report, and see how the multi agent architecture manages specialized agents to execute it. Instead of one agent accomplishing all the tasks, the tasks are broken down into smaller components where each agent specializes in a specific task â€“ such as planning, integrating, analyzing, and more like a team of specialists working together, where each brings their unique expertise to the table. And because these AI agents can collaborate dynamically and run these tasks in parallel, they can tackle more complex problems where the environment is always changing and evolving. Multi agent interactions allow these AI agents to communicate within the system, monitor and debug in real-time, and handle handoffs efficiently, enabling dynamic workflows. The better you understand how these multi agents interact, the better you can optimize these systems and scale your operations, without worrying about bottlenecks or performance issues. And thatâ€™s exactly what multi agent frameworks help you with. Additionally, these frameworks give you a set of pre-packaged tools and features to help you quickly build any type of agent systems, be it knowledge oriented, process oriented or predictive. In short, agentic frameworks are the backbone of scalable, efficient and autonomous AI operations. With that said letâ€™s get to the comparison. What is Crewai? It is an open-source multi agent orchestration framework, that helps you build multi agent systems, and integrate them with the latest LLMs and your codebase. The framework automates multi-agent workflows, enables them to communicate and collaborate on tasks as a team, and make decisions autonomously. Their modular design includes a range of tools such as agents, tools, tasks, processes, and an agent development kit, to engage, collect information, handle complex tasks, and manage their operations through tool calls. Crewâ€™s hierarchical process generates a supervisor agent to oversee task execution and agent coordination. The agent engine facilitates the transition from prototype to production by managing various complexities including infrastructure, security, and performance evaluation, while integrating seamlessly with existing frameworks. With asynchronous tool execution, agents can work on different business processes concurrently, increasing productivity. Crew is primarily designed for research and quick prototypes. What is LangGraph framework (by LangChain)? LangGraph is an open-source AI framework to develop, deploy and manage advanced workflows for generative AI agents. Using graph-based architecture LangGraph handles the relationships between different components of the AI workflow. LangGraph applications offer robust features like scalable infrastructure and self-hosted deployment options that enhance workflow management and processing without adding overhead. Built on LangChain, a Python framework for AI applications, LangGraph excels at managing large language models. Function calling is integral to enhancing the capabilities of AI applications, particularly when combined with structured outputs and fine-tuning techniques. The LangGraph platform provides various deployment options, including self-hosted solutions, cloud SaaS, and the Bring Your Own Cloud (BYOC) option, catering to different user needs and preferences. With LangGraph Studio, a visual interface to develop workflows, users can reduce coding. One of LangGraphâ€™s key features is its state management system which acts like a memory bank, storing information and enabling better state management and debugging. Users can run LangGraph Platform entirely on their own infrastructure as part of a Self-Hosted Enterprise solution, allowing companies to maintain control over their data while outsourcing the management of some services. The integrated development environment in LangGraph Studio is specifically tailored for AI agent development, offering powerful tools for visualization, real-time interaction, and debugging, which enhances the overall development experience for users working with agent workflows. This is crucial for graph execution as it allows managing individual agents as graph nodes in the system. LangGraph supports a human-in-the-loop approach, so human input can enhance AI decision making and user interactions. LangGraph is primarily designed for engineering teams and production. Architecture and design philosophy Architecture and design of agent frameworks play a huge role in determining their effectiveness to support development of complex AI applications. A well-designed framework ideally provides a modular and flexible structure, allowing you to easily integrate various components and tools. Additionally, the framework should support seamless interaction between multiple agents, making it easy to build sophisticated workflows and applications. Crewai agents approach Crew is an agent framework that adopts a collaborative intelligence approach, helping you create multi-agent systems where specialized agents work together to achieve shared objectives. This approach is particularly useful for applications requiring dynamic task allocation, large language models, and multi-agent orchestration. LangGraph agents framework LangGraph is an agent framework that takes a state-centric approach, providing a graph-based architecture to enable creation of complex workflows and applications. This approach is ideal for applications requiring sophisticated state management, kno... [truncated]
ðŸ“– Read URL: https://www.3pillarglobal.com/insights/blog/compar... (8015 chars)
   â””â”€ [https://www.3pillarglobal.com/insights/blog/comparison-crewai-langgraph-n8n/]
      Preview: CrewAI vs LangGraph vs n8n | AI Agent Framework Comparison Skip to main content What We Do Industries Healthcare Financial Services Insurance High Tech Information Services Media Services Application Technology Strategy Chart your path to compete in a tech-forward world Data Strategy Cloud &amp; Platform Strategy Product Strategy Product Validation UX Assessment Product Development Maturity Assessment Digital Product Engineering Create unparalleled digital experiences that keep users coming back MVP Development Product Growth &amp; Scale Design Systems DevOps Quality Engineering Application Modernization Low-Code/No-Code Data &amp; Artificial Intelligence Fuel growth with intelligent data and AI solutions Data Engineering Data Analytics &amp; Insights Data Platform Modernization GenAI Strategy GenAI Application Development Why 3Pillar Why 3Pillar LightwaveÂ® The Product MindsetÂ® Innovation Lab Strategic Partners Innovation Lab Explore the latest accelerators for GenAI, modern data engineering, and platform modernization Learn more Strategic Partners 3Pillar and our partner ecosystem are ready to solve your most complex technology challenges Learn more Insights Blog Case Studies Whitepapers & eBooks Webinars The Innovation Engine Podcast HEALTHCARE Navigating Change: How Regional Payers Stay Ahead May 3, 2024 HEALTHCARE Healthcare Innovation Requires Deliberate Action, Not More Strategy April 26, 2024 About About Leadership News Leadership Our leadership team of industry innovators guide high-performing teams to deliver breakthrough results for our clients. Learn more Careers We harness the extraordinary power thatâ€™s sparked when people from all walks of life, all around the world, collaborate together. Learn more Careers Careers Opportunities Career Opportunities Our unique training and software engineering methodologies create exciting opportunities for our employees. Current job openings About 3Pillar is a modern application strategy, design & engineering firm. Learn about what makes us unique. About Contact Skip to main content What We Do Industries Healthcare Financial Services Insurance High Tech Information Services Media Services Application Technology Strategy Chart your path to compete in a tech-forward world Data Strategy Cloud &amp; Platform Strategy Product Strategy Product Validation UX Assessment Product Development Maturity Assessment Digital Product Engineering Create unparalleled digital experiences that keep users coming back MVP Development Product Growth &amp; Scale Design Systems DevOps Quality Engineering Application Modernization Low-Code/No-Code Data &amp; Artificial Intelligence Fuel growth with intelligent data and AI solutions Data Engineering Data Analytics &amp; Insights Data Platform Modernization GenAI Strategy GenAI Application Development Why 3Pillar Why 3Pillar LightwaveÂ® The Product MindsetÂ® Innovation Lab Strategic Partners Innovation Lab Explore the latest accelerators for GenAI, modern data engineering, and platform modernization Learn more Strategic Partners 3Pillar and our partner ecosystem are ready to solve your most complex technology challenges Learn more Insights Blog Case Studies Whitepapers & eBooks Webinars The Innovation Engine Podcast HEALTHCARE Navigating Change: How Regional Payers Stay Ahead May 3, 2024 HEALTHCARE Healthcare Innovation Requires Deliberate Action, Not More Strategy April 26, 2024 About About Leadership News Leadership Our leadership team of industry innovators guide high-performing teams to deliver breakthrough results for our clients. Learn more Careers We harness the extraordinary power thatâ€™s sparked when people from all walks of life, all around the world, collaborate together. Learn more Careers Careers Opportunities Career Opportunities Our unique training and software engineering methodologies create exciting opportunities for our employees. Current job openings About 3Pillar is a modern application strategy, design & engineering firm. Learn about what makes us unique. About Contact CrewAI vs LangGraph vs n8n: Comparing the Top AI Agent Frameworks for Developers October 9, 2025 --> Forget what you know about AI models. A significant shift is underway: autonomous agents that can execute complex tasks using tools and reasoning without constant human oversight. At the heart of this transformation are agentic frameworks, the foundational architectures that enable developers to orchestrate everything from large language models (LLMs) to tools and memory systems into successful AI-based applications. A significant wave of these frameworks appeared in late 2023 and early 2024, quickly gaining traction and signaling a market-wide shift towards more sophisticated AI systems. This has created a rich and varied ecosystem, with contributions from big companies like Microsoft and OpenAI, nimble startups like CrewAI, and even passionate community projects. Choosing the right framework is a crucial decision, as it will directly impact your team&#8217;s efficiency, the project&#8217;s scalability, and the long-term success of your solution. Below is a detailed comparison of frameworks we have evaluated through hands-on development: CrewAI, LangGraph, and n8n. To help you prioritize your project&#8217;s needs, we have evaluated each framework across a number of key criteria. (The ratings on a 1-10 scale are based on an arbitrary system where 1 signifies a feature that is badly implemented or non-existent, 5 indicates a standard or unremarkable implementation, and 10 represents a truly exceptional capability in its given category.) CrewAI: Orchestrating Collaborative Teams CrewAI is an open-source as well as an enterprise framework designed to create and manage teams of AI agents that collaborate to complete complex tasks efficiently. It allows different AI agents to take on specialized roles such as researchers, writers, coders, or analysts, each contributing with their expertise within a pre-defined â€œcrewâ€ structure where they can communicate. Framework Flexibility and Architecture CrewAI offers a highly flexible and modular architecture centered around role-based &#8220;crews&#8221; and &#8220;flows&#8221;. The addition of &#8220;flows,&#8221; CrewAI&#8217;s programmable workflow structures, allows its core agent crew mechanic to shine while keeping the application structured and rigid. The created crews can also function on their own, sending information between themselves and acting based on predefined behaviors. It supports both sequential and hierarchical processes, with the latter leveraging an LLM-powered manager for dynamic task delegation. This balance of structure and customizability makes creating complex agentic flows easier.&nbsp; 3Pillar rating: 8/10 Integration Capabilities CrewAI provides strong integration capabilities through its modular tool system, allowing agents to interact with external APIs, databases, and other systems. While the open-source version primarily relies on custom tool creation via Python code, its enterprise offering, CrewAI Enterprise, significantly expands this by providing a wider array of pre-built connectors for popular business applications. At the same time, the possibility of custom tools is always a necessity for more flexible applications.&nbsp; 3Pillar rating: 6/10 LLM Agnosticism &amp; Pricing Models CrewAI is highly LLM-agnostic, supporting most commercial and open-source LLMs out of the box while including the ability to integrate custom LLMs.&nbsp; It provides token usage metrics for transparency and offers max_rpm settings for rate-limiting agents to avoid API limits. The enterprise pricing is tiered rather than pay-as-you-go, the framework encourages the building of more complex tasks as it is tier based on the number of monthly executions. This means small tasks executed often can rack up very high costs while big flows running less often can keep the cost lower. The pricing also takes the number of crews and developers into account, m... [truncated]
ðŸ”¬ Analyzer [2]: Analyzing 5 results, 6 contents

ðŸ”¬ Analyzer [2]: Analyzed 5 results, 6 contents
   â””â”€ New findings:
      [1] CrewAI uses a role-based model, simplifying workflow visualization through teamwork simulation.
      [2] LangGraph uses a graph-based orchestration, which represents workflows as nodes and edges to enable modular and conditional execution.
      [3] LangGraph is suitable for code generation and complex multi-agent coding workflows.
      [4] CrewAI is user-friendly and ideal for those new to multi-agent AI.
      [5] LangGraph offers flexibility for advanced users, allowing custom logic and orchestration.
   â””â”€ Decision: More research needed
   â””â”€ Next query: CrewAI LangGraph architecture comparison examples
ðŸ” Searcher [3]: Follow-up search for: CrewAI LangGraph architecture comparison examples

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.datacamp.com/fr/tutorial/crewai-vs-langgraph-vs-autogen
      [2] https://www.zams.com/blog/crewai-vs-langgraph
      [3] https://latenode.com/blog/platform-comparisons-alternatives/automation-platform-comparisons/langgraph-vs-autogen-vs-crewai-complete-ai-agent-framework-comparison-architecture-analysis-2025
      [4] https://www.premai.io/blog/open-source-agentic-frameworks-langgraph-vs-crewai-more
      [5] https://developer.ibm.com/articles/awb-comparing-ai-agent-frameworks-crewai-langgraph-and-beeai/
   â””â”€ Snippets:
      â€¢ Architecture is the foundation of each framework. CrewAI follows a role-based model where agents behave like employees with specific responsibilities. This makes it easy to visualize workflows in terms of teamwork.   LangGraph, by contrast, focuses on graph-based orchestration, where workflows are represented as nodes and edges, enabling highly modular and conditional execution. [...] The significance of this comparison lies in how each framework approaches the challenge of multi-agent coordination. CrewAI adopts a role-based model inspired by real-world organizational structures, LangGraph embraces a graph-based workflow approach, and AutoGen focuses on conversational collaboration. Each framework offers unique design philosophies, strengths, and trade-offs. [...] Each framework approaches multi-agent orchestration from a unique angle. CrewAI emphasizes role assignment, LangGraph emphasizes workflow structure, and AutoGen emphasizes conversation.   These differences affect how developers design, manage, and scale their systems, and understanding them is essential before making a choice.   Letâ€™s break down these differences across several important dimensions, starting with architecture.  ### Architectural differences
      â€¢ While Crewai offers a beginner-friendly and is easy-to-use, it is limited in flexibility. On the other hand, LangGraph offers great control and flexibility but is not easy to quickly set up and get going.  > Both these platforms are primarily for software development teams.  When engineering teams want to research and quickly prototype, they go for Crew. And during production, they prefer LangGraph to develop agents for complex and detailed workflows. [...] LangGraph is an open-source AI framework to develop, deploy and manage advanced workflows for generative AI agents. Using graph-based architecture LangGraph handles the relationships between different components of the AI workflow.  LangGraph applications offer robust features like scalable infrastructure and self-hosted deployment options that enhance workflow management and processing without adding overhead. [...] With Crew, you can start by creating a simple agent and gradually add more complexity and functionality as you get more comfortable with the framework.  LangGraph has a different approach where you can design graph based workflows that can be deployed in a scalable and secure environment. This visual interface simplifies the process of building and managing complex workflows.  But before pulling the trigger and making a choice, letâ€™s see how to evaluate these two.  ## Key evaluation factors
      â€¢ LangGraph is an excellent choice for workflows involving multiple stages and extensive context management. Examples include document processing pipelines, research synthesis projects, or code generation workflows. However, scaling such systems requires careful attention to state persistence, execution monitoring, and concurrency management. [...] The architecture of CrewAI reflects how real teams operate, with each agent assigned a distinct role, set tasks, and coordinated workflows. Inspired by AutoGen's conversational methodology, CrewAI emphasizes the importance of role clarity for effective collaboration. Here's a practical example of how this framework might work in a research and content production team: [...] LangGraph shines with its graph-based workflows, offering precise control over processes and supporting complex system integrations. This makes it particularly effective for large-scale, intricate deployments where detailed management is key. AutoGen focuses on conversational agent interactions, providing integration options tailored to dynamic, interaction-heavy environments. Meanwhile, CrewAI prioritizes role-based team coordination, streamlining collaboration between human teams and AI

ðŸ“– ContentReader: Reading 3 URLs...
   â””â”€ [https://www.datacamp.com/fr/tutorial/crewai-vs-langgraph-vs-autogen]
      Preview: Error: HTTP 403 for URL: https://www.datacamp.com/fr/tutorial/crewai-vs-langgraph-vs-autogen
ðŸ“– Read URL: https://www.zams.com/blog/crewai-vs-langgraph... (8015 chars)
   â””â”€ [https://www.zams.com/blog/crewai-vs-langgraph]
      Preview: Crewai vs. LangGraph: Multi agent framework comparison | Zams Integrations Customers Pricing Blog Login Login Get Started Get Started Technology April 19, 2025 Crewai vs. LangGraph: Which multi agent framework should you use? Yaagneshwaran Ganesh Objective feature comparison to help you decide - based on features, benefits, and ideal use cases. While there are different ways to build an AI agent from scratch, itâ€™s great that you are taking the efficient approach of using multi agent frameworks. Youâ€™re probably here because youâ€™ve shortlisted Crew ai and LangGraph and want to decide which one is right for you. Youâ€™re in the right place. In this blog, we will compare the two in detail - on their features, benefits, and ideal use cases, including how agents connect to establish communication and interoperability. To get started, we need to have a basic understanding of multi agent systems. So, letâ€™s first get a few basics out of the way. Why do you need a multi-agent framework? As your AI systems scale and you add multiple agents with different capabilities, the complexity of these applications grow. As the complexity grows, you will need a structured environment that orchestrates the agent activities, including the technical steps and requirements involved in building agents. That is where agentic frameworks come in. Multi agent frameworks provide you with a foundational structure for developing autonomous systems, and define parameters and protocols to handle interactions between multiple specialized agents. These frameworks also incorporate agent actions, which are fundamental components within a node-based AI framework, facilitating the execution of complex tasks. An agentic application can significantly enhance user experience and system efficiency by streamlining user interaction through minimal input and adaptive responses. Single agent systems These systems are autonomous but rely on one agent to handle a wide range of tasks, like a jack of all trades. For example, hereâ€™s how it works when requested for a sales pipeline report: As you can see, one agent carries out a series of tasks to accomplish the requested outcome. Single agent systems are great for specialized tasks where the problem is well defined and the scope is limited. But as your environment and context evolves, they fall short. Multi agent systems Multi agent systems, on the contrary, consist of multiple AI agents working together to achieve common goals. Letâ€™s look at the same example of requesting to email the sales report, and see how the multi agent architecture manages specialized agents to execute it. Instead of one agent accomplishing all the tasks, the tasks are broken down into smaller components where each agent specializes in a specific task â€“ such as planning, integrating, analyzing, and more like a team of specialists working together, where each brings their unique expertise to the table. And because these AI agents can collaborate dynamically and run these tasks in parallel, they can tackle more complex problems where the environment is always changing and evolving. Multi agent interactions allow these AI agents to communicate within the system, monitor and debug in real-time, and handle handoffs efficiently, enabling dynamic workflows. The better you understand how these multi agents interact, the better you can optimize these systems and scale your operations, without worrying about bottlenecks or performance issues. And thatâ€™s exactly what multi agent frameworks help you with. Additionally, these frameworks give you a set of pre-packaged tools and features to help you quickly build any type of agent systems, be it knowledge oriented, process oriented or predictive. In short, agentic frameworks are the backbone of scalable, efficient and autonomous AI operations. With that said letâ€™s get to the comparison. What is Crewai? It is an open-source multi agent orchestration framework, that helps you build multi agent systems, and integrate them with the latest LLMs and your codebase. The framework automates multi-agent workflows, enables them to communicate and collaborate on tasks as a team, and make decisions autonomously. Their modular design includes a range of tools such as agents, tools, tasks, processes, and an agent development kit, to engage, collect information, handle complex tasks, and manage their operations through tool calls. Crewâ€™s hierarchical process generates a supervisor agent to oversee task execution and agent coordination. The agent engine facilitates the transition from prototype to production by managing various complexities including infrastructure, security, and performance evaluation, while integrating seamlessly with existing frameworks. With asynchronous tool execution, agents can work on different business processes concurrently, increasing productivity. Crew is primarily designed for research and quick prototypes. What is LangGraph framework (by LangChain)? LangGraph is an open-source AI framework to develop, deploy and manage advanced workflows for generative AI agents. Using graph-based architecture LangGraph handles the relationships between different components of the AI workflow. LangGraph applications offer robust features like scalable infrastructure and self-hosted deployment options that enhance workflow management and processing without adding overhead. Built on LangChain, a Python framework for AI applications, LangGraph excels at managing large language models. Function calling is integral to enhancing the capabilities of AI applications, particularly when combined with structured outputs and fine-tuning techniques. The LangGraph platform provides various deployment options, including self-hosted solutions, cloud SaaS, and the Bring Your Own Cloud (BYOC) option, catering to different user needs and preferences. With LangGraph Studio, a visual interface to develop workflows, users can reduce coding. One of LangGraphâ€™s key features is its state management system which acts like a memory bank, storing information and enabling better state management and debugging. Users can run LangGraph Platform entirely on their own infrastructure as part of a Self-Hosted Enterprise solution, allowing companies to maintain control over their data while outsourcing the management of some services. The integrated development environment in LangGraph Studio is specifically tailored for AI agent development, offering powerful tools for visualization, real-time interaction, and debugging, which enhances the overall development experience for users working with agent workflows. This is crucial for graph execution as it allows managing individual agents as graph nodes in the system. LangGraph supports a human-in-the-loop approach, so human input can enhance AI decision making and user interactions. LangGraph is primarily designed for engineering teams and production. Architecture and design philosophy Architecture and design of agent frameworks play a huge role in determining their effectiveness to support development of complex AI applications. A well-designed framework ideally provides a modular and flexible structure, allowing you to easily integrate various components and tools. Additionally, the framework should support seamless interaction between multiple agents, making it easy to build sophisticated workflows and applications. Crewai agents approach Crew is an agent framework that adopts a collaborative intelligence approach, helping you create multi-agent systems where specialized agents work together to achieve shared objectives. This approach is particularly useful for applications requiring dynamic task allocation, large language models, and multi-agent orchestration. LangGraph agents framework LangGraph is an agent framework that takes a state-centric approach, providing a graph-based architecture to enable creation of complex workflows and applications. This approach is ideal for applications requiring sophisticated state management, kno... [truncated]
ðŸ“– Read URL: https://latenode.com/blog/platform-comparisons-alt... (8015 chars)
   â””â”€ [https://latenode.com/blog/platform-comparisons-alternatives/automation-platform-comparisons/langgraph-vs-autogen-vs-crewai-complete-ai-agent-framework-comparison-architecture-analysis-2025]
      Preview: LangGraph vs AutoGen vs CrewAI: Complete AI Agent Framework Comparison + Architecture Analysis 2025 Are You a SaaS Founder? Embed 1,000+ Latenode integrations and AI into your SaaS. Learn more A low-code platform blending no-code simplicity with full-code power ðŸš€ Get started free Home / Blog / Platform Comparisons &amp; Alternatives / Automation Platform Comparisons / LangGraph vs AutoGen vs CrewAI: Complete AI Agent Framework Comparison + Architecture Analysis 2025 LangGraph vs AutoGen vs CrewAI: Complete AI Agent Framework Comparison + Architecture Analysis 2025 Turn ideas into automations instantly with AI Builder Prompt, create, edit, and deploy automations and AI agents in seconds Powered by Latenode AI Request history: Lorem ipsum dolor sit amet, consectetur adipiscing elit Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse varius enim in eros elementum tristique. Duis cursus, mi quis viverra ornare, eros dolor interdum nulla, ut commodo diam libero vitae erat. It&#x27;ll take a few seconds for the magic AI to create your scenario. Ready to Go Name nodes using in this scenario Open in the Workspace How it works? Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse varius enim in eros elementum tristique. Duis cursus, mi quis viverra ornare, eros dolor interdum nulla, ut commodo diam libero vitae erat. Aenean faucibus nibh et justo cursus id rutrum lorem imperdiet. Nunc ut sem vitae risus tristique posuere. Change request or modify steps below: Original request Step 1 : Application one - Open the Scenario Powered by Latenode AI Something went wrong while submitting the form. Try again later. Try again LangGraph , AutoGen , and CrewAI are frameworks for building multi-agent AI systems, each offering distinct approaches to task coordination. LangGraph uses graph-based workflows for stateful, multi-step processes. AutoGen facilitates conversational collaboration between agents, while CrewAI assigns specific roles to agents for structured task execution. Choosing the right framework depends on your project's complexity, workflow style, and scalability needs. LangGraph excels in handling workflows that require detailed state management and iterative steps. AutoGen is ideal for conversational tasks like brainstorming or customer support. CrewAI, with its role-based design, works well for sequential, clearly defined processes. For those seeking a simpler, visual alternative, Latenode enables teams to prototype and manage agent workflows with minimal coding. Itâ€™s especially useful for rapidly iterating on system designs or integrating tools like Notion or APIs for custom automation. Understanding these frameworks' strengths and limitations ensures you can align your choice with your technical requirements and business goals. Which Agentic AI Framework to Pick? LangGraph vs. CrewAI vs. AutoGen 1. LangGraph LangGraph uses a graph-based architecture to transform multi-agent coordination into a visual workflow. Nodes represent functions, while edges define execution paths, making it easier to map out complex processes. Technical Architecture and State Management At its core, LangGraph relies on directed graph structures and stateful workflows. This setup allows agents to maintain context across multiple interactions, which is particularly useful for iterative tasks like code review or multi-phase content refinement. It integrates seamlessly with existing ecosystems, letting developers create custom nodes using Python functions. These nodes are then connected through a graph structure that carries state from one step to the next. Hereâ€™s a simple example of how LangGraph manages state transitions: from langgraph.graph import StateGraph from typing import TypedDict class AgentState(TypedDict): messages: list current_step: str iteration_count: int def research_node(state: AgentState): # Agent performs research task return {&quot;messages&quot;: state[&quot;messages&quot;] + [&quot;Research completed&quot;]} def analysis_node(state: AgentState): # Agent analyzes research findings return {&quot;messages&quot;: state[&quot;messages&quot;] + [&quot;Analysis completed&quot;]} workflow = StateGraph(AgentState) workflow.add_node(&quot;research&quot;, research_node) workflow.add_node(&quot;analysis&quot;, analysis_node) workflow.add_edge(&quot;research&quot;, &quot;analysis&quot;) In this example, the framework ensures smooth transitions between nodes while preserving the state throughout the workflow. Development Experience and Learning Curve Using LangGraph requires a solid understanding of Python and some familiarity with graph theory. Developers need to grasp state management techniques, node design patterns, and how to define conditions for connecting nodes effectively. While documentation and examples are available, mastering the intricacies of graph-based workflows demands time and effort. For simpler tasks, the requirement to manage state may introduce unnecessary complexity and performance trade-offs. LangGraph shines in scenarios involving multi-step interactions, but its design might feel excessive for straightforward agent communication. Integration Capabilities and Ecosystem LangGraph also stands out for its integration capabilities. It supports custom tool connections, enabling agents to interact with APIs, databases, and various language model providers. Built-in error recovery features, such as conditional edges and retry mechanisms, add resilience. For instance, if something goes wrong, execution can be rerouted to error-handling nodes, ensuring the system remains stable even in production environments. Best-Fit Scenarios and Production Readiness LangGraph is an excellent choice for workflows involving multiple stages and extensive context management. Examples include document processing pipelines, research synthesis projects, or code generation workflows. However, scaling such systems requires careful attention to state persistence, execution monitoring, and concurrency management. For tasks that demand stateful agent collaboration and complex workflows, LangGraph provides the necessary tools. However, for simpler needs, its overhead may not be justified. In such cases, platforms like Latenode offer an alternative by delivering similar coordination capabilities with less coding effort. Next, the focus shifts to AutoGen and its unique approach to contextual multi-agent collaboration. 2. AutoGen AutoGen introduces a unique method for multi-agent coordination by structuring interactions as conversations between agents, each with defined roles and communication patterns. This conversational approach mirrors how human teams collaborate, making it an intuitive framework for dialogue-based tasks. Conversational Architecture and Agent Roles At the heart of AutoGen's design is its focus on conversational flows. Agents interact through message exchanges, with each agent embodying a specific persona and set of skills. This setup fosters seamless collaboration, akin to human team dynamics. Hereâ€™s an example of how AutoGen facilitates multi-agent conversations: import autogen config_list = [{&quot;model&quot;: &quot;gpt-4&quot;, &quot;api_key&quot;: &quot;your-api-key&quot;}] assistant = autogen.AssistantAgent( name=&quot;assistant&quot;, llm_config={&quot;config_list&quot;: config_list}, system_message=&quot;You are a helpful AI assistant.&quot; ) user_proxy = autogen.UserProxyAgent( name=&quot;user_proxy&quot;, human_input_mode=&quot;TERMINATE&quot;, code_execution_config={&quot;work_dir&quot;: &quot;coding&quot;} ) user_proxy.initiate_chat(assistant, message=&quot;Analyze this dataset and create visualizations&quot;) This code snippet highlights AutoGenâ€™s ability to maintain context through message history, eliminating the need for manual state management. Development Experience and Scalability Considerations For teams familiar with conversational AI, AutoGen offers a straightforward development experience. Its natural dialog... [truncated]
ðŸ”¬ Analyzer [3]: Analyzing 5 results, 9 contents

ðŸ”¬ Analyzer: Max iterations reached, proceeding to Writer

ðŸ”¬ Analyzer [3]: Analyzed 5 results, 9 contents
   â””â”€ New findings:
      [1] CrewAI uses a role-based model, simplifying workflow visualization through teamwork simulation.
      [2] LangGraph uses a graph-based orchestration, which represents workflows as nodes and edges to enable modular and conditional execution.
      [3] LangGraph is suitable for code generation and complex multi-agent coding workflows.
      [4] CrewAI is user-friendly and ideal for those new to multi-agent AI.
      [5] LangGraph offers flexibility for advanced users, allowing custom logic and orchestration.
   â””â”€ Decision: Research complete (28 total findings)

ðŸ“¦ Compress: Compressing 28 findings, 9 contents
   â””â”€ Compressed to 2017 chars (from ~5165 raw chars)
   â””â”€ Sources cited: 7
   â””â”€ Preview:
      ## Key Findings
      - CrewAI employs a role-based model, simplifying workflow visualization through teamwork simulation [1]. It is user-friendly and ideal for those new to multi-agent AI [1].
      - LangGraph utilizes graph-based orchestration, representing workflows as nodes and edges to enable modular and conditional execution [1]. It offers flexibility for advanced users, allowing custom logic and orchestration [1].
      - CrewAI is geared towards longer-running operations with demonstrated scalability and HitL capabilities [1].
      - LangGraph is suitable for code generation and complex multi-agent coding workflows [1].
      - LangGraph provides stable and trackable flows, especially in enterprise settings [1].
      - CrewAI features a modular architecture with clear abstraction [1].
      - CrewAI is easy to set up but may lack the ability to refine complex systems due to debugging difficulties [1].
      - CrewAI has seamless state management and memory management [1].
      
      ## Detailed Information
      
      **CrewAI:**
      
      *   CrewAI simplifies workflow visualization through a role-based model that simulates teamwork [1].
      *   It is user-friendly and suitable for individuals new to multi-agent AI [1].
      *   CrewAI is designed for longer-running operations, demonstrating scalability and Human-in-the-Loop (HitL) capabilities [1].
      *   Its architecture has a modular design with clear abstraction [1].
      *   CrewAI is easy to set up, but can be difficult to debug complex systems [1].
      *   It also offers seamless state and memory management [1].
      
      **LangGraph:**
      
      *   LangGraph uses a graph-based orchestration, representing workflows as nodes and edges, which enables modular and conditional execution [1].
      *   It offers flexibility for advanced users, allowing for custom logic and orchestration [1].
      *   LangGraph is suitable for code generation and complex multi-agent coding workflows [1].
      *   It provides stable and trackable flows, especially beneficial in enterprise environments [1].
      
      ## Sources
      
      [1] https://www.zams.com/blog/crewai-vs-langgraph

âœï¸ Writer: Composing response from 28 findings
   â””â”€ Using compressed notes (2017 chars)

âœï¸ Writer: Generated response (2323 chars)
   â””â”€ Sources used:
      [1] https://www.zams.com/blog/crewai-vs-langgraph
      [2] https://medium.com/@saeedhajebi/multiagent-orchestration-showdown-comparing-crewai-smolagents-and-langgraph-0e169b6a293d
      [3] https://www.datacamp.com/fr/tutorial/crewai-vs-langgraph-vs-autogen
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      CrewAIì™€ LangGraphëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í”„ë ˆìž„ì›Œí¬ë¡œ, CrewAIëŠ” ì—­í•  ê¸°ë°˜ ëª¨ë¸ì„ í†µí•´ íŒ€ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜ì„ ìš©ì´í•˜ê²Œ í•˜ê³ , LangGraphëŠ” ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•´ ë³µìž¡í•˜ê³  ì¡°ê±´ë¶€ ì‹¤í–‰ì´ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•œë‹¤ [1]. CrewAIëŠ” ì´ˆë³´ìžì—ê²Œ ì í•©í•˜ë©° ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ë°˜ë©´, LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžì—ê²Œ ë” í° ìœ ì—°ì„±ê³¼ ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤ [1].
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      *   **CrewAI:** ì—­í•  ê¸°ë°˜ ëª¨ë¸, ê°„íŽ¸í•œ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”, ì‚¬ìš©ìž ì¹œí™”ì , ìž¥ê¸° ì‹¤í–‰ ìž‘ì—…ì— ì í•©, í™•ìž¥ì„± ë° HitL ê¸°ëŠ¥ ì§€ì›, ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜, ì‰¬ìš´ ì„¤ì •, ì›í™œí•œ ìƒíƒœ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ [1]. ë””ë²„ê¹…ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ ë³µìž¡í•œ ì‹œìŠ¤í…œ ê°œì„ ì—ëŠ” ì–´ë ¤ì›€ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤ [1].
      *   **LangGraph:** ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, ëª¨ë“ˆí™” ë° ì¡°ê±´ë¶€ ì‹¤í–‰ ê°€ëŠ¥, ê³ ê¸‰ ì‚¬ìš©ìžë¥¼ ìœ„í•œ ìœ ì—°ì„± ë° ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥, ì½”ë“œ ìƒì„± ë° ë³µìž¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì½”ë”© ì›Œí¬í”Œë¡œìš°ì— ì í•©, ì•ˆì •ì ì´ê³  ì¶”ì  ê°€ëŠ¥í•œ íë¦„ ì œê³µ (íŠ¹ížˆ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½) [1].
      
      ## ìƒì„¸ ë¶„ì„
      
      CrewAIì™€ LangGraphëŠ” ê°ê° ë‹¤ë¥¸ ëª©í‘œì™€ ê°•ì ì„ ê°€ì§„ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• í”„ë ˆìž„ì›Œí¬ì´ë‹¤.
      
      **CrewAI:**
      
      *   **ìž¥ì :** CrewAIëŠ” íŒ€ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìžˆëŠ” ì—­í•  ê¸°ë°˜ ëª¨ë¸ì„ ì±„íƒí•˜ê³  ìžˆë‹¤ [1]. ì´ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ì²˜ìŒ ìž…ë¬¸í•˜ëŠ” ì‚¬ìš©ìžì—ê²Œ ë§¤ìš° ìœ ìš©í•˜ë©°, ì„¤ì •ì´ ê°„íŽ¸í•˜ì—¬ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ êµ¬ì¶•í•˜ê³  ì‹¤í—˜í•  ìˆ˜ ìžˆë„ë¡ ë•ëŠ”ë‹¤. CrewAIëŠ” ìž¥ê¸° ì‹¤í–‰ ìž‘ì—…ì— ìµœì í™”ë˜ì–´ ìžˆìœ¼ë©°, í™•ìž¥ì„±(Scalability)ê³¼ ì¸ê°„ ê°œìž…(Human-in-the-Loop, HitL) ê¸°ëŠ¥ì„ ì§€ì›í•˜ì—¬ ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ìœ ìš©í•˜ë‹¤ [1]. ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜ëŠ” ì½”ë“œì˜ ìž¬ì‚¬ìš©ì„±ì„ ë†’ì´ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ìš©ì´í•˜ê²Œ í•œë‹¤. ë˜í•œ, CrewAIëŠ” ìƒíƒœ ê´€ë¦¬(State Management)ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬(Memory Management)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ ìš´ì˜ì„ ë³´ìž¥í•œë‹¤ [1].
      
      *   **ë‹¨ì :** CrewAIëŠ” ë³µìž¡í•œ ì‹œìŠ¤í…œì„ ë””ë²„ê¹…í•˜ê³  ê°œì„ í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìžˆë‹¤ [1]. ì—­í•  ê¸°ë°˜ ëª¨ë¸ì˜ ë‹¨ìˆœì„±ì€ íŠ¹ì • ìˆ˜ì¤€ ì´ìƒì˜ ë³µìž¡ì„±ì„ ì²˜ë¦¬í•˜ëŠ” ë° í•œê³„ë¥¼ ê°€ì§ˆ ìˆ˜ ìžˆìœ¼ë©°, ê³ ê¸‰ ì‚¬ìš©ìžì—ê²Œ í•„ìš”í•œ ì„¸ë°€í•œ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìžˆë‹¤.
      
      **LangGraph:**
      
      *   **ìž¥ì :** LangGraphëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ë…¸ë“œì™€ ì—£ì§€ë¡œ í‘œí˜„í•˜ëŠ” ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ [1]. ì´ë¥¼ í†µí•´ ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì„±ê³¼ ì¡°ê±´ë¶€ ì‹¤í–‰ì´ ê°€ëŠ¥í•˜ë©°, ë³µìž¡í•˜ê³  ìœ ì—°í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•  ìˆ˜ ìžˆë‹¤. LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•˜ë©°, ì‚¬ìš©ìž ì •ì˜ ë¡œì§ê³¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•´ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë§žì¶˜ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•œë‹¤ [1]. ì½”ë“œ ìƒì„± ë° ë³µìž¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì½”ë”© ì›Œí¬í”Œë¡œìš°ì— íŠ¹ížˆ ì í•©í•˜ë©° [1], ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ìš”êµ¬ë˜ëŠ” ì•ˆì •ì ì´ê³  ì¶”ì  ê°€ëŠ¥í•œ íë¦„ì„ ì œê³µí•œë‹¤ [1].
      
      *   **ë‹¨ì :** LangGraphëŠ” CrewAIì— ë¹„í•´ í•™ìŠµ ê³¡ì„ ì´ ê°€íŒŒë¥´ë©°, ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•˜ë‹¤. ì´ˆë³´ìžì—ê²ŒëŠ” ë³µìž¡í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìžˆìœ¼ë©°, ì´ˆê¸° ì„¤ì • ë° êµ¬ì„±ì— ë” ë§Žì€ ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•  ìˆ˜ ìžˆë‹¤.
      
      **ì„ íƒ ê°€ì´ë“œ:**
      
      CrewAIëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ëŒ€í•œ ê²½í—˜ì´ ì—†ëŠ” ì‚¬ìš©ìžê°€ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ ë§Œë“¤ê³  ê¸°ë³¸ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í—˜í•˜ëŠ” ë° ì í•©í•˜ë‹¤. ë°˜ë©´, LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžê°€ ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•˜ê³ , ì„¸ë°€í•œ ì œì–´ì™€ ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë§žëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° ì í•©í•˜ë‹¤. ì–´ë–¤ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí• ì§€ëŠ” í”„ë¡œì íŠ¸ì˜ ë³µìž¡ì„±, íŒ€ì˜ ê¸°ìˆ  ìˆ˜ì¤€, ê·¸ë¦¬ê³  í•„ìš”í•œ ìœ ì—°ì„± ë° ì‚¬ìš©ìž ì •ì˜ ìˆ˜ì¤€ì— ë”°ë¼ ê²°ì •í•´ì•¼ í•œë‹¤.
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      *   [1] https://www.zams.com/blog/crewai-vs-langgraph
      
      ## ê²°ë¡  ë° í‰ê°€
      
      CrewAIì™€ LangGraphëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í›Œë¥­í•œ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, ì„œë¡œ ë‹¤ë¥¸ ê°•ì ê³¼ ì•½ì ì„ ê°€ì§€ê³  ìžˆë‹¤. CrewAIëŠ” ì ‘ê·¼ì„±ê³¼ ì‚¬ìš© íŽ¸ì˜ì„±ì´ ë›°ì–´ë‚˜ì§€ë§Œ, LangGraphëŠ” ìœ ì—°ì„±ê³¼ ì‚¬ìš©ìž ì •ì˜ ì¸¡ë©´ì—ì„œ ë” ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. ë”°ë¼ì„œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ íŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì•žìœ¼ë¡œ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•¨ì— ë”°ë¼, CrewAIì™€ LangGraphëŠ” ì§€ì†ì ìœ¼ë¡œ ë°œì „í•˜ê³  ê²½ìŸí•˜ë©°, ë”ìš± ê°•ë ¥í•˜ê³  ìœ ì—°í•œ ê¸°ëŠ¥ì„ ì œê³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 2 Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: LangGraphì™€ CrewAIì˜ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ë¥¼ ë¹„êµí•˜ê³  ìž¥ë‹¨ì ì„ ë¶„ì„í•´ì¤˜...
â”‚ Time: 29.15s
â”‚ LLM Calls: 2
â”‚ Search Calls: 3
â”‚ URLs Read: 9
â”‚ Iterations: 3
â”‚ Est. Tokens: 995
â”‚ Response Length: 2323 chars
â”‚ Has Citations: âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

CrewAIì™€ LangGraphëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í”„ë ˆìž„ì›Œí¬ë¡œ, CrewAIëŠ” ì—­í•  ê¸°ë°˜ ëª¨ë¸ì„ í†µí•´ íŒ€ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜ì„ ìš©ì´í•˜ê²Œ í•˜ê³ , LangGraphëŠ” ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•´ ë³µìž¡í•˜ê³  ì¡°ê±´ë¶€ ì‹¤í–‰ì´ ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•œë‹¤ [1]. CrewAIëŠ” ì´ˆë³´ìžì—ê²Œ ì í•©í•˜ë©° ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ë°˜ë©´, LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžì—ê²Œ ë” í° ìœ ì—°ì„±ê³¼ ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤ [1].

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

*   **CrewAI:** ì—­í•  ê¸°ë°˜ ëª¨ë¸, ê°„íŽ¸í•œ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”, ì‚¬ìš©ìž ì¹œí™”ì , ìž¥ê¸° ì‹¤í–‰ ìž‘ì—…ì— ì í•©, í™•ìž¥ì„± ë° HitL ê¸°ëŠ¥ ì§€ì›, ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜, ì‰¬ìš´ ì„¤ì •, ì›í™œí•œ ìƒíƒœ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ [1]. ë””ë²„ê¹…ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ ë³µìž¡í•œ ì‹œìŠ¤í…œ ê°œì„ ì—ëŠ” ì–´ë ¤ì›€ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤ [1].
*   **LangGraph:** ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜, ëª¨ë“ˆí™” ë° ì¡°ê±´ë¶€ ì‹¤í–‰ ê°€ëŠ¥, ê³ ê¸‰ ì‚¬ìš©ìžë¥¼ ìœ„í•œ ìœ ì—°ì„± ë° ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥, ì½”ë“œ ìƒì„± ë° ë³µìž¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì½”ë”© ì›Œí¬í”Œë¡œìš°ì— ì í•©, ì•ˆì •ì ì´ê³  ì¶”ì  ê°€ëŠ¥í•œ íë¦„ ì œê³µ (íŠ¹ížˆ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½) [1].

## ìƒì„¸ ë¶„ì„

CrewAIì™€ LangGraphëŠ” ê°ê° ë‹¤ë¥¸ ëª©í‘œì™€ ê°•ì ì„ ê°€ì§„ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• í”„ë ˆìž„ì›Œí¬ì´ë‹¤.

**CrewAI:**

*   **ìž¥ì :** CrewAIëŠ” íŒ€ì›Œí¬ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìžˆëŠ” ì—­í•  ê¸°ë°˜ ëª¨ë¸ì„ ì±„íƒí•˜ê³  ìžˆë‹¤ [1]. ì´ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ì²˜ìŒ ìž…ë¬¸í•˜ëŠ” ì‚¬ìš©ìžì—ê²Œ ë§¤ìš° ìœ ìš©í•˜ë©°, ì„¤ì •ì´ ê°„íŽ¸í•˜ì—¬ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ êµ¬ì¶•í•˜ê³  ì‹¤í—˜í•  ìˆ˜ ìžˆë„ë¡ ë•ëŠ”ë‹¤. CrewAIëŠ” ìž¥ê¸° ì‹¤í–‰ ìž‘ì—…ì— ìµœì í™”ë˜ì–´ ìžˆìœ¼ë©°, í™•ìž¥ì„±(Scalability)ê³¼ ì¸ê°„ ê°œìž…(Human-in-the-Loop, HitL) ê¸°ëŠ¥ì„ ì§€ì›í•˜ì—¬ ì‹¤ì œ ë¬¸ì œ í•´ê²°ì— ìœ ìš©í•˜ë‹¤ [1]. ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜ëŠ” ì½”ë“œì˜ ìž¬ì‚¬ìš©ì„±ì„ ë†’ì´ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ìš©ì´í•˜ê²Œ í•œë‹¤. ë˜í•œ, CrewAIëŠ” ìƒíƒœ ê´€ë¦¬(State Management)ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬(Memory Management)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ ìš´ì˜ì„ ë³´ìž¥í•œë‹¤ [1].

*   **ë‹¨ì :** CrewAIëŠ” ë³µìž¡í•œ ì‹œìŠ¤í…œì„ ë””ë²„ê¹…í•˜ê³  ê°œì„ í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìžˆë‹¤ [1]. ì—­í•  ê¸°ë°˜ ëª¨ë¸ì˜ ë‹¨ìˆœì„±ì€ íŠ¹ì • ìˆ˜ì¤€ ì´ìƒì˜ ë³µìž¡ì„±ì„ ì²˜ë¦¬í•˜ëŠ” ë° í•œê³„ë¥¼ ê°€ì§ˆ ìˆ˜ ìžˆìœ¼ë©°, ê³ ê¸‰ ì‚¬ìš©ìžì—ê²Œ í•„ìš”í•œ ì„¸ë°€í•œ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ëª»í•  ìˆ˜ ìžˆë‹¤.

**LangGraph:**

*   **ìž¥ì :** LangGraphëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ë…¸ë“œì™€ ì—£ì§€ë¡œ í‘œí˜„í•˜ëŠ” ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ [1]. ì´ë¥¼ í†µí•´ ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì„±ê³¼ ì¡°ê±´ë¶€ ì‹¤í–‰ì´ ê°€ëŠ¥í•˜ë©°, ë³µìž¡í•˜ê³  ìœ ì—°í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•  ìˆ˜ ìžˆë‹¤. LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí•˜ë©°, ì‚¬ìš©ìž ì •ì˜ ë¡œì§ê³¼ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•´ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë§žì¶˜ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•œë‹¤ [1]. ì½”ë“œ ìƒì„± ë° ë³µìž¡í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì½”ë”© ì›Œí¬í”Œë¡œìš°ì— íŠ¹ížˆ ì í•©í•˜ë©° [1], ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ìš”êµ¬ë˜ëŠ” ì•ˆì •ì ì´ê³  ì¶”ì  ê°€ëŠ¥í•œ íë¦„ì„ ì œê³µí•œë‹¤ [1].

*   **ë‹¨ì :** LangGraphëŠ” CrewAIì— ë¹„í•´ í•™ìŠµ ê³¡ì„ ì´ ê°€íŒŒë¥´ë©°, ê·¸ëž˜í”„ ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•˜ë‹¤. ì´ˆë³´ìžì—ê²ŒëŠ” ë³µìž¡í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ ìžˆìœ¼ë©°, ì´ˆê¸° ì„¤ì • ë° êµ¬ì„±ì— ë” ë§Žì€ ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•  ìˆ˜ ìžˆë‹¤.

**ì„ íƒ ê°€ì´ë“œ:**

CrewAIëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ëŒ€í•œ ê²½í—˜ì´ ì—†ëŠ” ì‚¬ìš©ìžê°€ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ìž…ì„ ë§Œë“¤ê³  ê¸°ë³¸ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í—˜í•˜ëŠ” ë° ì í•©í•˜ë‹¤. ë°˜ë©´, LangGraphëŠ” ê³ ê¸‰ ì‚¬ìš©ìžê°€ ë³µìž¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•˜ê³ , ì„¸ë°€í•œ ì œì–´ì™€ ì‚¬ìš©ìž ì •ì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë§žëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° ì í•©í•˜ë‹¤. ì–´ë–¤ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí• ì§€ëŠ” í”„ë¡œì íŠ¸ì˜ ë³µìž¡ì„±, íŒ€ì˜ ê¸°ìˆ  ìˆ˜ì¤€, ê·¸ë¦¬ê³  í•„ìš”í•œ ìœ ì—°ì„± ë° ì‚¬ìš©ìž ì •ì˜ ìˆ˜ì¤€ì— ë”°ë¼ ê²°ì •í•´ì•¼ í•œë‹¤.

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

*   [1] https://www.zams.com/blog/crewai-vs-langgraph

## ê²°ë¡  ë° í‰ê°€

CrewAIì™€ LangGraphëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ ìœ„í•œ í›Œë¥­í•œ í”„ë ˆìž„ì›Œí¬ì´ì§€ë§Œ, ì„œë¡œ ë‹¤ë¥¸ ê°•ì ê³¼ ì•½ì ì„ ê°€ì§€ê³  ìžˆë‹¤. CrewAIëŠ” ì ‘ê·¼ì„±ê³¼ ì‚¬ìš© íŽ¸ì˜ì„±ì´ ë›°ì–´ë‚˜ì§€ë§Œ, LangGraphëŠ” ìœ ì—°ì„±ê³¼ ì‚¬ìš©ìž ì •ì˜ ì¸¡ë©´ì—ì„œ ë” ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. ë”°ë¼ì„œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ íŒ€ì˜ ìˆ™ë ¨ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í”„ë ˆìž„ì›Œí¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì•žìœ¼ë¡œ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì— ëŒ€í•œ ìˆ˜ìš”ê°€ ì¦ê°€í•¨ì— ë”°ë¼, CrewAIì™€ LangGraphëŠ” ì§€ì†ì ìœ¼ë¡œ ë°œì „í•˜ê³  ê²½ìŸí•˜ë©°, ë”ìš± ê°•ë ¥í•˜ê³  ìœ ì—°í•œ ê¸°ëŠ¥ì„ ì œê³µí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.
======================================================================


ðŸ“Š Running benchmark: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´...
ðŸ“‹ Planner: Creating research plan for: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´...

ðŸ“‹ Planner: Generated 4 queries
   â””â”€ Queries:
      [1] LLM based agent system papers 2024
      [2] Large Language Model agent trends 2024
      [3] 2024 LLM agent architecture research
      [4] Recent advances in LLM agents 2024
   â””â”€ Focus: Agent architecture, Training methodologies, Applications of LLM agents, Performance evaluation
ðŸ” Searcher [1]: Searching for: LLM based agent system papers 2024

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://github.com/AGI-Edgerunners/LLM-Agents-Papers
      [2] https://arxiv.org/abs/2402.01680
      [3] https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf
      [4] https://papers.nips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138-Abstract-Conference.html
      [5] https://pmc.ncbi.nlm.nih.gov/articles/PMC12492978/
   â””â”€ Snippets:
      â€¢ [2024/08/12] Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models | ( | [code]     [2024/08/01] AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation | ( | [code]     [2024/07/04] Controllable Conversations: Planning-Based Dialogue Agent with Large Language Models | ( | [code]     [2024/06/17] RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents | ( | [code] [...] [2024/10/17] MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling | ( | [code]     [2024/10/16] MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration | ( | [code]     [2024/10/02] Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics | ( | [code]     [2024/08/28] Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions | ( | [code] [...] [2024/03/11] Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation | ( | [code]     [2024/03/10] TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision | ( | [code]     [2024/03/05] KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents | ( | [code]     [2024/02/29] PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval | ( | [code]
      â€¢ From: Taicheng Guo [view email]     (/abs/2402.01680v1) Sun, 21 Jan 2024 23:36:14 UTC (5,000 KB)    [v2] Fri, 19 Apr 2024 01:15:16 UTC (5,001 KB)  Full-text links:  ## Access Paper:  View a PDF of the paper titled Large Language Model based Multi-Agents: A Survey of Progress and Challenges, by Taicheng Guo and 7 other authors   View PDF  HTML (experimental)  TeX Source  view license  Current browse context:  cs.CL  < prev")    |    next >")  new  |  recent  | 2024-02  Change to browse by: [...] We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate  > cs > arXiv:2402.01680  # Computer Science > Computation and Language  arXiv:2402.01680 (cs)  Submitted on 21 Jan 2024 ([v1), last revised 19 Apr 2024 (this version, v2)]  # Title:Large Language Model based Multi-Agents: A Survey of Progress and Challenges  Authors:Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang [...] > Abstract:Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of
      â€¢ Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. DÃ³lares or dollars? unraveling the bilingual prowess of financial llms between spanish and english. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD â€™24, page 6236â€“6246, New York, NY, USA, 2024. Association for Computing Machinery. [...] Molei Qin, Shuo Sun, Wentao Zhang, Haochong Xia, Xinrun Wang, and Bo An. Earnhft: Efficient hierarchical reinforcement learning for high frequency trading. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14669â€“14676, 2024.  Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022. [...] Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024.  Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.

ðŸ“– ContentReader: Reading 3 URLs...
ðŸ“– Read URL: https://github.com/AGI-Edgerunners/LLM-Agents-Pape... (8015 chars)
   â””â”€ [https://github.com/AGI-Edgerunners/LLM-Agents-Papers]
      Preview: GitHub - AGI-Edgerunners/LLM-Agents-Papers: A repo lists papers related to LLM based agent Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform AI CODE CREATION GitHub Copilot Write better code with AI GitHub Spark Build and deploy intelligent apps GitHub Models Manage and compare prompts MCP Registry New Integrate external tools DEVELOPER WORKFLOWS Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes APPLICATION SECURITY GitHub Advanced Security Find and fix vulnerabilities Code security Secure your code as you build Secret protection Stop leaks before they start EXPLORE Why GitHub Documentation Blog Changelog Marketplace View all features Solutions BY COMPANY SIZE Enterprises Small and medium teams Startups Nonprofits BY USE CASE App Modernization DevSecOps DevOps CI/CD View all use cases BY INDUSTRY Healthcare Financial services Manufacturing Government View all industries View all solutions Resources EXPLORE BY TOPIC AI Software Development DevOps Security View all topics EXPLORE BY TYPE Customer stories Events &amp; webinars Ebooks &amp; reports Business insights GitHub Skills SUPPORT &amp; SERVICES Documentation Customer support Community forum Trust center Partners Open Source COMMUNITY GitHub Sponsors Fund open source developers PROGRAMS Security Lab Maintainer Community Accelerator Archive Program REPOSITORIES Topics Trending Collections Enterprise ENTERPRISE SOLUTIONS Enterprise platform AI-powered developer platform AVAILABLE ADD-ONS GitHub Advanced Security Enterprise-grade security features Copilot for Business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} AGI-Edgerunners / LLM-Agents-Papers Public Notifications You must be signed in to change notification settings Fork 133 Star 2.2k A repo lists papers related to LLM based agent 2.2k stars 133 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Issues 1 Pull requests 4 Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights AGI-Edgerunners/LLM-Agents-Papers main Branches Tags Go to file Code Open more actions menu Folders and files Name Name Last commit message Last commit date Latest commit History 164 Commits parsed_v5 parsed_v5 .gitignore .gitignore README.md README.md config_v5.json config_v5.json download_pdf.py download_pdf.py papers_v5.json papers_v5.json script_v5_step1.py script_v5_step1.py script_v5_step2.py script_v5_step2.py View all files Repository files navigation README LLM-Agents-Papers âœï¸ Description Last Updated Time: 2025/7/12 A repo lists papers related to LLM based agent. Includes Survey Technique For Enhancement Planning Memory Mechanism Feedback&amp;Reflection RAG Search Interaction Role Playing Conversation Game Playing Human-Agent Interaction Tool Usage Simulation Application Math Chemistry Biology Physics Geography Art Medicine Finance Software Engineering Research Automation Workflow Automatic Evaluation Training Fine tuning RL DPO Scaling Single-Agent Framework Multi-Agent System Stability Safety Bias Hallucination Infrastructure Benchmark&amp;Evaluation Environment&amp;Platform Dataset Others ðŸ’› Recommendation For more comprehensive reading, we also recommend other paper lists: zjunlp/LLMAgentPapers : Must-read Papers on Large Language Model Agents. teacherpeterpan/self-correction-llm-papers : This is a collection of research papers for Self-Correcting Large Language Models with Automated Feedback. Paitesanshi/LLM-Agent-Survey : A Survey on LLM-based Autonomous Agents. woooodyy/llm-agent-paper-list : Must-read papers for LLM-based agents. git-disl/awesome-LLM-game-agent-papers : Must-read papers for LLM-based Game agents. ðŸ“° Papers Survey [2025/06/10] Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents | [paper] | [code] [2025/06/06] Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey | [paper] | [code] [2025/05/27] Creativity in LLM-based Multi-Agent Systems: A Survey | [paper] | [code] [2025/05/24] Multi-Party Conversational Agents: A Survey | [paper] | [code] [2025/05/16] A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron? | [paper] | [code] [2025/05/02] AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains | [paper] | [code] [2025/05/01] A Survey on Large Language Model based Human-Agent Systems | [paper] | [code] [2025/04/30] Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications | [paper] | [code] [2025/04/22] A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment | [paper] | [code] [2025/04/20] Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey | [paper] | [code] [2025/04/14] A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science | [paper] | [code] [2025/04/12] A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems | [paper] | [code] [2025/03/28] Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey | [paper] | [code] [2025/03/27] Large Language Model Agent: A Survey on Methodology, Applications and Challenges | [paper] | [code] [2025/03/27] A Survey on (M)LLM-Based GUI Agents | [paper] | [code] [2025/03/24] A Survey of Large Language Model Agents for Question Answering | [paper] | [code] [2025/03/20] Survey on Evaluation of LLM-based Agents | [paper] | [code] [2025/03/13] LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems | [paper] | [code] [2025/03/12] Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions | [paper] | [code] [2025/02/20] Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems | [paper] | [code] [2025/02/18] Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents | [paper] | [code] [2025/02/16] A Survey of LLM-based Agents in Medicine: How far are we from Baymax? | [paper] | [code] [2025/01/15] Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG | [paper] | [code] [2024/12/23] A Survey on LLM-based Multi-Agent System: Recent Advances and New Frontiers in Application | [paper] | [code] [2024/12/18] A Survey on Large Language Model-based Agents for Statistics and Data Science | [paper] | [code] [2024/12/05] A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios | [paper] | [code] [2024/12/04] From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents | [paper] | [code] [2024/11/27] Large Language Model-Brained GUI Agents: A Survey | [paper] | [code] [2024/09/27] A Survey on Complex Tasks for Goal-Directed Interactive Agents | [paper] | [... [truncated]
ðŸ“– Read URL: https://arxiv.org/abs/2402.01680... (5331 chars)
   â””â”€ [https://arxiv.org/abs/2402.01680]
      Preview: [2402.01680] Large Language Model based Multi-Agents: A Survey of Progress and Challenges Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.01680 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2402.01680 (cs) [Submitted on 21 Jan 2024 ( v1 ), last revised 19 Apr 2024 (this version, v2)] Title: Large Language Model based Multi-Agents: A Survey of Progress and Challenges Authors: Taicheng Guo , Xiuying Chen , Yaqi Wang , Ruidi Chang , Shichao Pei , Nitesh V. Chawla , Olaf Wiest , Xiangliang Zhang View a PDF of the paper titled Large Language Model based Multi-Agents: A Survey of Progress and Challenges, by Taicheng Guo and 7 other authors View PDF HTML (experimental) Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents&#39; capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems. Comments: This work is ongoing and we welcome your contribution! Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA) Cite as: arXiv:2402.01680 [cs.CL] &nbsp; (or arXiv:2402.01680v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.01680 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Taicheng Guo [ view email ] [v1] Sun, 21 Jan 2024 23:36:14 UTC (5,000 KB) [v2] Fri, 19 Apr 2024 01:15:16 UTC (5,001 KB) Full-text links: Access Paper: View a PDF of the paper titled Large Language Model based Multi-Agents: A Survey of Progress and Challenges, by Taicheng Guo and 7 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs cs.AI cs.MA References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status
ðŸ“– Read URL: https://proceedings.neurips.cc/paper_files/paper/2... (8015 chars)
   â””â”€ [https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf]
      Preview: %PDF-1.5 %ï¿½ï¿½ï¿½ï¿½ 660 0 obj > endobj 661 0 obj > /W [ 1 3 1 ] /Index [ 660 305 ] /Info 164 0 R /Root 662 0 R /Size 965 /Prev 15275817 /ID [ ] >> stream xï¿½cbd`ï¿½g`b``8 "ï¿½ï¿½Ù ï¿½ï¿½ ï¿½d > endobj 663 0 obj > stream xï¿½c```b`ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ `6+ï¿½ï¿½ï¿½ï¿½!ï¿½ï¿½ï¿½ ï¿½8ï¿½ï¿½ï¿½$ b2É¬ecï¿½ï¿½ï¿½ï¿½~Òƒï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½k Aï¿½ï¿½ ï¿½8ï¿½8b8,8f20u0pï¿½kï¿½c`ï¿½ï¿½hï¿½#Rï¿½'b ï¿½ï¿½ï¿½T;ï¿½eï¿½v9ï¿½5 ï¿½ï¿½ï¿½ï¿½g1ï¿½ï¿½1ï¿½ï¿½10f` c`ï¿½e`fï¿½ï¿½ï¿½vrTØ”ï¿½ï¿½nA[ ï¿½ï¿½fï¿½Ú¨4ï¿½ï¿½Ó CUï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½Bï¿½isï¿½ï¿½ï¿½ ï¿½ï¿½_ï¿½ï¿½sUï¿½ï¿½ÂœmÎ…/Xï¿½2Oï¿½ï¿½ï¿½rï¿½Sï¿½d9ï¿½Ô”ï¿½ï¿½ï¿½ï¿½Wï¿½Yï¿½ï¿½ vyz,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½rï¿½ï¿½-ï¿½ï¿½ fMï¿½ï¿½bï¿½ï¿½ï¿½Ä¹lIï¿½ï¿½ > endobj 665 0 obj > stream xï¿½ï¿½ZIwï¿½ï¿½ï¿½ï¿½Wï¿½ï¿½Bwcuï¿½"[ï¿½y/ï¿½ ï¿½$aaMï¿½ï¿½|ï¿½ ï¿½ï¿½ï¿½g $6 ï¿½]ÕµWaï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½ï¿½[ï¿½ï¿½ï¿½ï¿½á¯·c;ï¿½Å»È‹Ý˜ï¿½ï¿½~D@ï¿½ï¿½;ï¿½&qï¿½ï¿½ï¿½ï¿½ï¿½Ñï¿½ï¿½qï¿½ï¿½ï¿½s&ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½jï¿½Xï¿½r/Ü…Qï¿½&lwï¿½ï¿½ï¿½:ï¿½ï¿½ï¿½.^Lï¿½Fï¿½Qï¿½sï¿½ï¿½ï¿½!ï¿½ï¿½w3ï¿½Ù™ï¿½ï¿½yï¿½ï¿½Ó„ï¿½'ï¿½ï¿½ï¿½9ï¿½37ï¿½b~ï¿½?ï¿½9Oï¿½ xÎ›ï¿½f(d_ï¿½!s xï¿½ï¿½[ï¿½ï¿½vï¿½ï¿½ï¿½ï¿½ï¿½Z6ï¿½Eï¿½Yï¿½ï¿½!ï¿½ï¿½ï¿½ï¿½WOxï¿½ï¿½)ï¿½Bc>kï¿½L ï¿½1ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½]Zï¿½Oï¿½lï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½?ï¿½Ã¯ï¿½ï¿½;ï¿½V ï¿½vï¿½ï¿½D"ï¿½ï¿½Aï¿½X bï¿½ï¿½4ï¿½ï¿½fX0ï¿½ï¿½Yï¿½ï¿½ï¿½Ë“zO| ï¿½wï¿½ï¿½Aï¿½a aï¿½ï¿½Vï¿½ï¿½cï¿½X>ï¿½ï¿½vï¿½ï¿½4 ï¿½sï¿½ÖŒï¿½,r^ï¿½m1 ï¿½uï¿½%ï¿½ï¿½ {ï¿½3ï¿½A ï¿½.^ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½1+Zï¿½.ï¿½fï¿½ï¿½8tP=gmIï¿½wï¿½0ï¿½6Hï¿½ÜŽï¿½ï¿½ï¿½Nï¿½Tï¿½mO>ï¿½ï¿½ï¿½ï¿½e|ï¿½ï¿½ |ï¿½ï¿½ï¿½ï¿½[Mï¿½-!6'ï¿½ï¿½_ï¿½ ï¿½ï¿½ï¿½ï¿½Vlï¿½Jï¿½\ï¿½O9ï¿½ï¿½ï¿½cï¿½aï¿½+` ï¿½ï¿½*ï¿½ï¿½ï¿½Gï¿½sz;vVï¿½ï¿½@ï¿½Jï¿½ï¿½Eï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½w4Vï¿½Kï¿½nï¿½$ï¿½ÐŸï¿½ï¿½wÐ¸/ï¿½tï¿½ï¿½^ï¿½-+ï¿½ï¿½ï¿½[xï¿½ï¿½Hï¿½*ï¿½ï¿½\ï¿½ï¿½xG,ï¿½Qï¿½ï¿½Ï¦ï¿½U{oï¿½ï¿½ï¿½}ï¿½ï¿½Ó·ï¿½ï¿½#"ï¿½ï¿½2ï¿½ï¿½%#ï¿½H'4î‰‚Aï¿½ï¿½+ï¿½ï¿½"ï¿½nï¿½9ï¿½ï¿½s9ï¿½ï¿½bï¿½Wfmï¿½ï¿½Xï¿½Aï¿½-ï¿½ï¿½&ï¿½^ï¿½ï¿½eï¿½ï¿½0ï¿½zlï¿½ï¿½ß‹}ï¿½z,+ï¿½ï¿½jï¿½ï¿½8a"pMï¿½Wï¿½w?/ï¿½ï¿½#ï¿½ï¿½vï¿½ï¿½ï¿½WSï¿½ï¿½ï¿½ï¿½rï¿½"Gï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½]ï¿½fÏ­>}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½,ï¿½ bï¿½Y ~ï¿½M"ï¿½ï¿½zC# ï¿½ï¿½ï¿½dï¿½ï¿½ï¿½ï¿½rï¿½=ï¿½5ï¿½ï¿½ Pï¿½Vï¿½ï¿½gï¿½ï¿½^ï¿½ï¿½ï¿½ï¿½}Tï¿½>[0ï¿½ï¿½:ï¿½ ï¿½&/ï¿½#hFï¿½ï¿½ï¿½!ï¿½YQ*\ï¿½OFï¿½ï¿½Ø¶ï¿½{)ï¿½~ï¿½VÒ•{ï¿½ï¿½ï¿½K@Rï¿½dï¿½ï¿½ ï¿½ ï¿½ï¿½eï¿½ï¿½ï¿½^ï¿½Cï¿½fï¿½*ï¿½×­Óšwï¿½zï¿½ï¿½ï¿½n kï¿½ï¿½Sï¿½a5{ï¿½ï¿½jï¿½ï¿½ï¿½ï¿½GNï¿½p1ï¿½ï¿½)tï¿½6ï¿½Uï¿½ï¿½ï¿½ï¿½ Vï¿½ï¿½ï¿½nï¿½ï¿½ï¿½ï¿½e ï¿½ï¿½ï¿½ï¿½Çªï¿½ ï¿½gï¿½ï¿½ ï¿½ï¿½eÓ¾ï¿½ï¿½ï¿½ï¿½x}pï¿½ï¿½9Ôº Qï¿½Nï¿½iï¿½ï¿½Y\ï¿½THrB>Jï¿½Zï¿½0ï¿½ï¿½Dï¿½ï¿½Ò¡ï¿½ï¿½ï¿½"~ï¿½ï¿½7**ï¿½8 Ë®mt{%p@ï¿½ï¿½`ï¿½\ï¿½4Rï¿½ï¿½]*ï¿½AVUy=ï¿½ï¿½ï¿½ï¿½ï¿½Æ©ï¿½*ï¿½ï¿½P$SAï¿½(,ï¿½ï¿½pï¿½ï¿½ï¿½]ï¿½bOï¿½ï¿½Jï¿½f/5ï¿½eï¿½8ï¿½ï¿½,{ï¿½ï¿½R5 iV ,sï¿½ s -ï¿½#ï¿½ï¿½ï¿½ï¿½Å·Ø Æ®ï¿½Jï¿½ï¿½rï¿½=ï¿½Phï¿½ï¿½mï¿½ï¿½Ú¾ ï¿½ï¿½ï¿½Å½Aï¿½p'ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½ï¿½É¢ï¿½]ï¿½#[Ô¸kï¿½ï¿½X=ï¿½Éï¿½ï¿½b vÕ¥ï¿½2ï¿½*ï¿½ï¿½ï¿½ï¿½ft ï¿½nï¿½Vï¿½ï¿½Æ¡=ï¿½ï¿½.ï¿½Ûºï¿½wg:ï¿½ePlVgm?ï¿½I×¨ï¿½ï¿½"ï¿½.Q?Uï¿½1DY?aï¿½M+ï¿½zp/Zï¿½ï¿½T!ï¿½ï¿½3ï¿½ YnzXï¿½Â½ï¿½ï¿½_nï¿½2ï¿½È¶Mï¿½ï¿½A bï¿½b/ï¿½ï¿½ï¿½Ù°ï¿½N/ï¿½Fï¿½Ã²ï¿½ï¿½ï¿½ï¿½ï¿½Cï¿½ï¿½7FÔ¥' ï¿½Jï¿½kï¿½xï¿½:ï¿½ï¿½vï¿½U[ï¿½fï¿½9ï¿½{ï¿½ï¿½ï¿½ï¿½{ï¿½!ï¿½ï¿½Î›lï¿½7b ~ï¿½Fï¿½ï¿½ï¿½ $yR[{Ô+ï¿½Jï¿½^ï¿½ï¿½ï¿½qNï¿½ï¿½ï¿½ï¿½ ï¿½C}ï¿½uï¿½ï¿½],ï¿½_ ï¿½3Nï¿½4ï¿½qï¿½$(ï¿½J ï¿½YVï¿½ï¿½wï¿½Ü–3EÚ¥ï¿½/_&ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½x;ï¿½Jï¿½ï¿½|=% 'ï¿½5ï¿½e#{ï¿½7Ã‹ï¿½ï¿½ï¿½tï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½}4J1ï¿½sq%vvk?Ö¯hï¿½ï¿½'%8ï¿½Oï¿½(ï¿½ï¿½ï¿½$9'ï¿½Î2oï¿½ï¿½jï¿½ï¿½ï¿½ ï¿½Èeï¿½SPï¿½ï¿½ï¿½ï¿½:ï¿½ï¿½ï¿½ ï¿½ï¿½Lï¿½ï¿½ï¿½K>ï¿½| ï¿½=qï¿½XQKï¿½ï¿½Óˆï¿½ "Cï¿½ï¿½ï¿½Í†ï¿½lb(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Jï¿½@ï¿½\ï¿½ï¿½$T{-C.lï¿½%ï¿½)ï¿½ ï¿½ï¿½Q6ï¿½ï¿½ï¿½$Uï¿½evxuï¿½ ï¿½ï¿½Â³ï¿½jï¿½1Ffï¿½Iï¿½ï¿½ï¿½'ï¿½ï¿½7M]ï¿½Û«Aï¿½=ï¿½ï¿½Ü€ ï¿½ï¿½ï¿½0ï¿½6ï¿½  |ï¿½Bï¿½oï¿½ï¿½ZCï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½J w ï¿½]ï¿½21ï¿½ )>LLï¿½ï¿½h*`ï¿½][Yï¿½5Aï¿½7Aï¿½ $ÖZEï¿½ï¿½ &ï¿½`ï¿½H.ï¿½Qï¿½ï¿½:?ï¿½ï¿½ï¿½vï¿½IL}ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½>èŒ“ï¿½ï¿½ 7ï¿½Pï¿½ï¿½1`#ï¿½ ï¿½F!ï¿½vï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½OJï¿½846ï¿½Eï¿½vï¿½ï¿½ï¿½Pï¿½ Zï¿½C[Oï¿½7Uï¿½Ú…p ï¿½Vï¿½$ï¿½U5Aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½3ï¿½F,Hï¿½Rï¿½{j >ÇˆÎŽï¿½ï¿½ï¿½sï¿½`nï¿½- ï¿½gÇ  ï¿½`ï¿½Dmï¿½ï¿½ï¿½ï¿½Q?Pï¿½ï¿½{ï¿½ï¿½ï¿½ï¿½t ï¿½ï¿½gï¿½ ï¿½ ï¿½7ï¿½ï¿½tï¿½cï¿½!ï¿½^ï¿½ï¿½.ï¿½ï¿½Hï¿½ï¿½Cï¿½= ï¿½uYhP7ï¿½ï¿½ ï¿½Xï¿½ï¿½ ï¿½ï¿½Kï¿½-ï¿½gï¿½~ï¿½Çž 7eeï¿½M RÛ ï¿½-ï¿½ï¿½\ï¿½BK.ï¿½v !Klï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½k'ï¿½ï¿½R ï¿½Olqï¿½~ï¿½Bï¿½Zï¿½T Ã±ï¿½ï¿½É“kï¿½ï¿½ï¿½ï¿½Ezï¿½ ï¿½ï¿½?ï¿½o}^6O_>ï¿½ï¿½ï¿½tï¿½}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pÔï¿½ ï¿½_Pï¿½xï¿½ï¿½ï¿½+ï¿½ï¿½2ï¿½Uï¿½ï¿½fNï¿½~0/ï¿½ï¿½Aï¿½ï¿½ï¿½noï¿½/qï¿½ï¿½ > stream xï¿½ï¿½\ï¿½sï¿½8ï¿½ï¿½ï¿½?vWWï¿½ï¿½ï¿½ï¿½6ï¿½r:ï¿½ï¿½ Ý•Ú’mï¿½Ö´,ï¿½%ï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½ J )É²;]Ó³5eA ï¿½ï¿½ï¿½# tQ.ï¿½BKï¿½ï¿½B{ï¿½ï¿½ï¿½;ï¿½ï¿½B)Eï¿½Phï¿½^VRï¿½ï¿½Xz_ï¿½"ï¿½ï¿½îª¨ 5ï¿½t!ï¿½ï¿½(c ï¿½-jï¿½Bï¿½ï¿½Pï¿½Qï¿½ï¿½ï¿½Bzï¿½ Yqï¿½ï¿½2ï¿½Â—eï¿½JÞ—ï¿½ï¿½%p|ï¿½ ï¿½=Jtï¿½ï¿½.)c(ï¿½5elï¿½ï¿½$8KG[Qï¿½ï¿½E _ï¿½Bï¿½ï¿½ï¿½Uï¿½KM=Ë’2è‡ ï¿½RQ?RQï¿½Qeï¿½ ï¿½Jï¿½! hï¿½ ï¿½@Tvï¿½6Tzï¿½xT&BXï¿½Kï¿½ï¿½bPï¿½K[Gï¿½ï¿½l1L[4Wï¿½sï¿½Be SRï¿½^ï¿½ï¿½("ï¿½ï¿½R Aï¿½|a W&ï¿½Dgï¿½ï¿½4`0ï¿½DÏ„ï¿½ï¿½ 7 ï¿½ï¿½?ï¿½ï¿½2ï¿½ï¿½ï¿½È€sï¿½!.u(ï¿½V(ï¿½ï¿½PYÊ˜ï¿½ï¿½ï¿½ï¿½Fï¿½ï¿½@ï¿½(ï¿½ï¿½ eï¿½ï¿½2ï¿½5!ï¿½ï¿½ï¿½#.ï¿½ Ð€ï¿½Ø¨*ï¿½ï¿½xI,ï¿½ ï¿½!Fï¿½Ð–8i ï¿½ï¿½%ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½*jï¿½-ï¿½a-ï¿½h ï¿½7ï¿½BR*pï¿½$ï¿½8ï¿½Ð–ï¿½pï¿½ï¿½ï¿½[ï¿½1s %Iï¿½G9Gc ï¿½ï¿½1ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½h _ï¿½(xGcï¿½PFcMï¿½{Gcï¿½qï¿½wï¿½*=ï¿½ï¿½ ï¿½ ï¿½2Oï¿½ï¿½/ï¿½ï¿½JU!ï¿½oï¿½ï¿½ï¿½nï¿½8ï¿½ï¿½Zï¿½LNï¿½ï¿½ï¿½ï¿½Å€Xicï¿½~ G|?ï¿½ï¿½_ï¿½Xï¿½ï¿½Ñ‡ï¿½wï¿½ï¿½ ï¿½ï¿½-ï¿½ï¿½ ï¿½d"ï¿½"ï¿½"ï¿½ï¿½W Þžï¿½ï¿½ï¿½idsï¿½HØ» ï¿½6@dz6 6ï¿½ï¿½Wï¿½sï¿½ï¿½ï¿½ï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½|uï¿½1 ï¿½ï¿½ï¿½ï¿½qï¿½Rf5@G'ï¿½{'ï¿½ï¿½Nï¿½cdR; ï¿½ï¿½Cï¿½ï¿½BDVm-H2ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)ï¿½ï¿½sï¿½ï¿½ ï¿½lï¿½(ï¿½ï¿½ï¿½ï¿½iï¿½ï¿½cVï¿½ï¿½Bï¿½ =ï¿½ï¿½?>ï¿½'ï¿½Xï¿½ï¿½ï¿½|~3ï¿½o!ï¿½Ã›Yï¿½ï¿½\ï¿½Üžï¿½ï¿½ï¿½ï¿½ï¿½vx1ï¿½ï¿½]ï¿½ï¿½ ï¿½fï¿½ï¿½Í„^}ï¿½*/v&Ó‹ï¿½ï¿½ï¿½ ï¿½Kï¿½kï¿½ï¿½2> ï¿½2=MSï¿½Lï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½Wï¿½ï¿½ï¿½dï¿½Êªï¿½:ï¿½ï¿½ï¿½ ï¿½iï¿½ï¿½- ï¿½ï¿½ï¿½ï¿½4ï¿½-ï¿½q,&31N>0Õ–lR#ï¿½ï¿½ï¿½ï¿½ï¿½4ï¿½{ï¿½ï¿½ï¿½%2ï¿½Y&ï¿½7ï¿½ï¿½ï¿½ï¿½CYï¿½ï¿½ï¿½JJ=-7* UJá™¨ï¿½ï¿½ ï¿½:ï¿½DUï¿½Gï¿½ca+%Ø­ ï¿½Oc [V`}ï¿½"ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½zwï¿½=W 2x:ï¿½ ï¿½J ^ï¿½ï¿½)ï¿½ï¿½nhï¿½ ï¿½ï¿½Xlï¿½ï¿½ï¿½ï¿½ï¿½ï¿½vs5sGï¿½ 5ï¿½ï¿½ï¿½ï¿½ï¿½Bï¿½s0ï¿½b2&7vï¿½ï¿½rï¿½ï¿½ï¿½'ï¿½S ï¿½;ï¿½gÃ¯ï¿½ ~Gï¿½A m ï¿½ï¿½ï¿½ï¿½OWï¿½Pï¿½ï¿½oï¿½ï¿½Sï¿½Ã”'ï¿½XGLbÙ§z ï¿½?ï¿½ï¿½p|ï¿½Fï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½ï¿½)ï¿½46p^sÄ—KKï¿½(Þ‘ï¿½ï¿½%ï¿½Sï¿½ï¿½ï¿½ï¿½+[r}kï¿½Ø–%__ï¿½ï¿½qï¿½;_ï¿½ï¿½hKwï¿½ï¿½ï¿½ï¿½u-ï¿½ï¿½ï¿½ï¿½ï¿½Hï¿½ï¿½ï¿½ï¿½Ç­ ï¿½ï¿½ï¿½_U|ï¿½ï¿½R.Ú€Fï¿½3ï¿½ï¿½mï¿½. ï¿½ ï¿½Fï¿½8aï¿½/Ê©Oï¿½Sï¿½wï¿½ï¿½ ï¿½Dï¿½ï¿½i`*mï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"?ï¿½ï¿½Kï¿½ï¿½/Iï¿½ï¿½ï¿½ï¿½ï¿½MO%7ï¿½\ï¿½]ï¿½^×¥ï¿½pï¿½Dï¿½ï¿½pnÕŠVï¿½ï¿½]ï¿½$Â’ï¿½ï¿½e Ksï¿½ï¿½ï¿½Ø²!iï¿½|ï¿½?ï¿½ï¿½ ï¿½ï¿½fnï¿½26ï¿½ß°L%ï¿½ï¿½ï¿½MNï¿½xï¿½ï¿½Uï¿½ï¿½ï¿½ï¿½d{:ï¿½åµ¢ï¿½ï¿½ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½2F Ó†"ï¿½ï¿½7ï¿½ï¿½+ï¿½Kh9^ï¿½ï¿½ ï¿½1 ï¿½vï¿½ï¿½8ï¿½Ó‘s[Iï¿½Þ§ï¿½PBï¿½ï¿½Lï¿½ï¿½ï¿½BUï¿½bx[&ï¿½^"ï¿½ï¿½e$ï¿½ï¿½7ï¿½|ï¿½=.mï¿½T-ï¿½Hï¿½ï¿½ï¿½Ú±ï¿½f[ï¿½mï¿½fHï¿½6Iï¿½yBï¿½I×ºï¿½ï¿½(CbJï¿½(hï¿½ ï¿½9È@ï¿½ï¿½Wsï¿½-ï¿½Ñ•Xï¿½|ï¿½ï¿½ï¿½ï¿½2JÄ«8ï¿½@eTÓ” Aï¿½ï¿½Ü{+yï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(Eï¿½$ï¿½kï¿½9kmcï¿½j& ï¿½>Pï¿½ï¿½:ï¿½ï¿½Yï¿½ï¿½vï¿½+|^B|ï¿½ï¿½ï¿½>.^8k0rdIKJï¿½ï¿½X/Mï¿½ï¿½2,Dï¿½ï¿½ï¿½ X}lï¿½9ï¿½ï¿½ï¿½!ï¿½!ï¿½ï¿½Qï¿½ï¿½Rï¿½yï¿½ï¿½Qï¿½ï¿½ï¿½Lï¿½|Þœ.ï¿½ï¿½ï¿½5Û¢SNVï¿½.ï¿½Ì¦ìµ«ï¿½ï¿½ï¿½Êªï¿½pbï¿½5ï¿½iï¿½]ï¿½ï¿½=ï¿½ï¿½,u>ï¿½Hï¿½ï¿½iQï¿½1ï¿½ï¿½.iï¿½ï¿½ï¿½m+DCï¿½ï¿½8uï¿½ ï¿½8ï¿½*UÉ¡%_ï¿½Þ¹ï¿½'ï¿½duï¿½ Qï¿½ï¿½8pï¿½:Uï¿½Cï¿½1pï¿½ ï¿½;]ï¿½ï¿½dï¿½ï¿½tÒ¥ ï¿½ï¿½ï¿½Â¯ &ï¿½.ï¿½cU ï¿½ï¿½Pqï¿½ï¿½ï¿½5ä“Š]ï¿½ï¿½Tï¿½kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½` ï¿½-ï¿½!ï¿½,#5Ø¡ï¿½ ï¿½ï¿½R/ï¿½-ï¿½ï¿½ï¿½9ï¿½ï¿½ï¿½ï¿½pËºï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½8ï¿½ï¿½&ï¿½ï¿½u`ï¿½?Aï¿½ï¿½zPÊ€ï¿½È‡Uï¿½ï¿½hï¿½ï¿½ï¿½Xï¿½$ï¿½ ï¿½s .ï¿½6s8(ï¿½Eï¿½ @ï¿½rï¿½ï¿½h-ï¿½ï¿½ï¿½ï¿½^zï¿½Ý‚ï¿½uYï¿½Tï¿½ï¿½eï¿½ï¿½ ï¿½ï¿½rï¿½\ï¿½Õ—ï¿½XÑ¾ï¿½ï¿½ï¿½ wï¿½Aï¿½qï¿½ï¿½pï¿½:ï¿½ ï¿½Z1Íï¿½cWï¿½ï¿½ï¿½Dï¿½ï¿½ ï¿½?ï¿½ï¿½?ï¿½8dxï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ !ï¿½_ï¿½$ï¿½ï¿½xÞ¸ï¿½ï¿½{5ï¿½ZKï¿½GPRï¿½ï¿½ï¿½"^IEï¿½1ï¿½UDØ¦ï¿½ï¿½c\ï¿½r+ï¿½wEï¿½Qï¿½*ï¿½xï¿½M3ï¿½`ï¿½eï¿½jï¿½>ï¿½ï¿½Mï¿½p>ï¿½ï¿½tï¿½Mï¿½ PYï¿½Rï¿½;ï¿½CNï¿½|ï¿½ç¶ï¿½.ï¿½UXï¿½_lï¿½yï¿½ï¿½ï¿½-ï¿½ï¿½0ï¿½X.nIï¿½ï¿½{cï¿½R}zï¿½[`ï¿½ï¿½ï¿½gï¿½kï¿½ï¿½hiZï¿½W ï¿½$HK&:ï¿½ï¿½ï¿½\ï¿½lq[,]ÕKï¿½Zyï¿½tï¿½ï¿½ê¤yï¿½ï¿½nï¿½ï¿½ï¿½<>|ï¿½Dï¿½ï¿½ß­ï¿½ï¿½ï¿½ï¿½;y ï¿½ï¿½!q0ï¿½_ï¿½ ï¿½ï¿½ï¿½ï¿½Lï¿½ï¿½ï¿½hï¿½Gàµ”. ï¿½ï¿½ï¿½ Gï¿½Wï¿½ï¿½ï¿½_ï¿½ï¿½ï¿½#dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½*ï¿½0ï¿½|pï¿½ï¿½wï¿½gï¿½ï¿½ï¿½#\ï¿½Cï¿½Äª7ï¿½oï¿½Ì¾xï¿½%:6ï¿½ï¿½B ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½ï¿½_/ï¿½ï¿½lï¿½Lï¿½$ï¿½ï¿½i=ï¿½ //zï¿½ï¿½ï¿½ï¿½Ûœï¿½Oï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|~1ï¿½ÝŒï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\ï¿½nï¿½ï¿½ï¿½Ø–ï¿½^ï¿½ï¿½ d ?ï¿½ï¿½ï¿½Â™^c ï¿½ï¿½ï¿½ï¿½@-ï¿½ï¿½Nï¿½ï¿½dï¿½ï¿½ï¿½#Zï¿½%_:gï¿½cï¿½C2ï¿½Lï¿½/Å¡x%^ï¿½7ï¿½H ï¿½ï¿½ï¿½nï¿½ï¿½ï¿½,ï¿½ï¿½Ñ¼/.ï¿½ï¿½ï¿½H\ßŠï¿½ï¿½ï¿½ï¿½ 7C1ï¿½ï¿½%ï¿½ï¿½ï¿½ï¿½Õ€*ï¿½.ï¿½l'ï¿½ï¿½ZDï¿½-Mï¿½hï¿½Tï¿½Mï¿½Ö‰ï¿½ï¿½Rï¿½ï¿½ï¿½ï¿½VDï¿½ï¿½L+ï¿½|+ï¿½ï¿½Oï¿½ï¿½hVï¿½6Í³ï¿½kï¿½ï¿½ï¿½ï¿½ï¿½3ï¿½#~ï¿½ï¿½ï¿½ï¿½ï¿½|+ï¿½hNJV'ï¿½;ï¿½ï¿½mï¿½'ï¿½ï¿½ ï¿½ï¿½Fï¿½j!Ã¹Wï¿½vDz6ï¿½Okyï¿½NVkï¿½ï¿½ï¿½â‰ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ ï¿½ï¿½ï¿½ï¿½!]>'Dï¿½ï¿½ï¿½ï¿½p f3ï¿½wï¿½ï¿½S1ï¿½ï¿½ï¿½HÐ¿_ï¿½ï¿½íˆ ï¿½Zaï¿½+ )ï¿½Ì/^ï¿½ ï¿½>a*Tï¿½ï¿½ï¿½ï¿½ï¿½n*4ï¿½ï¿½ï¿½ï¿½u1{ï¿½'8Zuï¿½zzpï¿½ï¿½ï¿½ã³³ï¿½ï¿½oï¿½ï¿½Ýï¿½Qï¿½&adï¿½ï¿½-@ï¿½ï¿½Cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½z;ï¿½ï¿½Õ³ï¿½Qï¿½+tï¿½ï¿½ï¿½ï¿½ï¿½>=Þï¿½ï¿½=ï¿½~wï¿½ï¿½ bï¿½ï¿½(VXS-ï¿½(ï¿½ï¿½ ï¿½ï¿½(ï¿½7'ï¿½_È£G ï¿½[9Ahï¿½Aï¿½ï¿½ï¿½Î‡ï¿½ï¿½ï¿½Anï¿½ï¿½ÜSï¿½Dï¿½ï¿½cvWï¿½ï¿½'ï¿½ï¿½ï¿½ï¿½pz~{ï¿½y4ï¿½?uï¿½ï¿½ï¿½hD3w(ï¿½ï¿½ï¿½G4ï¿½`ï¿½;ï¿½ï¿½M-ï¿½ï¿½}ï¿½Kï¿½3s7ï¿½ï¿½ï¿½ï¿½ft;ï¿½{ï¿½M>ï¿½ï¿½Fzï¿½wï¿½oGoï¿½).ï¿½ï¿½ï¿½ÔºCï¿½ï¿½/>bB>Zï¿½dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½"ï¿½@ï¿½ï¿½S5bï¿½Zï¿½î¦³>;~ï¿½ï¿½5ï¿½È­ï¿½k ï¿½uOï¿½Hï¿½ï¿½ï¿½1;Lï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½tÜ‰ï¿½Dï¿½ï¿½É»ï¿½=ï¿½uï¿½6ï¿½Ý™GH.Î4% "ï¿½E}{yï¿½ ï¿½Cwghï¿½;Ë©ï¿½{ON_ï¿½ï¿½ï¿½uï¿½zï¿½Ãï¿½7-ï¿½ï¿½ï¿½ï¿½Qï¿½Ç—ï¿½MY>Fï¿½ï¿½ï¿½ï¿½É—ï¿½É—qÌ'S2ï¿½ï¿½Yzï¿½Õ™Mnï¿½7ï¿½ï¿½"&ï¿½Cï¿½ï¿½ï¿½hï¿½PÎ–É‚ ï¿½Aï¿½ï¿½ï¿½? L#ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pLï¿½ï¿½ï¿½ Í‡7ï¿½oï¿½ bTï¿½ï¿½ï¿½Lï¿½ï¿½ï¿½l6ï¿½Uï¿½ï¿½z[w;&ï¿½Fï¿½ mï¿½ï¿½Çµ} ï¿½9|ï¿½ï¿½ï¿½{{Rï¿½ï¿½O[ï¿½_ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½;ï¿½ ï¿½nhï¿½Jï¿½bm ! ï¿½Wï¿½Yï¿½ï¿½*ï¿½Qï¿½Z#ï¿½pï¿½ï¿½ï¿½vï¿½ï¿½ï¿½X ï¿½zï¿½K[ï¿½ï¿½ï¿½ï¿½3è£ï¿½Í“ï¿½ï¿½ï¿½ï¿½]K'ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Bï¿½ï¿½ï¿½=ï¿½uï¿½ ï¿½??ï¿½}Û’ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½_"lï¿½~ï¿½Kï¿½ poï¿½ï¿½ï¿½O*ï¿½ï¿½ÉŸ70ï¿½ï¿½ï¿½uï¿½ï¿½ï¿½ï¿½ï¿½i,MÇºîŸ¼ \ï¿½ï¿½ï¿½VbWHï¿½'ï¿½ï¿½ï¿½ï¿½ï¿½ endstream endobj 667 0 obj > stream xï¿½ï¿½ï¿½Tï¿½ï¿½-ï¿½ï¿½'Hhï¿½ï¿½ï¿½ ï¿½ï¿½ ï¿½@#ï¿½5ï¿½wï¿½`!hpw48ï¿½ï¿½\ ï¿½ï¿½;3ï¿½ï¿½ï¿½Zï¿½-ï¿½jï¿½]ï¿½ï¿½NUï¿½]ï¿½ï¿½\Uï¿½Yï¿½ï¿½ï¿½(cï¿½0ï¿½ï¿½ï¿½  $ï¿½ï¿½ï¿½xllï¿½,ll (ï¿½ï¿½ï¿½@[ï¿½_fj-ï¿½ï¿½3ï¿½ ,ï¿½/ï¿½ï¿½ï¿½ï¿½bï¿½2ï¿½ï¿½ï¿½ï¿½ï¿½ y[ ;'ï¿½ï¿½Gï¿½ï¿½Wï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ @ï¿½ï¿½dPbï¿½Ûƒï¿½ï¿½(Ô’ï¿½ N K+ï¿½ï¿½1ï¿½ï¿½Ð™ï¿½ï¿½ï¿½ï¿½yï¿½ï¿½ï¿½ï¿½ï¿½@f&`ï¿½ï¿½ ï¿½ hï¿½rï¿½ï¿½ï¿½-@ï¿½ï¿½ ï¿½xï¿½W :!+ï¿½Aï¿½ï¿½ï¿½ï¿½Íï¿½ï¿½Î™ï¿½ï¿½ï¿½Rï¿½ï¿½ ï¿½ï¿½XÔï¿½@'Wï¿½9ï¿½wï¿½ e;ï¿½ï¿½ï¿½ï¿½ï¿½Pï¿½Yï¿½ï¿½ï¿½ï¿½kï¿½[@ï¿½Lï¿½ï¿½ï¿½ï¿½-ï¿½ v~ï¿½pï¿½ï¿½ /ï¿½4ï¿½*@ï¿½dï¿½? Lï¿½ï¿½f`gaï¿½;ï¿½_Ñ¿ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½=@`Kï¿½ï¿½Pï¿½Qdï¿½ï¿½Cï¿½ &`ï¿½ï¿½D[gï¿½ï¿½xWï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½M 2ï¿½j ï¿½ï¿½ï¿½jï¿½ï¿½ï¿½ ï¿½ qfqï¿½ï¿½nï¿½ï¿½wï¿½ï¿½)Kï¿½ï¿½%ï¿½ï¿½ï¿½`ï¿½3ï¿½ï¿½ï¿½ï¿½@N@ï¿½ï¿½ï¿½{ï¿½ï¿½yï¿½6`{7ï¿½ï¿½_ï¿½6ï¿½ï¿½Ý„ï¿½ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bBï¿½ï¿½f ï¿½ ï¿½ï¿½ï¿½x9ï¿½8 @G ï¿½ï¿½ÌŠï¿½wï¿½w ï¿½?ï¿½ï¿½ï¿½/ ï¿½x9ï¿½; ,^ï¿½ ï¿½ï¿½,ï¿½/ï¿½Pï¿½ï¿½M\ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½ßŽï¿½F(ï¿½ï¿½ sï¿½` ï¿½ï¿½Qï¿½ï¿½ï¿½bZï¿½ï¿½_.ï¿½ ï¿½ï¿½g{ï¿½ ;ï¿½ï¿½ï¿½ï¿½ï¿½_ï¿½_ï¿½enï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½IIï¿½ï¿½*1ï¿½ï¿½ï¿½ï¿½> {wï¿½3;ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½}ï¿½ï¿½ï¿½ï¿½,ï¿½&ï¿½ï¿½ï¿½`ï¿½'Tla`gï¿½ï¿½Ú—1ï¿½ï¿½b×¿@ï¿½ï¿½rï¿½ï¿½;ï¿½ï¿½ï¿½ï¿½jï¿½ ï¿½Dnï¿½ï¿½ï¿½fï¿½ï¿½ï¿½ï¿½ï¿½,ï¿½?Bï¿½ï¿½ï¿½;ï¿½ï¿½Mï¿½ï¿½[ï¿½ï¿½ï¿½ï¿½ï¿½ nï¿½?ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½/Â‹h] / ï¿½dï¿½ï¿½ï¿½ï¿½ï¿½jï¿½\Z%ï¿½9ï¿½ï¿½ï¿½ï¿½rï¿½ï¿½E[ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½O;ï¿½Yï¿½4WAÌ¬ï¿½ï¿½ÌŸvï¿½ß«f Uï¿½Aï¿½ß–ï¿½(6ï¿½ï¿½ï¿½ì—™ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½?]&ï¿½/ï¿½ï¿½ï¿½cï¿½ï¿½:ï¿½w ï¿½`3{ï¿½ï¿½{ï¿½ï¿½ï¿½0qr2ï¿½@yï¿½ï¿½ï¿½ ï¿½bYPsï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½!/!ï¿½ï¿½ï¿½} ï¿½N(ï¿½ï¿½ï¿½ï¿½ ï¿½*ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½?ï¿½ï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½^ï¿½ï¿½jbï¿½`ï¿½ï¿½ï¿½ï¿½%ï¿½ï¿½3`ï¿½/ï¿½ï¿½ h ï¿½ï¿½ï¿½ï¿½ï¿½jï¿½/ï¿½Â°ï¿½ï¿½ï¿½ï¿½ï¿½_Nï¿½ï¿½×‰l Vï¿½ï¿½K*ï¿½ï¿½ï¿½TNCï¿½ï¿½Fï¿½!&Nï¿½ï¿½ï¿½ X!ï¿½ï¿½ï¿½ï¿½^ï¿½ï¿½Wiï¿½5Q3'ï¿½ï¿½ï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½ï¿½YÝf( ï¿½ï¿½fï¿½Aï¿½5Am7Uï¿½ï¿½nï¿½[cBï¿½'i7: ï¿½cï¿½Hï¿½~ï¿½)ï¿½ï¿½ï¿½ï¿½Yï¿½ ï¿½2 }ï¿½2ï¿½Öï¿½ï¿½7ï¿½+s^ï¿½ï¿½du \Wï¿½d2ï¿½ï¿½dï¿½qï¿½ï¿½Wï¿½Ó‰^ï¿½$ï¿½ï¿½MPyï¿½)oï¿½ï¿½ï¿½ï¿½TI#_u!ï¿½ï¿½[ï¿½kPczï¿½ï¿½ï¿½ï¿½'ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)wtUg)ï¿½ï¿½k ï¿½ï¿½Cï¿½Mï¿½mq_ï¿½ï¿½ï¿½ï¿½~Øˆ2OUÎ Pï¿½EIpï¿½|ï¿½s`xpï¿½ï¿½ï¿½;2@ï¿½5pï¿½*ï¿½%-8H"Å…Xï¿½ï¿½ï¿½ï¿½ï¿½Sï¿½zï¿½ï¿½ï¿½eï¿½GBï¿½^ï¿½>ï¿½8n+ï¿½_ï¿½ ï¿½/ï¿½rï¿½n;zï¿½ï¿½`ï¿½ï¿½ï¿½Aï¿½Cï¿½ï¿½ï¿½ï¿½:R Lï¿½,cï¿½o_ï¿½ï¿½kï¿½/ï¿½ï¿½/Éžï¿½ aï¿½b+8OÅ²ï¿½ ï¿½Ë¶ï¿½ï¿½Tï¿½ï¿½ï¿½a 4ï¿½Iï¿½5ï¿½ï¿½ï¿½ï¿½ï¿½sdï¿½ï¿½ 1ï¿½_ï¿½+Qï¿½ï¿½5ï¿½@>ï¿½ï¿½Y>ï¿½9ï¿½!7Oï¿½/ï¿½ï¿½ï¿½Lï¿½Q5uï¿½9"ï¿½ï¿½@ï¿½ï¿½9ï¿½hï¿½aï¿½Äƒï¿½ï¿½ï¿½Ajï¿½Iï¿½ï¿½ï¿½oï¿½aï¿½ï¿½JJï¿½-|FÑ¸Í¬ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½Pmï¿½,>U3_ï¿½ï¿½ï¿½@ï¿½ï¿½Z.ï¿½ï¿½Oï¿½ï¿½g4Iï¿½kï¿½ï¿½ï¿½Ãï¿½5nï¿½m ï¿½(ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½Ðªï¿½â¬¥Iï¿½ï¿½ ï¿½ï¿½UJï¿½ï¿½ï¿½ë·ºï¿½eï¿½ï¿½/5ï¿½ï¿½ï¿½iySjm{ï¿½%ï¿½nï¿½yï¿½ï¿½ï¿½ï¿½Hï¿½Zï¿½yU\ ï¿½%ï¿½hï¿½ï¿½z:ï¿½ï¿½jï¿½0ï¿½O;ï¿½ï¿½ï¿½%\ï¿½Yï¿½ï¿½ï¿½)ï¿½Mï¿½#ï¿½Ì” "ï¿½ Yï¿½ï¿½ï¿½ï¿½IVï¿½ï¿½E0jï¿½3iFï¿½ï¿½Zï¿½ï¿½Gï¿½aï¿½n`ï¿½tGï¿½ï¿½ï¿½;1Y:0ï¿½×€Fï¿½ï¿½ÙƒNfrï¿½ï¿½ï¿½ï¿½!Tmï¿½Ý¡ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½sï¿½ï¿½Z 8DYï¿½ï¿½zï¿½Vï¿½ï¿½n Ã”zï¿½ï¿½iï¿½ï¿½,ï¿½?lï¿½ï¿½ï¿½ï¿½ï¿½7ï¿½ï¿½ï¿½ï¿½ykï¿½ï¿½ï¿½F ï¿½ï¿½ï¿½4ï¿½ï¿½i ï¿½ï¿½ï¿½I0pï¿½Eg=Î¢ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½%ï¿½8ï¿½ï¿½g#ï¿½0ï¿½>63ï¿½Ö§lp?ï¿½ï¿½ï¿½yB ï¿½{ï¿½MFï¿½Î¡Üƒï¿½ï¿½dï¿½ï¿½ ï¿½Zï¿½ï¿½ï¿½ï¿½ï¿½ï¿½AFï¿½ï¿½Ò¢ ï¿½:ï¿½ï¿½ï¿½ï¿½ï¿½Û”+Xï¿½ï¿½Å‡êŒ„ï¿½ï¿½ Vï¿½ ï¿½ï¿½1ï¿½ ï¿½Vï¿½ï¿½ï¿½r ï¿½jï¿½ï¿½Aï¿½ï¿½ï¿½ï¿½è˜»ï¿½ï¿½S ï¿½ï¿½ï¿½&8ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½kï¿½0E-ï¿½cî‡—ï¿½[pï¿½ï¿½ï¿½ï¿½'ï¿½p,ï¿½*3ï¿½ï¿½/kï¿½iï¿½ZN` Oï¿½ï¿½ï¿½bï¿½ï¿½x!\ï¿½ï¿½I* ï¿½,D=ï¿½Ô£ï¿½ ï¿½ï¿½ï¿½\ a"0ï¿½ï¿½Sï¿½. ï¿½Pï¿½ï¿½nï¿½ï¿½ï¿½ï¿½rï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Vv&ï¿½ï¿½+#ï¿½}$xï¿½[ï¿½+ï¿½Èˆï¿½Mï¿½\ï¿½ï¿½ï¿½zï¿½ï¿½qï¿½ #ï¿½$Oï¿½ï¿½ \Éš)_ï¿½&"ï¿½ï¿½fï¿½I ï¿½ï¿½~ï¿½ï¿½Dï¿½T, Fï¿½.OOï¿½`ï¿½(ï¿½ï¿½A#%WRï¿½ï¿½kï¿½ï¿½ï¿½)[ï¿½=Tï¿½5_È€Iï¿½Ô‘d gï¿½Â‰%#;ï¿½!lGï¿½ï¿½ ï¿½&ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½[ï¿½2ï¿½sï¿½ï¿½ï¿½cK,oï¿½,ï¿½ï¿½8 S^ï¿½ï¿½yï¿½ï¿½aÈ…ï¿½ï¿½Dï¿½6FDï¿½&ï¿½oï¿½ï¿½yä£\ï¿½$C5 ï¿½ï¿½ ï¿½H3Q#>aï¿½Vï¿½Xï¿½ï¿½Òƒ ï¿½ï¿½ï¿½ @ï¿½E[*ï¿½ï¿½ ï¿½ Yï¿½UÝ¯q(ï¿½l{Bpï¿½ Sï¿½ï¿½ï¿½4 P`ï¿½ï¿½ï¿½ï¿½Çªï¿½Å˜ï¿½6ï¿½@ÒžFï¿½>;2ï¿½ï¿½rDï¿½&mxï¿½ï¿½iï¿½Lï¿½ï¿½vï¿½ï¿½Yï¿½ï¿½ï¿½{ï¿½ï¿½/ï¿½v Hk^ï¿½wãŸ6 ï¿½ï¿½ï¿½ï¿½M0 ï¿½ï¿½ï¿½wï¿½ï¿½?ï¿½>ï¿½0ï¿½ï¿½ï¿½cï¿½} ï¿½BÛ©s3ï¿½lï¿½ï¿½ +0ï¿½ï¿½ï¿½Nmï¿½ï¿½ï¿½ï¿½Â¼zï¿½ï¿½ï¿½ï¿½ï¿½^ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½zï¿½wÑ¬ï¿½WGT\oï¿½Æ¹fÏ–ï¿½mï¿½*\n .ï¿½ï¿½%ï¿½fï¿½ï¿½)ï¿½ ï¿½kï¿½ï¿½ï¿½*qï¿½7é‹½tï¿½Aï¿½ï¿½?ï¿½ï¿½?zW[,ï¿½ï¿½ï¿½ï¿½... [truncated]
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] Several LLM-based agent papers were published in 2024, covering topics like adaptive planning, environment and task generation, controllable conversations, and prompt engineering.
      [2] MAGIS is an LLM-based multi-agent framework for GitHub issue resolution presented at NeurIPS 2024.
      [3] A survey paper titled "Large Language Model based Multi-Agents: A Survey of Progress and Challenges" was submitted to arXiv in January 2024 and revised in April 2024.
   â””â”€ Decision: More research needed
   â””â”€ Next query: trends in LLM agent systems 2024
ðŸ” Searcher [2]: Follow-up search for: trends in LLM agent systems 2024

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue
      [2] https://blog.promptlayer.com/types-of-llm-agent/
      [3] https://blog.langchain.com/langchain-state-of-ai-2024/
      [4] https://arxiv.org/html/2508.17281v1
      [5] https://thenewstack.io/top-5-ai-engineering-trends-of-2024/
   â””â”€ Snippets:
      â€¢ A significant trend observed in 2024 and continuing into 2025 is the strategic shift towards developing smaller, yet highly capable LLMs. Models such as TinyGPT, TinyGPT-V, and TinyLlama (1.1 billion parameters) exemplify this movement, demonstrating that high performance does not exclusively correlate with massive scale. This focus on efficiency is critical for broader accessibility and deployment, enabling LLMs to run effectively on devices with limited memory, such as mobile applications, [...] The evolution of LLMs is increasingly prioritizing sophisticated reasoning capabilities over mere pattern recognition. OpenAI's o1, released in December 2024, signaled a significant shift towards a "reasoning-first" architecture, emphasizing structured thinking for complex problem-solving. This focus addresses one of the long-standing challenges in AI: the ability to perform logical deduction and explain its decision-making process. Tools like SHAP, LIME, and attention visualization are gaining [...] Simultaneously, the use of synthetic data for training LLMs is emerging as a transformative trend in 2025. Traditionally, LLM development has been constrained by the availability and quality of real-world data. Synthetic data, artificially generated yet representative of real-world patterns, addresses this limitation, enabling the creation of larger and more diverse training datasets. This not only accelerates development but also offers opportunities to mitigate biases present in real-world
      â€¢ The future of LLM agents lies in their seamless integration into our daily lives. These advancements will transform LLM agents from tools into partners, enhancing human capabilities and enriching our interactions with technology.  ## Conclusion  The evolution in types of LLM agents marks a leap in artificial intelligence, promising to reshape how we interact with technology and automate tasks. [...] In 2024, Large Language Model (LLM) agents have diversified into specialized categories, each tailored to specific functionalities and applications. Here's an overview of the different types of LLM agents:  ### 1. Conversational Agents  These agents engage in natural dialogue with users, providing information, answering questions, and assisting with various tasks. They rely on LLMs to understand and generate human-like responses. [...] Platform  -- Prompt Management  -- Evaluations  -- Observability  -- Dataset Management  -- Prompt Chaining  Docs  Blog  Case Studies  Careers  Log in  Contact Us  Contact Us   Log In  Back   # LLM Agents Explained: Types, Use Cases, and Future Trends  By Erich H.   Nov 21, 2024  type of LLM agent
      â€¢ In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation. As more LLM apps are created, weâ€™re excited to see how folks dig into smarter workflows, better performance, and stronger reliability. [...] As we did last year, we want to share some product usage patterns that showcase how the AI ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, weâ€™ve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows.

ðŸ“– ContentReader: Reading 3 URLs...
ðŸ“– Read URL: https://www.linkedin.com/pulse/evolving-landscape-... (8015 chars)
   â””â”€ [https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue]
      Preview: The Evolving Landscape of Large Language Models (2024-2025) Agree & Join LinkedIn By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement , Privacy Policy , and Cookie Policy . Skip to main content LinkedIn Top Content People Learning Jobs Games Join now Sign in #Techfriday The Evolving Landscape of Large Language Models (2024-2025) Report this article Hahn Software Hahn Software Your expert for artificial intelligence and customised software | development, project management, data &amp; AI. Published Aug 1, 2025 + Follow I. Executive Summary The period spanning 2024 to 2025 marks a pivotal phase in the evolution of Large Language Models (LLMs), characterized by their rapid transition from specialized research tools to indispensable components in mainstream business applications. The global market for LLMs is experiencing exponential growth, valued at $6.4 billion in 2024 and projected to reach $36.1 billion by 2030, reflecting widespread enterprise adoption across sectors such as finance, healthcare, and technology. This expansion is driven by significant technological advancements, including the development of smaller, more efficient models, the widespread integration of multimodal capabilities, and a pronounced focus on reasoning-centric architectures. Concurrently, the increasing maturity of open-source LLMs is democratizing access and fostering innovation, while sophisticated fine-tuning techniques and the use of synthetic data are accelerating their production readiness. The impact of LLMs on software development is profound, transforming traditional methodologies and fostering new paradigms like "Vibe Coding." These AI-powered coding assistants are enhancing developer productivity, automating repetitive tasks, and improving code quality across the entire Software Development Lifecycle (SDLC). However, this rapid integration also brings critical considerations, including the persistent challenges of hallucinations, inherent biases, and the substantial environmental footprint associated with LLM training and inference. Addressing these concerns necessitates robust ethical frameworks, transparent development practices, and, crucially, sustained human oversight to ensure responsible and effective AI deployment. II. The Current State of Large Language Models A. Foundational Advancements and Market Growth The trajectory of Large Language Models has seen an unprecedented acceleration from academic curiosities to foundational enterprise technologies. The market's valuation underscores this shift, with a projected compound annual growth rate that highlights a significant investment and reliance on these systems. Enterprises are integrating LLMs to drive automation, extract deeper insights from vast datasets, and enhance customer interactions, moving beyond initial exploratory phases to advanced, integrated implementations within their core operations. This widespread adoption is a testament to the tangible value LLMs are delivering, from streamlining complex workflows to enabling entirely new forms of customer engagement. B. Key Technological Shifts The technological landscape of LLMs is undergoing dynamic transformations, pushing the boundaries of what these models can achieve and how they are deployed. The Rise of Smaller, More Efficient Models A significant trend observed in 2024 and continuing into 2025 is the strategic shift towards developing smaller, yet highly capable LLMs. Models such as TinyGPT, TinyGPT-V, and TinyLlama (1.1 billion parameters) exemplify this movement, demonstrating that high performance does not exclusively correlate with massive scale. This focus on efficiency is critical for broader accessibility and deployment, enabling LLMs to run effectively on devices with limited memory, such as mobile applications, low-power hardware, or in environments with constrained internet access. The emergence of sparse expert models, which activate only relevant parts of their network for specific tasks, further enhances speed and energy efficiency, optimizing resource utilization while maintaining robust performance. This development expands the practical utility of LLMs beyond large-scale data centers, opening avenues for on-device AI and edge computing. Mainstreaming of Multimodal AI Multimodal AI has transitioned from a specialized research area to a mainstream capability, profoundly impacting how users interact with AI systems. OpenAI's GPT-4o, released in May 2024, marked a significant milestone by enabling real-time understanding and response across text, images, and audio. This was quickly followed by similar advancements from other major players, including Google's Gemini 2.0 and Meta's LLaMA 3.2, and Anthropic's Claude 3.5 Sonnet. These models can process and generate content across various modalities, expanding AI's utility in creative tools, enhancing accessibility features, and revolutionizing customer service interactions. For instance, Google's Veo demo showcasing a knife slicing through fruit exemplifies the increasingly realistic and versatile nature of AI-generated video content. The ability to integrate and interpret diverse data types allows for more nuanced and comprehensive AI applications, mirroring human perception more closely. OpenAI's 4o image generation, which integrates text and visual creation into a unified system, allows for multi-turn generation where users can refine images conversationally, demonstrating a sophisticated interplay between language and visuals. Emphasis on Reasoning-Centric Architectures and Interpretability The evolution of LLMs is increasingly prioritizing sophisticated reasoning capabilities over mere pattern recognition. OpenAI's o1, released in December 2024, signaled a significant shift towards a "reasoning-first" architecture, emphasizing structured thinking for complex problem-solving. This focus addresses one of the long-standing challenges in AI: the ability to perform logical deduction and explain its decision-making process. Tools like SHAP, LIME, and attention visualization are gaining traction to help users understand how LLMs arrive at their conclusions, which is particularly crucial in high-stakes domains such as healthcare and finance where transparency and accountability are paramount. This move towards interpretability is not merely a technical refinement but a fundamental step towards building trust and enabling responsible deployment of AI systems. Research is actively challenging the simplified view that LLMs merely "predict the next word," revealing that these models may engage in far more complex internal reasoning and conceptual representation. Advancements in Fine-Tuning Techniques The efficiency and accessibility of deploying LLMs have been significantly bolstered by advancements in fine-tuning techniques. Methods such as prompt tuning and various hybrid approaches allow organizations to customize models for specific tasks without the prohibitive cost and time associated with retraining them from scratch. The integration of Automated Machine Learning (AutoML) further streamlines this process by automating decisions like hyperparameter selection, making fine-tuning more accessible to a broader range of developers and accelerating the journey from development to production readiness. These techniques enable rapid adaptation of general-purpose LLMs to specialized domains, unlocking new application possibilities and reducing development cycles. The Growing Influence of Open-Source LLMs Open-source LLMs have emerged as a powerful force, challenging the dominance of proprietary models and fostering a more collaborative and flexible development ecosystem. Models like Mistral, DeepSeek-V3, and Meta's LLaMA 3.2 have demonstrated strong performance while being publicly available, granting developers greater control and flexibility to build custom applications. This open approach encourages community collaboration, leading to continuous improvements in... [truncated]
ðŸ“– Read URL: https://blog.promptlayer.com/types-of-llm-agent/... (8015 chars)
   â””â”€ [https://blog.promptlayer.com/types-of-llm-agent/]
      Preview: 7 Types of LLM Agents: A Comprehensive Guide (2024) Platform -- Prompt Management -- Evaluations -- Observability -- Dataset Management -- Prompt Chaining Docs Blog Case Studies Careers Log in Contact Us Contact Us Log In Back LLM Agents Explained: Types, Use Cases, and Future Trends By &nbsp;Erich H. Nov 21, 2024 type of LLM agent Large Language Model (LLM) agents have rapidly evolved, becoming one of the hot topics in the tech industry. Initially designed for natural language processing tasks, LLMs can now serve as autonomous agents capable of complex decision-making and task execution. In this guide, weâ€™ll explore the basics of LLM Agents, their types (such as Conversational Agents, Task-Oriented Agents, Creative Agents, and more), real-world applications, and the emerging trends driving their evolution. Whether you're a tech enthusiast, a professional exploring AI, or someone curious about how these systems work, this guide will provide valuable insights into the ever-expanding world of LLM Agents. What are LLM Agents? LLM Agents are advanced AI systems that leverage large language models to autonomously perform tasks by interpreting inputs, planning actions, and executing them using integrated tools. Unlike traditional chatbots, which are limited to predefined responses, LLM Agents can exhibit complex reasoning, maintain memory of past interactions, and adapt their behavior based on environmental feedback. This enables them to handle intricate tasks across various domains, from automating workflows to providing personalized assistance Different Types of LLM Agents In 2024, Large Language Model (LLM) agents have diversified into specialized categories, each tailored to specific functionalities and applications. Here's an overview of the different types of LLM agents: 1. Conversational Agents These agents engage in natural dialogue with users, providing information, answering questions, and assisting with various tasks. They rely on LLMs to understand and generate human-like responses. Example : Customer support chatbots that handle inquiries and provide solutions. 2. Task-Oriented Agents Focused on performing specific tasks or achieving predefined objectives, these agents interact with users to understand their needs and then execute actions to fulfill those needs. Example : AI assistants that schedule appointments or manage emails. 3. Creative Agents Capable of generating original and creative content such as artwork, music, or writing, these agents use LLMs to understand human preferences and artistic styles, enabling them to produce content that resonates with audiences. Example : Content generation tools that draft articles or create digital art. 4. Collaborative Agents These agents work alongside humans to accomplish shared goals or tasks, facilitating communication, coordination, and cooperation between team members or between humans and machines. LLMs may support collaborative agents by assisting in decision-making, generating reports, or providing insights. Example: Project management bots that help teams coordinate tasks and deadlines. 5. Multimodal Agents Extending beyond text, multimodal agents process and generate content across various formats, including audio, images, and video. This capability allows for richer and more immersive interactions. Example: Virtual assistants that can interpret visual data or generate images based on textual descriptions. 6. Autonomous Agents Designed to operate with minimal human intervention, autonomous agents can make decisions and perform actions independently. They are often used in environments where continuous operation is essential. Example: Robotic process automation bots that handle repetitive tasks in business processes. 7. Multi-Agent Systems Involving multiple LLM agents working collaboratively, these systems tackle complex tasks by leveraging the strengths of various specialized agents. They communicate and coordinate to achieve common objectives. Example: A team of agents where one gathers data, another analyzes it, and a third generates reports based on the analysis. Understanding these categories helps in selecting the appropriate type of LLM agent for specific applications, ensuring optimal performance and user experience. ðŸ° Interesting in building your own agents? PromptLayer provides the tools to manage and monitor prompts with your whole team.&nbsp; Get started here. Real-World Applications of LLM Agents LLM agents are now essential across industries, providing innovative solutions to complex challenges. Below are some notable real-world applications across different industries: Healthcare Clinical Decision Support : LLM agents assist healthcare professionals by analyzing patient data and medical literature to provide evidence-based treatment recommendations. For instance, Meditron, a Llama-based model, has been fine-tuned on clinical guidelines and medical research to enhance diagnostic accuracy and patient care. Patient Interaction : Virtual health assistants powered by LLMs engage with patients, offering personalized health advice, appointment scheduling, and medication reminders, thereby improving patient engagement and adherence to treatment plans. Finance Fraud Detection : Financial institutions employ LLM agents to monitor transactions and identify patterns indicative of fraudulent activities, enhancing security and trust in financial systems. Investment Analysis : LLM agents analyze vast financial datasets to provide insights into market trends, aiding investors in making informed decisions. They can generate comprehensive reports and forecasts based on real-time data. Education Personalized Learning : Educational platforms utilize LLM agents to create customized learning experiences, adapting content to individual student needs and learning styles. For example, AI tutors can simulate classroom interactions, providing tailored feedback and support. Content Creation : LLM agents assist educators in developing course materials, quizzes, and interactive content, streamlining the curriculum development process and ensuring content relevance. Legal Document Review : Law firms leverage LLM agents to analyze legal documents, contracts, and case law, expediting the review process and identifying critical information with high accuracy. Legal Research : LLM agents can quickly retrieve and summarize pertinent legal precedents and statutes, supporting attorneys in case preparation and strategy development. Customer Service Automated Support : Companies deploy LLM-powered chatbots to handle customer inquiries, providing instant responses and resolving common issues, thereby enhancing customer satisfaction and reducing operational costs.&nbsp; Sentiment Analysis : LLM agents analyze customer feedback to gauge sentiment, enabling businesses to address concerns proactively and improve their products or services. Media and Entertainment Content Generation : LLM agents are used to create scripts, articles, and marketing materials, assisting writers and marketers in generating creative content efficiently. Interactive Experiences : In gaming and virtual reality, LLM agents enable dynamic storytelling and character interactions, providing users with immersive and personalized experiences. Emerging Trends and Future Prospects As we look ahead, LLM agents are poised to move past traditional applications, embedding themselves into the very fabric of our daily lives. Beyond their current roles, these agents can become integral components of our personal and professional ecosystems, operating seamlessly and sometimes imperceptibly. Ambient Intelligence Integration Agents could be built into ambient intelligence systems, continuously monitoring and interpreting our environments to provide context-aware assistance. Imagine a workspace where an LLM agent autonomously organizes your schedule, drafts emails based on brief verbal cues, and anticipates your informational needs by proactively gathering r... [truncated]
ðŸ“– Read URL: https://blog.langchain.com/langchain-state-of-ai-2... (8015 chars)
   â””â”€ [https://blog.langchain.com/langchain-state-of-ai-2024/]
      Preview: LangChain State of AI 2024 Report Skip to content Case Studies In the Loop Web Home Try LangSmith Docs Sign in Subscribe LangChain State of AI 2024 Report Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving. By LangChain 6 min read Dec 19, 2024 Another year of building with LLMs is coming to an end â€”&nbsp;and 2024 didnâ€™t disappoint. With nearly 30k users signing up for LangSmith every month, weâ€™re lucky to have front row seats to whatâ€™s happening in the industry.&nbsp; As we did last year , we want to share some product usage patterns that showcase how the AI&nbsp;ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, weâ€™ve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows.&nbsp; Dive into the stats below to learn exactly what developers are building, testing, and prioritizing. Infrastructure usage With Large Language Models (LLMs) eating the world, everyoneâ€™s asking the mirror-mirror-on-the-wall question: â€œWhich model is the most utilized of them all?â€ Letâ€™s unpack what weâ€™ve seen. Top LLM providers Like last yearâ€™s results , OpenAI reigns as the most used LLM provider among LangSmith users â€”&nbsp;used more than 6x as much as Ollama, the next-most popular provider (counted by LangSmith organization usage). Interestingly, Ollama and Groq (which both allow users to run open source models, with the former focusing on local execution and the latter on cloud deployment) have accelerated in momentum this year, breaking into the top 5. This shows a growing interest in more flexible deployment options and customizable AI infrastructure. When it comes to providers that offer open-source models, the top providers have stayed relatively consistent compared to last year - Ollama, Mistral, and Hugging Face have made it easy for developers to run open source models on their platforms. These OSS providersâ€™ collective usage represents 20% of the top 20 LLM providers (by the number of organizations using them).&nbsp; Top Retrievers / Vector Stores Performing retrieval is still critical for many GenAI workflows. The top 3 vector stores have remained the same as last year, with Chroma and FAISS as the most popular choices. This year, Milvus, MongoDB, and Elasticâ€™s vector databases have also entered the top 10.&nbsp; Building with LangChain products As developers have gained more experience utilizing generative AI, they are also building more dynamic applications. From the growing sophistication of workflows, to the rise of AI agents â€” weâ€™re seeing a few trends that point to an evolving ecosystem of innovation. Observability isnâ€™t limited to LangChain applications While langchain (our open source framework) is central to many folksâ€™ LLM app development journeys, 15.7% of LangSmith traces this year come from non- langchain frameworks. This reflects a broader trend where observability is needed regardless of what framework youâ€™re using to build the LLM app â€” and that interoperability is supported by LangSmith. Python remains dominant, while JavaScript usage grows Debugging, testing, and monitoring certainly has a special place in our Python developersâ€™ hearts, with 84.7% usage coming from the Python SDK. But there is a notable and growing interest in JavaScript as developers pursue web-first applications â€”&nbsp;the JavaScript SDK accounts for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.&nbsp; AI agents are gaining traction As companies are getting more serious about incorporating AI agents across various industries , adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction â€”&nbsp;with 43% of LangSmith organizations are now sending LangGraph traces . These traces represent complex, orchestrated tasks that go beyond basic LLM interactions. This growth aligns with the rise in agentic behavior: we see that on average 21.9% of traces now involve tool calls, up from an average of 0.5% in 2023. Tool calling allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action. Increased use of tool calling can enhance an agentâ€™s ability to interact with external systems and perform tasks like writing to databases.&nbsp; Performance and optimization Balancing speed and sophistication is a key challenge when developing applications â€” especially those leveraging LLM resources. Below, we explore how organizations are interacting with their applications to align the complexity of their needs with efficient performance. Complexity is growing, but tasks are being handled efficiently&nbsp;&nbsp; The average number of steps per trace has more than doubled over the past year, rising from on average 2.8 steps (2023) to 7.7 steps (2024). We define a step as a distinct operation within a trace, such as a call to an LLM, retriever, or tool. This growth in steps signals that organizations are leveraging more complex and multi-faceted workflows. Rather than a simple question-answer interaction, users are building systems that chain together multiple tasks, such as retrieving information, processing it, and generating actionable results. In contrast, the average number of LLM calls per trace has grown more modestlyâ€” from on average 1.1 to 1.4 LLM calls. This speaks to how developers are designing systems to achieve more with fewer LLM calls, balancing functionality while keeping expensive LLM requests in check LLM testing &amp; evaluation What are organizations doing to test their LLM applications to guard against inaccurate or low-quality LLM-generated responses? While itâ€™s no easy feat to keep the quality of your LLM app high , we see organizations using LangSmithâ€™s evaluation capabilities to automate testing and generate user feedback loops to create more robust, reliable applications. LLM-as-Judge: Evaluating what matters LLM-as-Judge evaluators capture grading rules into an LLM prompt and use the LLM to score whether the output adheres to specific criteria. We see developers testing for these characteristics the most: Relevance , Correctness , Exact Match , and Helpfulness These highlight that most developers are doing coarse checks for response quality to make sure AI generated outputs donâ€™t completely miss the mark.&nbsp; Iterating with human feedback&nbsp; Human feedback is a key part of the iteration loop for folks building LLM apps. LangSmith speeds up the process of collecting and incorporating human feedback on traces and runs (i.e. spans) â€“ so that users can create rich datasets for improvement and optimization. Over the past year, annotated runs grew 18x , scaling linearly with growth in LangSmith usage. Feedback volume per run also increased slightly, rising from 2.28 to 2.59 feedback entries per run . Still, feedback is relatively sparse per run. Users may be prioritizing speed in reviewing runs over providing comprehensive feedback, or commenting on only the most critical or problematic runs that need attention.&nbsp; Conclusion In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation. As more LLM apps are created, weâ€™re excited to see how folks dig into smarter workflows, better performance, and stronger reliability.&nbsp; Learn more here about how LangSmith can bring more visibility into your LLM app development and improve performance over time â€”&nbsp;from debugging bottlenecks to evaluating response quality to monitoring regressions.&nbsp;&nbsp; Tags By LangChain Join our newsletter Updates from the LangChain team and community ... [truncated]
ðŸ”¬ Analyzer [2]: Analyzing 5 results, 6 contents

ðŸ”¬ Analyzer [2]: Analyzed 5 results, 6 contents
   â””â”€ New findings:
      [1] A trend in 2024 is developing smaller but highly capable LLMs like TinyGPT, TinyGPT-V, and TinyLlama to enable broader accessibility and deployment on devices with limited memory.
      [2] LLM agents are evolving to integrate seamlessly into daily life, enhancing human capabilities.
      [3] In 2024, developers focused on multi-step agents, efficiency improvements (fewer LLM calls), and quality checks.
      [4] LLMs are used as decision-making agents for interpreting instructions, managing sequential tasks, and adapting through feedback.
      [5] AI got integrated into the core tools of developers (IDEs), while new techniques for creating â€œAI agentsâ€ arose in secondary tools like LangChain and LlamaIndex.
   â””â”€ Decision: Research complete (9 total findings)

ðŸ“¦ Compress: Compressing 9 findings, 6 contents
   â””â”€ Compressed to 2388 chars (from ~6202 raw chars)
   â””â”€ Sources cited: 6
   â””â”€ Preview:
      ## Key Findings
      - LLM agents are rapidly evolving and integrating into daily life, enhancing human capabilities [4, 2].
      - 2024 saw advancements in multi-step agents, efficiency improvements, and quality checks [6].
      - Smaller, highly capable LLMs like TinyGPT are being developed for broader accessibility [4].
      - LLMs are utilized as decision-making agents for instruction interpretation, task management, and adaptation through feedback [2].
      
      ## Detailed Information
      
      **LLM Agent Evolution and Applications:**
      - LLM agents are evolving rapidly, transitioning from specialized research tools to essential components in mainstream business [4, 2]. They are being integrated into daily life, enhancing human capabilities [4, 2].
      - LLMs serve as autonomous agents, capable of complex decision-making and task execution [2]. They function as decision-making agents by interpreting instructions, managing sequential tasks, and adapting through feedback [2].
      - Several LLM-based agent papers were published in 2024, covering adaptive planning, environment/task generation, controllable conversations, and prompt engineering [6].
      - Key areas of focus in 2024 include multi-step agents, efficiency improvements (fewer LLM calls), and quality checks [6].
      
      **Multi-Agent Frameworks and Surveys:**
      - MAGIS, an LLM-based multi-agent framework for GitHub issue resolution, was presented at NeurIPS 2024 [3].
      - A survey paper titled "Large Language Model based Multi-Agents: A Survey of Progress and Challenges" was submitted to arXiv in January 2024 and revised in April 2024 [1].
      
      **Smaller LLMs:**
      - A trend in 2024 involves developing smaller yet highly capable LLMs, such as TinyGPT, TinyGPT-V, and TinyLlama, to broaden accessibility and enable deployment on devices with limited memory [4].
      
      **Developer Tooling:**
      - AI is being integrated into core developer tools (IDEs), while new techniques for creating AI agents are emerging in secondary tools like LangChain [5].
      
      ## Sources
      [1] https://arxiv.org/abs/2402.01680
      [2] https://blog.promptlayer.com/types-of-llm-agent/
      [3] https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf
      [4] https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue
      [5] https://blog.langchain.com/langchain-state-of-ai-2024/
      [6] https://github.com/AGI-Edgerunners/LLM-Agents-Papers

âœï¸ Writer: Composing response from 9 findings
   â””â”€ Using compressed notes (2388 chars)

âœï¸ Writer: Generated response (3518 chars)
   â””â”€ Sources used:
      [1] https://arxiv.org/abs/2402.01680
      [2] https://blog.promptlayer.com/types-of-llm-agent/
      [3] https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ, íš¨ìœ¨ì„± í–¥ìƒ, í’ˆì§ˆ ê²€ì¦ì— ì§‘ì¤‘ë˜ì—ˆìœ¼ë©°, ë” ìž‘ê³  íš¨ìœ¨ì ì¸ LLM ëª¨ë¸ì˜ ë“±ìž¥ê³¼ ê°œë°œìž ë„êµ¬ì— AI í†µí•©ì´ ë‘ë“œëŸ¬ì§„ë‹¤. LLM ì—ì´ì „íŠ¸ëŠ” ì „ë¬¸ ì—°êµ¬ ë„êµ¬ì—ì„œ ì¼ìƒ ìƒí™œ ë° ë¹„ì¦ˆë‹ˆìŠ¤ì— í†µí•©ë˜ì–´ ì¸ê°„ì˜ ì—­ëŸ‰ì„ ê°•í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆë‹¤ [4, 2].
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      * **LLM ì—ì´ì „íŠ¸ì˜ ì§„í™” ë° í™œìš©:** LLM ì—ì´ì „íŠ¸ëŠ” ì „ë¬¸ì ì¸ ì—°êµ¬ ë„êµ¬ì—ì„œ ì£¼ë¥˜ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ í•µì‹¬ ìš”ì†Œë¡œ ë¹ ë¥´ê²Œ ì§„í™”í•˜ê³  ìžˆìœ¼ë©°, ì¼ìƒìƒí™œì— í†µí•©ë˜ì–´ ì¸ê°„ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìžˆë‹¤ [4, 2]. LLMì€ ìžìœ¨ ì—ì´ì „íŠ¸ë¡œì„œ ë³µìž¡í•œ ì˜ì‚¬ ê²°ì •ê³¼ ìž‘ì—… ì‹¤í–‰ì„ ìˆ˜í–‰í•˜ë©°, ì§€ì¹¨ í•´ì„, ìˆœì°¨ì  ìž‘ì—… ê´€ë¦¬ ë° í”¼ë“œë°±ì„ í†µí•œ ì ì‘ì„ í†µí•´ ì˜ì‚¬ ê²°ì • ì—ì´ì „íŠ¸ ì—­í• ì„ í•œë‹¤ [2]. 2024ë…„ì—ëŠ” ì ì‘í˜• ê³„íš, í™˜ê²½/ìž‘ì—… ìƒì„±, ì œì–´ ê°€ëŠ¥í•œ ëŒ€í™”, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë“± ë‹¤ì–‘í•œ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê´€ë ¨ ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆë‹¤ [6].
      * **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ ë° ì¡°ì‚¬:** NeurIPS 2024ì—ì„œëŠ” GitHub ì´ìŠˆ í•´ê²°ì„ ìœ„í•œ LLM ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì¸ MAGISê°€ ë°œí‘œë˜ì—ˆë‹¤ [3]. "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸: ì§„í–‰ ìƒí™© ë° ê³¼ì œ ì¡°ì‚¬"ë¼ëŠ” ì œëª©ì˜ ì„¤ë¬¸ ì¡°ì‚¬ ë…¼ë¬¸ì´ 2024ë…„ 1ì›”ì— arXivì— ì œì¶œë˜ì—ˆê³  2024ë…„ 4ì›”ì— ìˆ˜ì •ë˜ì—ˆë‹¤ [1].
      * **ì†Œê·œëª¨ LLM:** ë” ë„“ì€ ì ‘ê·¼ì„±ì„ ì œê³µí•˜ê³  ì œí•œëœ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ìž¥ì¹˜ì— ë°°í¬í•  ìˆ˜ ìžˆë„ë¡ TinyGPT, TinyGPT-V, TinyLlamaì™€ ê°™ì´ ë” ìž‘ì§€ë§Œ ê°•ë ¥í•œ LLMì„ ê°œë°œí•˜ëŠ” ì¶”ì„¸ê°€ 2024ë…„ì— ë‚˜íƒ€ë‚¬ë‹¤ [4].
      * **ê°œë°œìž ë„êµ¬:** AIëŠ” í•µì‹¬ ê°œë°œìž ë„êµ¬(IDE)ì— í†µí•©ë˜ê³  ìžˆìœ¼ë©°, LangChainê³¼ ê°™ì€ ë³´ì¡° ë„êµ¬ì—ì„œ AI ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë“±ìž¥í•˜ê³  ìžˆë‹¤ [5].
      
      ## ìƒì„¸ ë¶„ì„
      
      LLM ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ë°œì „ì€ ë‹¨ìˆœí•œ ê¸°ìˆ ì  ì§„ë³´ë¥¼ ë„˜ì–´, ì‚¬íšŒ ì „ë°˜ì— ê±¸ì³ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìžˆë‹¤. 2024ë…„ì— ë°œí‘œëœ ë…¼ë¬¸ë“¤ì€ ì´ëŸ¬í•œ ë³€í™”ì˜ ë°©í–¥ì„±ì„ ëª…í™•ížˆ ë³´ì—¬ì¤€ë‹¤.
      
      *   **ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ë° íš¨ìœ¨ì„±:**  ê¸°ì¡´ì˜ LLM ì—ì´ì „íŠ¸ê°€ ë‹¨ì¼ ìž‘ì—… ìˆ˜í–‰ì— ì§‘ì¤‘í–ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, 2024ë…„ì—ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ë³µìž¡í•œ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ì—ì´ì „íŠ¸ ê°œë°œì— ë”ìš± ì§‘ì¤‘í•˜ê³  ìžˆë‹¤ [6]. ì´ëŠ” ì‹¤ì œ ì—…ë¬´ í™˜ê²½ì—ì„œì˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ë°œì „ì´ë‹¤. ë˜í•œ, LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì—¬ ë¹„ìš© íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì‘ë‹µ ì†ë„ë¥¼ ê°œì„ í•˜ëŠ” ì—°êµ¬ ì—­ì‹œ í™œë°œížˆ ì§„í–‰ ì¤‘ì´ë‹¤ [6].
      *   **í’ˆì§ˆ ê²€ì¦ì˜ ì¤‘ìš”ì„±:** LLM ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¬¼ì´ í•­ìƒ ì •í™•í•˜ê±°ë‚˜ ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ, í’ˆì§ˆ ê²€ì¦ ê³¼ì •ì´ ë”ìš± ì¤‘ìš”í•´ì§€ê³  ìžˆë‹¤ [6]. ìƒì„±ëœ ë‚´ìš©ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê±°ë‚˜, íŽ¸í–¥ì„±ì„ ì¤„ì´ëŠ” ë“±ì˜ ì—°êµ¬ê°€ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ê³  ìžˆë‹¤.
      *   **ì†Œí˜• LLMì˜ ë“±ìž¥:** ê³ ì„±ëŠ¥ LLMì€ ë†’ì€ ì»´í“¨íŒ… ìžì›ì„ ìš”êµ¬í•˜ë¯€ë¡œ, ì‚¬ìš© í™˜ê²½ì— ì œì•½ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ TinyGPTì™€ ê°™ì€ ì†Œí˜• LLMì´ ê°œë°œë˜ê³  ìžˆìœ¼ë©°, ì´ëŠ” LLM ì—ì´ì „íŠ¸ì˜ ì ‘ê·¼ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤ [4]. ì†Œí˜• LLMì€ ì œí•œëœ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, íŠ¹ížˆ ëª¨ë°”ì¼ ê¸°ê¸°ë‚˜ IoT ìž¥ì¹˜ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.
      *   **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ:** ê°œë³„ ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ë„ í™œë°œí•˜ë‹¤.  MAGIS [3] ì™€ ê°™ì€ í”„ë ˆìž„ì›Œí¬ëŠ” GitHub ì´ìŠˆ í•´ê²°ê³¼ ê°™ì€ í˜‘ì—… í™˜ê²½ì—ì„œ LLM ì—ì´ì „íŠ¸ì˜ ìž ìž¬ë ¥ì„ ë³´ì—¬ì¤€ë‹¤.  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ê° ì—ì´ì „íŠ¸ì˜ ì—­í• ì„ ë¶„ë‹´í•˜ê³ , ìƒí˜¸ ìž‘ìš©ì„ í†µí•´ ë³´ë‹¤ ì •êµí•˜ê³  íš¨ìœ¨ì ì¸ ë¬¸ì œ í•´ê²°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
      *   **ê°œë°œìž ë„êµ¬ì™€ì˜ í†µí•©:** LLM ì—ì´ì „íŠ¸ ê°œë°œì„ ìœ„í•œ ë„êµ¬ ë° í”„ë ˆìž„ì›Œí¬ë„ ë°œì „í•˜ê³  ìžˆë‹¤. LangChain [5] ê³¼ ê°™ì€ ë„êµ¬ëŠ” LLMì„ í™œìš©í•œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ì‰½ê²Œ í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•˜ë©°, AIê°€ IDEì™€ ê°™ì€ í•µì‹¬ ê°œë°œ ë„êµ¬ì— í†µí•©ë˜ë©´ì„œ ê°œë°œ ìƒì‚°ì„±ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      *   [1] Large Language Model based Multi-Agents: A Survey of Progress and Challenges: [https://arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)
      *   [2] Types of LLM Agent: [https://blog.promptlayer.com/types-of-llm-agent/](https://blog.promptlayer.com/types-of-llm-agent/)
      *   [3] MAGIS: [https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf)
      *   [4] Evolving Landscape of Large Language Models 2024-2025: [https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue](https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue)
      *   [5] LangChain State of AI 2024: [https://blog.langchain.com/langchain-state-of-ai-2024/](https://blog.langchain.com/langchain-state-of-ai-2024/)
      *   [6] LLM-Agents-Papers: [https://github.com/AGI-Edgerunners/LLM-Agents-Papers](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)
      
      ## ê²°ë¡  ë° í‰ê°€
      
      2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ì‹¤ìš©ì ì¸ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìžˆë‹¤. ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸, íš¨ìœ¨ì„± ê°œì„ , í’ˆì§ˆ ê²€ì¦, ì†Œí˜• LLM ê°œë°œ ë“±ì€ LLM ì—ì´ì „íŠ¸ê°€ ë”ìš± ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í™œìš©ë  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ì¤‘ìš”í•œ ë°œê±¸ìŒì´ë‹¤. íŠ¹ížˆ, ê°œë°œìž ë„êµ¬ì™€ì˜ í†µí•©ì€ LLM ì—ì´ì „íŠ¸ ê°œë°œì˜ ì§„ìž… ìž¥ë²½ì„ ë‚®ì¶”ê³ , ë” ë§Žì€ ê°œë°œìžë“¤ì´ LLM ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìžˆë„ë¡ í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.  ë‹¤ë§Œ, LLM ì—ì´ì „íŠ¸ì˜ ìœ¤ë¦¬ì  ë¬¸ì œ (íŽ¸í–¥ì„±, ì±…ìž„ ë¬¸ì œ ë“±) ì— ëŒ€í•œ ì—°êµ¬ì™€ ë…¼ì˜ë„ ë”ìš± í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì ¸ì•¼ í•  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ  ë°œì „ì€ ì¸ê°„ì˜ ëŠ¥ë ¥ì„ ì¦ê°•ì‹œí‚¤ê³  ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìžˆì§€ë§Œ, ë™ì‹œì— ë°œìƒí•  ìˆ˜ ìžˆëŠ” ìž ìž¬ì ì¸ ìœ„í—˜ì— ëŒ€í•œ ëŒ€ë¹„ë„ í•„ìš”í•˜ë‹¤. ì•žìœ¼ë¡œ LLM ì—ì´ì „íŠ¸ ê¸°ìˆ ì€ ë”ìš± ë°œì „í•˜ê³ , ìš°ë¦¬ ì‚¬íšŒì— ë” í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 2 Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: 2024ë…„ ë°œí‘œëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ì£¼ìš” íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì¤˜...
â”‚ Time: 34.27s
â”‚ LLM Calls: 2
â”‚ Search Calls: 2
â”‚ URLs Read: 6
â”‚ Iterations: 2
â”‚ Est. Tokens: 1331
â”‚ Response Length: 3518 chars
â”‚ Has Citations: âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ê°œë°œ, íš¨ìœ¨ì„± í–¥ìƒ, í’ˆì§ˆ ê²€ì¦ì— ì§‘ì¤‘ë˜ì—ˆìœ¼ë©°, ë” ìž‘ê³  íš¨ìœ¨ì ì¸ LLM ëª¨ë¸ì˜ ë“±ìž¥ê³¼ ê°œë°œìž ë„êµ¬ì— AI í†µí•©ì´ ë‘ë“œëŸ¬ì§„ë‹¤. LLM ì—ì´ì „íŠ¸ëŠ” ì „ë¬¸ ì—°êµ¬ ë„êµ¬ì—ì„œ ì¼ìƒ ìƒí™œ ë° ë¹„ì¦ˆë‹ˆìŠ¤ì— í†µí•©ë˜ì–´ ì¸ê°„ì˜ ì—­ëŸ‰ì„ ê°•í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìžˆë‹¤ [4, 2].

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

* **LLM ì—ì´ì „íŠ¸ì˜ ì§„í™” ë° í™œìš©:** LLM ì—ì´ì „íŠ¸ëŠ” ì „ë¬¸ì ì¸ ì—°êµ¬ ë„êµ¬ì—ì„œ ì£¼ë¥˜ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ í•µì‹¬ ìš”ì†Œë¡œ ë¹ ë¥´ê²Œ ì§„í™”í•˜ê³  ìžˆìœ¼ë©°, ì¼ìƒìƒí™œì— í†µí•©ë˜ì–´ ì¸ê°„ì˜ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³  ìžˆë‹¤ [4, 2]. LLMì€ ìžìœ¨ ì—ì´ì „íŠ¸ë¡œì„œ ë³µìž¡í•œ ì˜ì‚¬ ê²°ì •ê³¼ ìž‘ì—… ì‹¤í–‰ì„ ìˆ˜í–‰í•˜ë©°, ì§€ì¹¨ í•´ì„, ìˆœì°¨ì  ìž‘ì—… ê´€ë¦¬ ë° í”¼ë“œë°±ì„ í†µí•œ ì ì‘ì„ í†µí•´ ì˜ì‚¬ ê²°ì • ì—ì´ì „íŠ¸ ì—­í• ì„ í•œë‹¤ [2]. 2024ë…„ì—ëŠ” ì ì‘í˜• ê³„íš, í™˜ê²½/ìž‘ì—… ìƒì„±, ì œì–´ ê°€ëŠ¥í•œ ëŒ€í™”, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë“± ë‹¤ì–‘í•œ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ê´€ë ¨ ë…¼ë¬¸ì´ ë°œí‘œë˜ì—ˆë‹¤ [6].
* **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ ë° ì¡°ì‚¬:** NeurIPS 2024ì—ì„œëŠ” GitHub ì´ìŠˆ í•´ê²°ì„ ìœ„í•œ LLM ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í”„ë ˆìž„ì›Œí¬ì¸ MAGISê°€ ë°œí‘œë˜ì—ˆë‹¤ [3]. "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸: ì§„í–‰ ìƒí™© ë° ê³¼ì œ ì¡°ì‚¬"ë¼ëŠ” ì œëª©ì˜ ì„¤ë¬¸ ì¡°ì‚¬ ë…¼ë¬¸ì´ 2024ë…„ 1ì›”ì— arXivì— ì œì¶œë˜ì—ˆê³  2024ë…„ 4ì›”ì— ìˆ˜ì •ë˜ì—ˆë‹¤ [1].
* **ì†Œê·œëª¨ LLM:** ë” ë„“ì€ ì ‘ê·¼ì„±ì„ ì œê³µí•˜ê³  ì œí•œëœ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ìž¥ì¹˜ì— ë°°í¬í•  ìˆ˜ ìžˆë„ë¡ TinyGPT, TinyGPT-V, TinyLlamaì™€ ê°™ì´ ë” ìž‘ì§€ë§Œ ê°•ë ¥í•œ LLMì„ ê°œë°œí•˜ëŠ” ì¶”ì„¸ê°€ 2024ë…„ì— ë‚˜íƒ€ë‚¬ë‹¤ [4].
* **ê°œë°œìž ë„êµ¬:** AIëŠ” í•µì‹¬ ê°œë°œìž ë„êµ¬(IDE)ì— í†µí•©ë˜ê³  ìžˆìœ¼ë©°, LangChainê³¼ ê°™ì€ ë³´ì¡° ë„êµ¬ì—ì„œ AI ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ì´ ë“±ìž¥í•˜ê³  ìžˆë‹¤ [5].

## ìƒì„¸ ë¶„ì„

LLM ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ë°œì „ì€ ë‹¨ìˆœí•œ ê¸°ìˆ ì  ì§„ë³´ë¥¼ ë„˜ì–´, ì‚¬íšŒ ì „ë°˜ì— ê±¸ì³ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìžˆë‹¤. 2024ë…„ì— ë°œí‘œëœ ë…¼ë¬¸ë“¤ì€ ì´ëŸ¬í•œ ë³€í™”ì˜ ë°©í–¥ì„±ì„ ëª…í™•ížˆ ë³´ì—¬ì¤€ë‹¤.

*   **ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ ë° íš¨ìœ¨ì„±:**  ê¸°ì¡´ì˜ LLM ì—ì´ì „íŠ¸ê°€ ë‹¨ì¼ ìž‘ì—… ìˆ˜í–‰ì— ì§‘ì¤‘í–ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, 2024ë…„ì—ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ë³µìž¡í•œ ìž‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆëŠ” ì—ì´ì „íŠ¸ ê°œë°œì— ë”ìš± ì§‘ì¤‘í•˜ê³  ìžˆë‹¤ [6]. ì´ëŠ” ì‹¤ì œ ì—…ë¬´ í™˜ê²½ì—ì„œì˜ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ë°œì „ì´ë‹¤. ë˜í•œ, LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¤„ì—¬ ë¹„ìš© íš¨ìœ¨ì„±ì„ ë†’ì´ê³  ì‘ë‹µ ì†ë„ë¥¼ ê°œì„ í•˜ëŠ” ì—°êµ¬ ì—­ì‹œ í™œë°œížˆ ì§„í–‰ ì¤‘ì´ë‹¤ [6].
*   **í’ˆì§ˆ ê²€ì¦ì˜ ì¤‘ìš”ì„±:** LLM ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¬¼ì´ í•­ìƒ ì •í™•í•˜ê±°ë‚˜ ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ, í’ˆì§ˆ ê²€ì¦ ê³¼ì •ì´ ë”ìš± ì¤‘ìš”í•´ì§€ê³  ìžˆë‹¤ [6]. ìƒì„±ëœ ë‚´ìš©ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê±°ë‚˜, íŽ¸í–¥ì„±ì„ ì¤„ì´ëŠ” ë“±ì˜ ì—°êµ¬ê°€ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£¨ì–´ì§€ê³  ìžˆë‹¤.
*   **ì†Œí˜• LLMì˜ ë“±ìž¥:** ê³ ì„±ëŠ¥ LLMì€ ë†’ì€ ì»´í“¨íŒ… ìžì›ì„ ìš”êµ¬í•˜ë¯€ë¡œ, ì‚¬ìš© í™˜ê²½ì— ì œì•½ì´ ìžˆì„ ìˆ˜ ìžˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ TinyGPTì™€ ê°™ì€ ì†Œí˜• LLMì´ ê°œë°œë˜ê³  ìžˆìœ¼ë©°, ì´ëŠ” LLM ì—ì´ì „íŠ¸ì˜ ì ‘ê·¼ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤ [4]. ì†Œí˜• LLMì€ ì œí•œëœ í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, íŠ¹ížˆ ëª¨ë°”ì¼ ê¸°ê¸°ë‚˜ IoT ìž¥ì¹˜ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.
*   **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ:** ê°œë³„ ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ë„ í™œë°œí•˜ë‹¤.  MAGIS [3] ì™€ ê°™ì€ í”„ë ˆìž„ì›Œí¬ëŠ” GitHub ì´ìŠˆ í•´ê²°ê³¼ ê°™ì€ í˜‘ì—… í™˜ê²½ì—ì„œ LLM ì—ì´ì „íŠ¸ì˜ ìž ìž¬ë ¥ì„ ë³´ì—¬ì¤€ë‹¤.  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ê° ì—ì´ì „íŠ¸ì˜ ì—­í• ì„ ë¶„ë‹´í•˜ê³ , ìƒí˜¸ ìž‘ìš©ì„ í†µí•´ ë³´ë‹¤ ì •êµí•˜ê³  íš¨ìœ¨ì ì¸ ë¬¸ì œ í•´ê²°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.
*   **ê°œë°œìž ë„êµ¬ì™€ì˜ í†µí•©:** LLM ì—ì´ì „íŠ¸ ê°œë°œì„ ìœ„í•œ ë„êµ¬ ë° í”„ë ˆìž„ì›Œí¬ë„ ë°œì „í•˜ê³  ìžˆë‹¤. LangChain [5] ê³¼ ê°™ì€ ë„êµ¬ëŠ” LLMì„ í™œìš©í•œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ì‰½ê²Œ í•  ìˆ˜ ìžˆë„ë¡ ì§€ì›í•˜ë©°, AIê°€ IDEì™€ ê°™ì€ í•µì‹¬ ê°œë°œ ë„êµ¬ì— í†µí•©ë˜ë©´ì„œ ê°œë°œ ìƒì‚°ì„±ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

*   [1] Large Language Model based Multi-Agents: A Survey of Progress and Challenges: [https://arxiv.org/abs/2402.01680](https://arxiv.org/abs/2402.01680)
*   [2] Types of LLM Agent: [https://blog.promptlayer.com/types-of-llm-agent/](https://blog.promptlayer.com/types-of-llm-agent/)
*   [3] MAGIS: [https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/f7ae4fe91d96f50abc2211f09b6a7e49-Paper-Conference.pdf)
*   [4] Evolving Landscape of Large Language Models 2024-2025: [https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue](https://www.linkedin.com/pulse/evolving-landscape-large-language-models-2024-2025-zqgue)
*   [5] LangChain State of AI 2024: [https://blog.langchain.com/langchain-state-of-ai-2024/](https://blog.langchain.com/langchain-state-of-ai-2024/)
*   [6] LLM-Agents-Papers: [https://github.com/AGI-Edgerunners/LLM-Agents-Papers](https://github.com/AGI-Edgerunners/LLM-Agents-Papers)

## ê²°ë¡  ë° í‰ê°€

2024ë…„ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì—°êµ¬ëŠ” ì‹¤ìš©ì ì¸ í™œìš© ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ê³  ìžˆë‹¤. ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸, íš¨ìœ¨ì„± ê°œì„ , í’ˆì§ˆ ê²€ì¦, ì†Œí˜• LLM ê°œë°œ ë“±ì€ LLM ì—ì´ì „íŠ¸ê°€ ë”ìš± ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í™œìš©ë  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ì¤‘ìš”í•œ ë°œê±¸ìŒì´ë‹¤. íŠ¹ížˆ, ê°œë°œìž ë„êµ¬ì™€ì˜ í†µí•©ì€ LLM ì—ì´ì „íŠ¸ ê°œë°œì˜ ì§„ìž… ìž¥ë²½ì„ ë‚®ì¶”ê³ , ë” ë§Žì€ ê°œë°œìžë“¤ì´ LLM ê¸°ìˆ ì„ í™œìš©í•  ìˆ˜ ìžˆë„ë¡ í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.  ë‹¤ë§Œ, LLM ì—ì´ì „íŠ¸ì˜ ìœ¤ë¦¬ì  ë¬¸ì œ (íŽ¸í–¥ì„±, ì±…ìž„ ë¬¸ì œ ë“±) ì— ëŒ€í•œ ì—°êµ¬ì™€ ë…¼ì˜ë„ ë”ìš± í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì ¸ì•¼ í•  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ  ë°œì „ì€ ì¸ê°„ì˜ ëŠ¥ë ¥ì„ ì¦ê°•ì‹œí‚¤ê³  ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìžˆì§€ë§Œ, ë™ì‹œì— ë°œìƒí•  ìˆ˜ ìžˆëŠ” ìž ìž¬ì ì¸ ìœ„í—˜ì— ëŒ€í•œ ëŒ€ë¹„ë„ í•„ìš”í•˜ë‹¤. ì•žìœ¼ë¡œ LLM ì—ì´ì „íŠ¸ ê¸°ìˆ ì€ ë”ìš± ë°œì „í•˜ê³ , ìš°ë¦¬ ì‚¬íšŒì— ë” í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.
======================================================================


ðŸ“Š Running benchmark: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜...
ðŸ“‹ Planner: Creating research plan for: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜...

ðŸ“‹ Planner: Generated 4 queries
   â””â”€ Queries:
      [1] RAG vs Agent based approach
      [2] Retrieval Augmented Generation vs Agent difference
      [3] When to use RAG
      [4] When to use Agent based approach
   â””â”€ Focus: RAG, Agents, Hybrid approaches, Use cases
ðŸ” Searcher [1]: Searching for: RAG vs Agent based approach

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
      [2] https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/
      [3] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [4] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [5] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
   â””â”€ Snippets:
      â€¢ ### Traditional RAG vs Agentic RAG  The key differences between traditional RAG and agentic RAG center on agency, workflow, and adaptability. Traditional RAG uses a static workflow. It relies on predefined queries and structured input. Agentic RAG introduces autonomous AI agents that adjust prompts and strategies in real time. This agentic approach enables dynamic decision-making and iterative reasoning. [...] Agentic RAG brings high-level adaptability and advanced reasoning to AI-powered systems. The agentic approach uses autonomous decision-making to handle complex, multi-step queries. Organizations benefit from agentic RAGâ€™s ability to adapt in real time and interpret user intent. The system supports digital transformation by integrating domain-specific ai agents for various applications. [...] Recent comparative studies show that traditional RAG follows a linear path and fits simple AI tasks, while Agentic RAG introduces intelligent agents for query reformulation and iterative refinement. Agentic RAG enables smarter decision-making, greater automation, and improved accuracy in AI systems. Companies use Agentic RAG to scale AI across business units and deliver better customer experiences. Matching the right ai solution to each use case matters. Organizations should consider data
      â€¢ In all fairness, thereâ€™s no single â€œwinnerâ€ across all scenarios. Fine-tuning shines in high-volume, domain-specific workloads; RAG thrives where data changes quickly; and agents unlock new possibilities for orchestrated, multi-step tasks. The key is to align the approach with your business model, growth plans, and tolerance for complexity.  ### Key takeaway [...] A quick reminder: fine-tune for stability, RAG for adaptability, and agents for orchestration. Combining these approaches creates a hybrid AI strategy that balances precision, flexibility, and scalability. Fine-tuned models handle repetitive or compliance-heavy tasks reliably, RAG keeps knowledge up-to-date and transparent, and agents automate multi-step workflows across systems. This layered approach minimizes risks, reduces maintenance overhead, and maximizes business impact, ensuring AI [...] Fine-tuning, RAG, and agents each solve different categories of problems, and no single method is a universal answer. Fine-tuning shines when domain expertise must be baked into the model itself. RAG ensures real-time accuracy by grounding outputs in fresh or proprietary data. Agents excel when tasks demand multi-step reasoning and tool orchestration. The real value comes from knowing when to use each (or how to combine them) so that your AI strategy is efficient, scalable, and aligned with
      â€¢ The table below makes it easy for you to see the trade-offs:   RAG is best for quick wins with static content.  Agentic RAG is a smart bridge when goal-based reasoning is needed.  MCP is for building durable, intelligent systems with real decision-making power. [...] Before diving into comparisons, letâ€™s define each approach in clear, business-relevant terms: what they are, what they do, and where they fit in real-world AI use cases.  ### RAG (Retrieval-Augmented Generation)  RAG enhances large language models (LLMs) by letting them â€œlook things up.â€ Instead of relying only on what the model was trained on, RAG retrieves relevant documents or knowledge from an external source (like a database or vector store) before generating a response.  Use it for: [...] Once weâ€™ve clarified your goals, we design and build using the framework that fits your current business readiness:   RAG â†’ When your priority is fast, accurate answers from internal data.  Example: A knowledge assistant trained on your HR policies or product manuals.   Agentic RAG â†’ When you need goal-driven, step-by-step reasoning with smart retrieval.  Example: A sales research agent that pulls competitive insights and drafts summaries.

ðŸ“– ContentReader: Reading 3 URLs...
ðŸ“– Read URL: https://www.pingcap.com/article/agentic-rag-vs-tra... (8015 chars)
   â””â”€ [https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/]
      Preview: Traditional RAG and Agentic RAG Key Differences Explained Product An open-source distributed SQL database trusted by innovators to power transactional, AI, and other modern applications. Product Overview Deployment Options TiDB Cloud TiDB Self-Managed Pricing Ecosystem Integrations TiKV TiSpark OSS Insight Solutions Customer Stories Trusted and verified by innovation leaders around the world. By Industry AI Fintech eCommerce SaaS By Use Case Lower Infrastructure Costs Enable Operational Intelligence Modernize MySQL Workloads Build GenAI Applications Resources Learn Blog eBooks &amp; Whitepapers Videos &amp; Replays Horizontal Scaling Engage Events &amp; Webinars Discord Community Developer Hub TiDB SCaiLE PingCAP University Courses Hands-on Labs Certifications Company Trust Hub Explore how TiDB ensures the confidentiality and availability of your data. About Press Releases &amp; News About Us Careers Partners Contact Us Docs Sign In Start for Free Product TiDB Overview --> An open-source distributed SQL database trusted by innovators to power transactional, AI, and other modern applications. Product Overview Deployment Options TiDB Cloud TiDB Self-Managed Pricing Ecosystem Integrations TiKV TiSpark OSS Insight Solutions Customer Stories Customer Stories --> Trusted and verified by innovation leaders around the world. By Industry AI Fintech eCommerce SaaS By Use Case Lower Infrastructure Costs Enable Operational Intelligence Modernize MySQL Workloads Build GenAI Applications Resources Learn Blog eBooks &amp; Whitepapers Videos &amp; Replays Horizontal Scaling Engage Events &amp; Webinars Discord Community Developer Hub TiDB SCaiLE PingCAP University Courses Hands-on Labs Certifications Company Trust Hub Trust Hub --> Explore how TiDB ensures the confidentiality and availability of your data. About Press Releases &amp; News About Us Careers Partners Contact Us Docs Sign In Start for Free Traditional RAG and Agentic RAG Key Differences Explained TiDB Team Traditional RAG uses a simpler workflow and suits static tasks. Agentic RAG offers greater adaptability and handles complex, multi-step queries. Industry experts note that traditional RAG works well for small applications with fixed FAQs. Agentic RAG fits evolving tasks and can dynamically use multiple knowledge sources. Choosing between these systems matters because selecting the right AI solution affects reliability, cost, and capability. Key Differences Traditional RAG vs Agentic RAG The key differences between traditional RAG and agentic RAG center on agency, workflow, and adaptability. Traditional RAG uses a static workflow. It relies on predefined queries and structured input. Agentic RAG introduces autonomous AI agents that adjust prompts and strategies in real time. This agentic approach enables dynamic decision-making and iterative reasoning. Approach Traditional RAG Agentic RAG Methodology Reactive, relies on predefined queries Proactive, autonomously determines needs Human Guidance Requires explicit human guidance Operates with minimal human intervention Adaptability Limited adaptability High adaptability, integrates diverse data Problem-Solving Static information retrieval Active problem-solving through dynamic retrieval Traditional RAG depends on human guidance. It cannot refine responses without manual intervention. Agentic RAG operates with minimal human input. Autonomous AI agents analyze context and user intent. They continuously re-evaluate information retrieval strategies. This agentic process enhances adaptability and performance. Agency and Autonomy Agency and autonomy define the agentic RAG approach. Autonomous AI agents in agentic RAG decide which information to search and how to process it. They adjust prompts dynamically based on goals and context. This agentic capability allows for real-time adaptability. Agentic RAG employs autonomous AI agents for dynamic decision-making. It enhances contextual understanding and adapts to changing user needs. Traditional RAG systems follow static workflows and lack the ability to refine responses. Agentic RAG continuously analyzes context and user intent, allowing for dynamic data retrieval. The key differences between traditional RAG and agentic RAG highlight the importance of agency, workflow flexibility, and adaptability. Agentic RAG leverages autonomous AI agents to deliver advanced reasoning and dynamic problem-solving. Traditional RAG provides reliable results for simple, static tasks but cannot match the agentic approach in complex scenarios. Retrieval-Augmented Generation Explained Retrieval-augmented generation (RAG) combines the strengths of information retrieval and natural language generation. This approach allows AI systems to fetch relevant data from external sources and generate coherent responses. The core features of RAG include a retrieval module and a generation module. These modules work together to improve the accuracy and relevance of ai outputs. Traditional RAG Features Traditional rag uses a straightforward workflow. The retrieval module identifies and fetches documents from a knowledge base using vector search and keyword matching. The generation module then combines this data with the original query to create a response. The augmentation step integrates retrieved information through document concatenation and embedding-based integration. The generation phase processes the augmented input using transformer architectures. Core Component Description Retrieval Module Identifies and fetches relevant documents from external sources based on a given query. Generation Module Combines retrieved data with the original input to synthesize a coherent response. Retrieval Process Involves searching through a knowledge base using vector search and keyword matching. Augmentation Step Integrates retrieved information with the original query through document concatenation and embedding-based integration. Generation Phase Processes the augmented input using transformer architectures and fine-tuning strategies for coherent responses. Traditional rag handles queries in a single pass. It relies on fixed retrieval strategies and limited context windows. Multi-step reasoning requires extra classifiers and models. In practical applications, AI teams use metrics like MRR, nDCG, Precision, Recall, and F1 to evaluate performance. For customer support, AI measures resolution rate and customer satisfaction. For sales enablement, ai tracks deal acceleration and content utilization. Agentic RAG Features Agentic rag introduces autonomy and adaptability. Autonomous AI agents identify missing elements and seek out information for task completion. They use dynamic information retrieval to access real-time data. Augmented generation integrates external information into contextually relevant responses. A feedback loop refines outputs based on user feedback, enabling continuous improvement. Autonomy: Agents identify and seek out missing elements for task completion. Dynamic Retrieval: Agents access real-time data for accurate outputs. Augmented Generation: Agents integrate external information into coherent responses. Feedback Loop: Agents refine responses based on feedback for continuous improvement. Agentic rag breaks queries into sub-queries and adapts retrieval strategies based on context. It excels at multi-step reasoning across documents. The system interacts with tools and databases without needing extra classifiers. Agentic rag demonstrates scalability by adjusting task granularity and scheduling tasks dynamically. Ai systems using agentic rag show enhanced responsiveness and robustness, even in complex environments. Benefits Traditional RAG Advantages Traditional RAG offers several advantages for organizations seeking reliable AI solutions. Teams often choose traditional rag for its simplicity and speed. The system uses a straightforward workflow, which reduces operational complexity. Many businesses find traditional rag cost-effective for sta... [truncated]
ðŸ“– Read URL: https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-... (8015 chars)
   â””â”€ [https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/]
      Preview: LLM fineâ€‘tuning vs. RAG vs. agents: a practical comparison - MITRIX Technology Home Services AI development for startups AI/ML & generative AI Consulting Cloud development Business intelligence Digital transformation Mobile development Web development Portfolio Dedicated Team Team augmentation Startup development Blog Contacts --> contact us Artificial intelligence Technical debt LLM fineâ€‘tuning vs. RAG vs. agents: a practical comparison September 29, 2025 7 min reading It&#8217;s fair to say that the past few years have reshaped natural language processing (NLP) and AIâ€‘driven applications. Nowadays, businesses face a pivotal question: â€œ Whatâ€™s the best way to harness large language models (LLMs) for their specific needs?â€ Three primary approaches dominate the landscape: LLM fineâ€‘tuning, retrievalâ€‘augmented generation (RAG), and AI agents. Each has unique strengths, tradeâ€‘offs, and implications for cost, scalability, and control. The differences, use cases, and a practical LLM comparison are essential for decision-makers seeking clarity. Key considerations include LLM leaderboard rankings, LLM benchmark performance, and best practices for LLM training, all of which guide informed choices. In this article, youâ€™ll learn: Why LLM fineâ€‘tuning, RAG, and AI agents matter for business AI. How fineâ€‘tuning embeds expertise, ensures consistency, and supports compliance. How RAG delivers fresh, explainable insights without retraining. How agents automate multi-step workflows across systems. Cost, scalability, and hybrid strategies for real-world adoption . Practical steps for piloting, benchmarking, and scaling anything LLM. How Mitrix helps implement tailored LLM solutions with AI agents, fine-tuning, and RAG. Why the comparison matters Naturally, not all AI problems need the same solution. Some demand custom expertise embedded in the model (fineâ€‘tuning). Others rely on realâ€‘time access to fresh or proprietary data (RAG). And increasingly, enterprises deploy multiâ€‘step AI agents that combine reasoning, planning, and tool use. Without a clear framework, teams risk over-engineering or under-delivering. Thatâ€™s where a practical comparison helps. It sharpens decision-making by matching solutions to real business needs. This practice also prevents wasted investment in approaches that donâ€™t scale or align with long-term goals. RAG/CAG/Fine-tuning comparison Key takeaway Fine-tuning, RAG, and agents each solve different categories of problems, and no single method is a universal answer. Fine-tuning shines when domain expertise must be baked into the model itself. RAG ensures real-time accuracy by grounding outputs in fresh or proprietary data. Agents excel when tasks demand multi-step reasoning and tool orchestration. The real value comes from knowing when to use each (or how to combine them) so that your AI strategy is efficient, scalable, and aligned with business objectives. LLM fineâ€‘tuning: embedding expertise into the model Fineâ€‘tuning refers to adjusting a preâ€‘trained model on a narrower dataset so that it better aligns with a companyâ€™s domain, style, or compliance needs. Advantages Custom expertise . Embeds industryâ€‘specific knowledge (e.g., legal, medical, or financial terminology). Consistency . Produces more predictable outputs for repetitive, structured tasks. Data security . Keeps sensitive workflows internal when paired with private LLM training pipelines. Challenges Cost . Training and hosting fineâ€‘tuned models can be expensive. Maintenance . Models can become stale if regulations or data shift. Complexity . Requires understanding of how to finetune LLM effectively. Best practices If youâ€™re exploring how to fineâ€‘tune LLM systems, start with: Curated datasets . Quality beats quantity: clean, representative samples matter most. Instruction tuning . Aligning outputs with human preferences. Benchmarking . Use an LLM leaderboard or LLM benchmark to measure progress against baselines. Use cases Customer service with domainâ€‘specific compliance. Contract analysis in law firms. Clinical decision support in healthcare . Key takeaway In a nutshell, LLM fine-tuning shines when stability, compliance, and domain expertise are non-negotiable. It embeds specialized knowledge directly into the model, delivers consistent outputs, and safeguards sensitive data , though at the cost of higher training and maintenance demands. For industries with strict requirements, itâ€™s the most reliable way to align AI with business realities. Retrievalâ€‘augmented generation (RAG): injecting fresh knowledge Unlike fineâ€‘tuning, RAG doesnâ€™t alter the base model. Instead, it enhances responses by retrieving documents from a knowledge base at query time. Think of it as â€œopenâ€‘book AI.â€ Advantages Dynamic knowledge . Access to realâ€‘time or frequently updated data. Lower cost . No need for repeated LLM training. Explainability . Ability to cite sources for improved trust. Challenges Latency . Retrieval adds overhead. Quality control . Poorly indexed data equals poor results. Limited reasoning . The model still relies on external context stitching. Best practices Use vector databases for efficient semantic search. Regularly update knowledge repositories. Benchmark retrieval quality against domainâ€‘specific LLM comparison tests. Use cases Enterprise search and knowledge management. Product support documentation. Financial research with realâ€‘time market data. Key takeaway RAG excels when access to up-to-date information is critical. It keeps models dynamic, cost-efficient, and explainable, while avoiding repeated LLM training. However, performance depends on high-quality indexing, efficient retrieval, and careful monitoring to ensure accuracy and reliability. AI agents: multiâ€‘step reasoners and doers While fineâ€‘tuning and RAG improve single interactions, agents represent a different paradigm. They are systems that use LLMs as reasoning engines but also interact with tools, APIs , and other agents to complete multiâ€‘step workflows. Advantages Autonomy . Capable of executing complex tasks with minimal supervision. Flexibility . Can combine RAG, fineâ€‘tuning, and external tools. Scalability . Extendable through modular architectures. Challenges Reliability . Prone to â€œ hallucinations â€ without guardrails. Governance . Harder to audit multiâ€‘step reasoning. Engineering complexity . Requires orchestration frameworks. Best practices Define clear boundaries for agent autonomy. Monitor workflows continuously. Leverage LLM benchmarks to evaluate reasoning performance. Use cases Automated research assistants. Supply chain optimization. Customer onboarding journeys that require multiple systems. Key takeaway To cut a long story short, AI agents take LLM capabilities beyond single-turn answers by chaining reasoning with action. They offer autonomy, flexibility, and scalability, but come with reliability and governance challenges that require careful orchestration. Teams that define clear guardrails, monitor workflows, and ground performance with LLM benchmarks can unlock use cases from automated research to end-to-end customer journeys. LLM leaderboards, benchmarks, and reality checks In 2025, the surge of public LLM leaderboards has made it easier to track how different models perform across tasks. But remember: benchmarks are abstractions. They often emphasize math, reasoning, or academic QA tasks, which may not reflect your business context. When running an LLM comparison for your project: Use public LLM benchmarks as a starting point. Create internal benchmarks tied to your data. Test across scenarios like latency, compliance, and user satisfaction. Benchmarks tell you how a model performs in theory, while pilots tell you how it performs in practice. Another limitation is that leaderboard results usually come from controlled test sets and donâ€™t capture operational realities. For example, a model may rank high on reasoning tasks but struggle when integrated into workflows with strict latency requirements. Cost... [truncated]
ðŸ“– Read URL: https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/... (8015 chars)
   â””â”€ [https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/]
      Preview: RAG vs Agentic RAG vs MCP: A 2025 Comparison Guide for Business Leaders Skip to main content Get awesome and let's build together! +1 858-683-3692 Hit enter to search or ESC to close Close Search search Menu Services Web Development Full Stack Development SaaS Development Ruby on Rails Development Node.js Development React Development Angular Development Vue.js Development eCommerce Development PHP Development Firebase Development WordPress Development WooCommerce Development Shopify Development Shopify Apps Development Mobile App Development iOS App Development Android App Development Progressive Web Application React Native App Development Hybrid App Development Flutter App Development Roku App Development Apple tvOS App Development Apple WatchOS App Development Tablet App Development Google Tv App Development UX UI Services Enterprise Digital Transformation Web Data Extraction Scraping Big Data and Business Intelligence AWS Serverless Computing Robotic Process Automation AWS Cloud Computing AI &#038; Machine Learning Development DevOps Services Cloud Computing Services Business Software Development Software Development Partner Contentful Digital Agency Chia Blockchain farming Automation &#038; AI AI Development Services Power Platform Consulting Chatbot Development GenAI Integration Services AI Consulting Services AI Development with v0 &#038; Cursor Healthcare AI Agents AI Agent Development Workflow automation Services CMS &#038; eCommerce Identity &#038; Access Management Modern Data Stack Low Code Development Industries Healthcare Wound Care Software Development EHR/EMR Software Development HMO Automation Software Advance Imaging Software Solutions Telemedicine Software Development Services RPM Software Development Healthcare Mobile App Development Medical Device Software Development Life Sciences Software Development Healthcare Web Design &#038; Development Manufacturing Fitness FinTech Non-Profit eCommerce Retail Startups Our Work Solutions Cloud Solutions Cloud Application Development Custom Solutions Digital SMS/IVR Will Call with Twilio Digital Inspection Platform Digital Lending Platform Custom Software Development Marketplace Platform IoT Development Appointment Scheduling Software HME Software Development Business Software Solutions No-Code Solutions RPA Solution CRM Integrations Integrations Payment Gateway Google Calendar Integration Google Maps Integration OAuth Authentication Twilio API Integration OKTA Integration NetSuite Integration Salesforce API Integration SugarCRM &#038; SuiteCRM Integration VGS Integration Partner Hire Us Hire Mobile App Developer Hire Ruby on Rails Developer Hire Swift Developer Hire Full Stack Developer Hire Xamarin Developer Hire PHP Developer Hire Vue.js Developer Hire Angular JS Developer Hire React Developers Hire Node.js Developer Hire DevOps Engineer Blog Jumpstart My Project search RAG vs Agentic RAG vs MCP: A 2025 Comparison Guide for Business Leaders By Raj Sanghvi June 24, 2025 AI , Automation ChatGPT Perplexity Claude Social Facebook Twitter LinkedIn WhatsApp Email No Comments For business leaders exploring AI-powered solutions, terms like RAG, Agentic RAG, and MCP are now at the center of strategy discussions. But what do they actually mean, and which one is right for your organization? This guide cuts through the technical jargon and gets straight to the point: What are these AI frameworks? How do they differ in capabilities and complexity? Which is best suited for your goals? Whether youâ€™re leading a digital transformation initiative or simply looking to make smarter decisions with AI, this side-by-side comparison of Retrieval-Augmented Generation (RAG), Agentic RAG, and the Model Context Protocol (MCP) will help you confidently choose the right approach. Letâ€™s break it down. Contents hide 1 What Are RAG, Agentic RAG, and MCP? 1.1 RAG (Retrieval-Augmented Generation) 1.2 Agentic RAG 1.3 MCP (Model Context Protocol) 2 How RAG, Agentic RAG, and MCP Differ: Side-by-Side Comparison 3 Which One Should You Use: RAG, Agentic RAG, or MCP? 4 Why Bitcot is the Right Partner No Matter Which AI Framework You Choose 5 Final Thoughts 6 FAQs What Are RAG, Agentic RAG, and MCP? Before diving into comparisons, letâ€™s define each approach in clear, business-relevant terms: what they are, what they do, and where they fit in real-world AI use cases. RAG (Retrieval-Augmented Generation) RAG enhances large language models (LLMs) by letting them â€œlook things up.â€ Instead of relying only on what the model was trained on, RAG retrieves relevant documents or knowledge from an external source (like a database or vector store) before generating a response. Use it for: Chatbots that answer based on company knowledge. Customer service bots with access to FAQs and manuals. Sales reps that summarize case studies or product info on demand. Strength: High-accuracy answers using up-to-date, controlled content. Limitation: Passive; answers only whatâ€™s asked, no autonomy or reasoning. Agentic RAG Agentic RAG adds goal-driven reasoning to traditional RAG. RAG AI agents and Agentic RAG represent two stages of intelligent retrieval systems, starting with basic response generation and evolving into systems with planning and tool-use capabilities. This evolution introduces a layer of autonomy that RAG alone cannot achieve. Instead of just answering questions, the AI becomes an agent that can plan steps, retrieve multiple pieces of information over time, use tools (like web search or APIs), and reflect on progress toward a goal. Use it for: Research agents that proactively gather insights. Internal assistants that answer, verify, and summarize from multiple sources. Complex workflows where context evolves. Strength: Autonomy + search = more useful, more flexible. Limitation: Harder to control, test, or explain compared to simple RAG. RAG AI agents and Agentic RAG represent two stages of intelligent retrieval systems, starting with basic response generation and evolving into systems with planning and tool-use capabilities. This evolution introduces a layer of autonomy that RAG alone cannot achieve. MCP (Model Context Protocol) MCP is a framework for building modular, transparent, and fully autonomous AI agents. It organizes everything an agent needs, its memory, tools, instructions, and roles, into a reusable â€œprotocolâ€ that drives how it reasons, acts, and evolves. Use it for: Building AI agents with MCP that interact seamlessly with tools, data, and people. Agents that learn from feedback and adapt over time. Enterprise AI thatâ€™s structured, auditable, and explainable. Strength: Full agent control, modularity, and long-term memory. Limitation: Requires thoughtful design; best used when agents must act autonomously with accountability. How RAG, Agentic RAG, and MCP Differ: Side-by-Side Comparison The comparison of MCP vs RAG in AI highlights the progression from simple retrieval-based responses (RAG) to more dynamic, goal-oriented systems (Agentic RAG), and ultimately to MCP, offering long-term memory, autonomy, and multi-system integration. The table below makes it easy for you to see the trade-offs: RAG is best for quick wins with static content. Agentic RAG is a smart bridge when goal-based reasoning is needed. MCP is for building durable, intelligent systems with real decision-making power. Feature / Capability RAG Agentic RAG MCP Primary Function Answering questions using retrieved content Multi-step reasoning with goal-driven retrieval Building modular, fully autonomous agents Autonomy None Partial (task-focused) Full autonomy (reasoning + action) Memory Stateless Limited (per session/task) Persistent memory &amp; state tracking Use of Tools None Can use tools/APIs during reasoning Tool-using via structured interface Task Complexity Simple Q&amp;A Medium-complex (e.g., research, synthesis) High-complex (multi-tool, multi-step workflows) Best For Knowledge chatbots, FAQ bots Research agents, assistant-style tasks Enterprise agen... [truncated]
ðŸ”¬ Analyzer [1]: Analyzing 5 results, 3 contents

ðŸ”¬ Analyzer [1]: Analyzed 5 results, 3 contents
   â””â”€ New findings:
      [1] Traditional RAG uses a static workflow, while Agentic RAG uses autonomous AI agents that adjust prompts and strategies in real time.
      [2] RAG is suitable for quick wins with static content.
      [3] Agentic RAG is useful when goal-based reasoning is needed.
      [4] Traditional RAG has simple retrieval, limited adaptability, and relies on static knowledge.
      [5] Agentic RAG uses AI agents that can make decisions, select tools, and refine queries.
   â””â”€ Decision: More research needed
   â””â”€ Next query: real world examples of RAG vs agentic RAG
ðŸ” Searcher [2]: Follow-up search for: real world examples of RAG vs agentic RAG

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://www.sprinklr.com/blog/agentic-ai-vs-rag/
      [2] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [3] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [4] https://fractal.ai/article/rag-vs-agentic-ai
      [5] https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis
   â””â”€ Snippets:
      â€¢ ### Practical examples of agentic AI vs RAG  When discussing Agentic AI vs. RAG, it helps to see them in action. Both approaches solve real enterprise problems, but in different ways.  Example 1: Agentic AI in workflow orchestration  Imagine your IT service desk bot tasked with resolving an outage ticket. Instead of just generating text, an agentic AI system can: [...] Example 3: ChatGPT Agents as hybrid systems  An increasingly familiar case is ChatGPT Agents. These systems blend RAG and agentic reasoning:   They use retrieval to ground outputs in enterprise knowledge (e.g., product manuals, policies, internal wikis).  They use agentic loops to plan tasks and take actions, from searching the web to calling APIs or updating records in business systems. [...] The debate around Agentic AI vs RAG often frames them as competing approaches. In reality, enterprise success depends on orchestrating both. RAG ensures responses are accurate, compliant, and grounded in trusted knowledge. Agentic AI extends this by driving decisions, coordinating workflows, and adapting in real time. Separately, each has limits. Together, they unlock scalable, resilient, and future-proof operations.
      â€¢ user for clarification if needed, look up the userâ€™s account info via an API, and even proactively follow up. IBMâ€™s example highlights that a traditional RAG is like an employee waiting for instructions, whereas an agentic system is more like a proactive team that â€œtake initiative and solve challenges on their ownâ€. In a multi-turn workflow, the agent can decide at each turn what the best course of action is â€” whether thatâ€™s fetching more data or executing a step. This leads to more intelligent [...] Complex Query Handling: If your application needs to handle complex tasks, multi-part questions, or open-ended requests that require decision-making or multiple operations, lean towards agentic RAG. For example, an AI assistant that can research a topic and produce a report (involving gathering info from various sources) would benefit from an agentic approach. Traditional RAG would struggle with such a query unless it was manually broken down by the user or developer. Agentic RAG shines when [...] Dynamic Task Execution: Agentic RAG blurs the line between Q&A and action-taking. In many applications, a user query might not just ask for information but implicitly request an action. For example, â€œSchedule a meeting with John for next weekâ€ or â€œWhatâ€™s the current status of order #12345?â€ or â€œSummarize the attached report and email it to my team.â€ A pure RAG system that only searches documents would fall short here. With an agentic design, the system can incorporate operation-specific tools:
      â€¢ Traditional RAG systems are context-aware to a limited extent, as they retrieve relevant information and augment the context for better responses.  Agentic RAG systems are highly context-aware. The agents assess the query, decide which tools to use, and ensure that the retrieved data is contextually relevant and integrated effectively.  ### Real-World Applications of RAG and Agentic RAG  ### Customer Support: [...] Traditional RAG systems are useful for content creation tasks that require AI to retrieve information from various sources to generate articles, blogs, or reports.  Agentic RAG systems can take this further by generating highly customized content that requires reasoning, such as creating marketing materials based on the latest trends, or producing reports with visualizations like graphs or charts.  ### Healthcare: [...] Traditional RAG relies on a single retrieval system, such as a vector database, to retrieve relevant information.  Agentic RAG, however, is deeply integrated with multiple retrieval systems, and the agents dynamically choose which system to use based on the context and complexity of the query.  ### Context-Awareness:

ðŸ“– ContentReader: Reading 3 URLs...
ðŸ“– Read URL: https://www.sprinklr.com/blog/agentic-ai-vs-rag/... (8015 chars)
   â””â”€ [https://www.sprinklr.com/blog/agentic-ai-vs-rag/]
      Preview: Agentic AI vs RAG: Choosing the Right Fit for Enterprises | Sprinklr Demo Login Support Sprinklr Products Product Suites Sprinklr Service Sprinklr Social Sprinklr Insights Sprinklr Marketing Platform Unified-CXM Sprinklr Service Overview Inbound Contact Center Outbound Contact Center Social Customer Service Live Chat Support Sprinklr VoiceConnect Omnichannel Routing Unified Agent Desktop Supervisor Console Conversational IVR Workforce Management Community Software AI Agent Platform Agent Copilot Quality Management Conversational Analytics Knowledge Base Platform Guided Workflows Omnichannel Surveys Reporting and Analytics Service Command Center Sprinklr Social Overview Social Publishing and Engagement Social Marketing for Distributed Teams Employee Advocacy Works Best With Social Listening Social Advertising Conversational Commerce Social Customer Service Sprinklr Insights Overview Social Listening Competitive Insights and Benchmarking Product Insights Location Insights Visual Insights Sprinklr Surveys Sprinklr Marketing Overview Content Marketing and Campaign Planning Social Advertising Marketing Analytics Ad Comment Moderation Unified-CXM Overview Integrations Sandbox Security APIs Active Data Retention Display Presentations SPOTLIGHT Sprinklr named a Leader in The Forrester Waveâ„¢: Social Suites, Q4 2024 Read full report Sprinklr named a Strong Performer in The Forrester Waveâ„¢: CCaaS Platforms, Q2 2025 Read full report Solutions For Industries For Teams CPG Financial Services Government Retail Technology Telecommunications Travel &amp; Hospitality Marketing Teams Customer Service Teams CX Teams Marketing Teams Overview Content Lifecycle Management Social Media Management Social Advertising Customer Service Teams Overview Omnichannel Contact Center Customer Self-Service Digital Customer Service Outbound Campaign Management Contact Center Intelligence CX Teams Overview Customer Feedback Management Contact Center Intelligence SPOTLIGHT Aramex delivers world-class CX with AI-powered CCaaS and 24/7 customer care Read full story Acer streamlines workflows across global markets and transforms the customer experience Read full story Sprinklr AI Overview Sprinklr AI Agents Sprinklr Copilot AI+ Studio SPOTLIGHT Check your AI maturity in minutes and get a free report with strategies to improve your AI readiness Get started Northwestern Mutual uses Sprinklr AI to power its most successful social campaign ever Read full story Customers Customer Stories Sprinklr Champions Sprinklr Community CXUnifier Awards SPOTLIGHT Uber achieves industry-leading SLA and response times Read full story Deutsche Telekom rings in the future of CX with AI-powered CCaaS Read full story Company Our Story Leadership Newsroom Partners Careers Investor Relations Security &amp; Data Privacy Sustainability SPOTLIGHT Sprinklr wins Cloud-Based CX Solution of the Year at the 2025 CCW Excellence Awards Learn more Join a culture where kindness and customer obsession defines everything we do Learn more Resources Learn Services Support Sprinklr Service ROI Estimator AI Maturity Assessment CX-WISE Analyst Reports Forbes Influential CMOs 2025 Sprinklr Demo Center eBooks &amp; Reports Events &amp; Webinars Blog Unified-CXM Guide Videos Our Services Training For Agencies Help Center Release Notes Contact Us SPOTLIGHT [New Podcast] Christina Garnett on The CX Insights Youâ€™re Missing Play now Powering CX Excellence: How Scalable GenAI Drives Customer Loyalty Read full report Demo The strategic AI-native platform for customer experience management The strategic AI-native platform for customer experience management Unify your customer-facing functions â€” from marketing and sales to customer experience and service â€” on a customizable, scalable, and fully extensible AI-native platform. Request Demo Blog Home Blog Categories Unified-CXM Platform &amp; Technology Customer Service Marketing &amp; Advertising Research &amp; Insights Social Media Management Announcements Culture &amp; Talent AI Guides Service Read guides on streamlining customer service and support. Marketing Learn tactics for audience engagement and brand visibility. Social Find insights on how to utilize social media for growth. Insights Learn to analyze data for strategic consumer insights and research. Experience Explore how to enhance the customer experience journey. Glossary Customer Service Glossary Social Media Glossary Podcasts CX-WISE AI-Wise Reports Industry Reports Ebooks MORE AI Agentic AI vs RAG: Choosing the Right Approach for Enterprise AI Strategy October 3, 2025 â€¢ 14 MIN READ Authors Bhavya Aggarwal Jayadeep Subhashis Senior Content Specialist Agentic AI vs RAG is one of the most important architectural choices enterprises face today. Agentic AI has captured executive attention because it promises more than just automation. It enables LLMs to reason across steps, invoke tools, and execute tasks in real systems. Retrieval-Augmented Generation (RAG), on the other hand, grounds LLM outputs in enterprise knowledge sources to reduce hallucinations and improve trust. These two approaches are often positioned as opposites, but in practice, they complement each other. Many agentic systems rely on RAG for factual grounding, and RAG pipelines can be extended with agentic action layers. In this blog, weâ€™ll break down Agentic AI vs RAG in plain terms, highlight where they differ, where they overlap, and provide a practical framework to help you choose with confidence for your use case. By the end, youâ€™ll have clarity on whether to deploy RAG, Agentic AI, or a hybrid, and the trade-offs each path entails for your business. Table of Contents Agentic AI vs RAG: What is the difference? Agentic AI vs RAG: Where each excels and when to use which? Practical criteria to guide your Agentic AI vs RAG decision From either/or to unified intelligence Agentic AI vs RAG: What is the difference? Agentic AI is designed to act, not just respond. Instead of passively answering prompts, it can independently plan, reason across steps, and execute multi-step workflows. Agentic systems dynamically build reasoning chains, apply conditional logic, and escalate when needed. For example, an agentic AI assistant in customer service might attempt to troubleshoot an issue using knowledge bases and APIs. If it recognizes the case exceeds automation thresholds, it can seamlessly hand over to a human agent â€” passing along the full interaction history for context. This autonomy makes agentic systems well-suited for workflows requiring adaptation, coordination, and resilience. Read : 5 Real-World Agentic AI Use Cases for Enterprises ðŸ’¡Do you know Gartner predicts that by 2029, agentic AI will autonomously resolve up to 80% of routine customer interactions without human intervention, underscoring its disruptive potential for enterprise support operations. Retrieval-Augmented Generation (RAG) , by contrast, strengthens large language models by grounding them in external knowledge. Instead of relying only on static pre-training, RAG retrieves relevant passages from enterprise content stores such as databases, documents, or APIs, and fuses them into the LLMâ€™s reasoning. The result: context-rich, accurate, and up-to-date responses. This makes RAG indispensable in domains where precision and traceability are crucial, such as responding to compliance queries, synthesizing research, or drafting customer-facing communications. For instance, when drafting a response to a customer about data privacy terms, a RAG system can fetch the latest compliance clause from policy documents, embed it into the modelâ€™s context, and generate a legally accurate reply â€” something a base LLM alone could not guarantee. Practical examples of agentic AI vs RAG When discussing Agentic AI vs. RAG , it helps to see them in action. Both approaches solve real enterprise problems, but in different ways. Example 1: Agentic AI in workflow orchestration Imagine your IT service desk bot tasked with resolving an o... [truncated]
   â””â”€ [https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167]
      Preview: Error: HTTP 403 for URL: https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
   â””â”€ [https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037]
      Preview: Error: HTTP 403 for URL: https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
ðŸ”¬ Analyzer [2]: Analyzing 5 results, 6 contents

ðŸ”¬ Analyzer [2]: Analyzed 5 results, 6 contents
   â””â”€ New findings:
      [1] Traditional RAG uses a static workflow, while Agentic RAG uses autonomous AI agents that adjust prompts and strategies in real time.
      [2] RAG is suitable for content creation tasks and customer support where real-time actions and context-aware generation are not critical.
      [3] Agentic RAG is useful when goal-based reasoning, context-aware generation, and real-time actions are needed, such as in robotics, legal advisory, healthcare diagnostics, and ongoing customer service.
      [4] Traditional RAG has simple retrieval, limited adaptability, and relies on static knowledge.
      [5] Agentic RAG uses AI agents that can make decisions, select tools, and refine queries for more intelligent and dependable AI systems.
   â””â”€ Decision: More research needed
   â””â”€ Next query: RAG vs Agentic RAG implementation examples
ðŸ” Searcher [3]: Follow-up search for: RAG vs Agentic RAG implementation examples

ðŸ” Searcher: Found 5 results
   â””â”€ URLs found:
      [1] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [2] https://pureinsights.com/blog/2025/agentic-rag-vs-traditional-rag/
      [3] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [4] https://developer.nvidia.com/blog/traditional-rag-vs-agentic-rag-why-ai-agents-need-dynamic-knowledge-to-get-smarter/
      [5] https://www.linkedin.com/posts/alexxubyte_systemdesign-coding-interviewtips-activity-7344040591753678849-j-6J
   â””â”€ Snippets:
      â€¢ In summary, traditional RAG is easier to build and maintain initially, whereas agentic RAG requires more sophisticated engineering practices (robust error handling, monitoring, prompt management, etc.) but yields a more adaptable system. Teams should weigh these trade-offs. For a small app answering a fixed set of FAQs, traditional RAG is likely sufficient (and will be cheaper and more reliable). But for a complex assistant meant to handle evolving tasks, the investment in an agentic approach [...] Complex Query Handling: If your application needs to handle complex tasks, multi-part questions, or open-ended requests that require decision-making or multiple operations, lean towards agentic RAG. For example, an AI assistant that can research a topic and produce a report (involving gathering info from various sources) would benefit from an agentic approach. Traditional RAG would struggle with such a query unless it was manually broken down by the user or developer. Agentic RAG shines when [...] Figure: An example Agentic RAG architecture (single-agent router). The agent (LLM with an â€œAgentâ€ role) sits in the loop between the user query and the LLMâ€™s final answer. It can decide among various tools or knowledge sources: e.g., performing a vector DB search on Database A vs. Database B, calling a web search API, or even invoking a calculator. The agent reasons about which tool best fits the query, uses it to retrieve or compute intermediate results, and feeds that into the generation of
      â€¢ Hereâ€™s a simplified example of how to implement an MCP tool for product search, enabling the agent to apply filters like brand and price dynamically. [...] ## Conclusion: Agentic RAG Is the Future of AI Search  Traditional RAG systems brought us closer to practical, information-aware language models. But as enterprise needs evolve toward more complex, interactive, and reliable AI solutions, Agentic RAG with Multi-Component Planning provides the scaffolding for the next frontier.  For organizations aiming to lead in AI-driven transformation, embracing agentic architectures is not just a technical upgradeâ€”itâ€™s a strategic necessity. [...] In other queries that may involve price ranges, the traditional RAG system can retrieve good results, for example:  However, it does not exactly match our requested range close enough, but when using the Agentic RAG approach, we can retrieve way better results based on our defined price range as seen in the following example:
      â€¢ RAG and Agentic RAG are both key advancements in AI, helping LLMs access and generate relevant, context-aware information. Traditional RAG improves LLMs by connecting them to external data sources, while Agentic RAG goes a step further by adding intelligent agents that handle decisions and complex tasks. If the task is simple and query-based, traditional RAG is enough. However, for more complex, multi-step processes, Agentic RAG offers more flexibility, adaptability, and accuracy. As AI [...] Agentic RAG is an evolution of the traditional RAG system. While RAG systems combine retrieval with generation, Agentic RAG introduces agents that play a more active role in the process. These agents are intelligent entities that make decisions about which resources to retrieve, how to process the data, and how to generate the response. In Agentic RAG, the agent orchestrates the entire process, enabling more complex, multi-step tasks that require deeper reasoning, tool integration, and informed [...] 3. Generation: Finally, the LLM uses the augmented input to generate a response. The result is a more accurate and contextually relevant answer.

ðŸ“– ContentReader: Reading 3 URLs...
   â””â”€ [https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167]
      Preview: Error: HTTP 403 for URL: https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
ðŸ“– Read URL: https://pureinsights.com/blog/2025/agentic-rag-vs-... (8015 chars)
   â””â”€ [https://pureinsights.com/blog/2025/agentic-rag-vs-traditional-rag/]
      Preview: Agentic RAG vs. Traditional RAG - Pureinsights Skip to content Company About Partners Careers Use Cases Retrieval Augmented Generation (RAG) AI Search AI Assistants AI Chatbots AI Agents Search as a Full Service E-Commerce Search with Open Source Services Search Application Consulting Search Application Assessment Generative AI Assessment Search Relevance Tuning Managed Services &#8211; AI and Search Elasticsearch Consulting Solr Consulting Algolia Consulting OpenSearch Consulting MongoDB Consulting Vector Search Consulting Technology Discovery Platform Discovery Search Analytics Pricing Resources Blog and News Case Studies Videos White Papers Contact Us Company About Partners Careers Use Cases Retrieval Augmented Generation (RAG) AI Search AI Assistants AI Chatbots AI Agents Search as a Full Service E-Commerce Search with Open Source Services Search Application Consulting Search Application Assessment Generative AI Assessment Search Relevance Tuning Managed Services &#8211; AI and Search Elasticsearch Consulting Solr Consulting Algolia Consulting OpenSearch Consulting MongoDB Consulting Vector Search Consulting Technology Discovery Platform Discovery Search Analytics Pricing Resources Blog and News Case Studies Videos White Papers Contact Us Contact Us Search Contact Us Blog / Agentic RAG vs. Traditional RAG Agentic RAG vs. Traditional RAG May 21, 2025 Fabian Solano Senior Software Engineer, Pureinsights Evolving search applications with Model Context Protocol (MCP) As enterprises push the boundaries of what AI can achieve, the limitations of early-generation systems like traditional RAG (Retrieval-Augmented Generation) are becoming more evident. This blog explores how Agentic RAGâ€”enhanced by Model Context Protocol (MCP) â€”offers a more dynamic, tool-aware, and autonomous alternative, capable of solving increasingly complex real-world tasks. Agentic RAG vs. Traditional RAG: Whatâ€™s the Difference? In recent years, Retrieval-Augmented Generation (RAG) has emerged as a foundational architecture for enterprise AI applications, especially in search, chatbots, document analysis, and decision support. Traditional RAG systems have helped bridge the gap between static language models and the dynamic, evolving needs of real-world information. However, the limitations of this approach are becoming increasingly apparent in complex enterprise use cases. Enter Agentic RAG , an evolution of RAG powered by MCP (Model Context Protocol) a smart adapter mechanism that provides context, tools, and prompts to AI clients. It can expose data sources like files, documents, databases and perform context aware executions. Traditional RAG: Powerful but Procedural At its core, a traditional RAG system consists of three primary components: Retriever : fetches relevant documents from a knowledge base with semantic context (for example using Vector-based search) Generator : uses an LLM (Large Language Model) such as GPT, LlaMA or Gemini to synthesize responses using the retrieved context. Prompt Template : orchestrates how information flows between retrieval and generation to provide an answer to the user. Limitations of Traditional RAG Despite its utility, the approach faces key limitations: Linear Process : The retriever-generator pipeline is often static and non-adaptive. It cannot change strategy mid-task or iterate based on intermediate feedback. Lack of Reasoning : it doesnâ€™t inherently support multi-step reasoning or decision-making. Complex questions often require chaining multiple retrievals and computations. Poor Coordination : Tasks that involve interacting with multiple tools (e.g., search APIs, calculators, or internal systems) often require external orchestration logic, increasing implementation overhead. Letâ€™s go over some examples, to help identify the strengths and key limitations of RAG systems. We will use a semantic search example for an e-commerce dataset. If a user wants to find a â€œbarbecue grillâ€ a natural semantic search would be something like: â€œI need to buy a barbecue grillâ€: Pretty good result, right? Semantic search helped narrowing down the results to match relevant products for â€œbarbecue grillsâ€ and recommended the product with the highest score. Following the same pattern the user would expect to find a â€œcurtain rodâ€ product, so it will type a query like â€œI need a StyleWell curtain rod.â€ Agentic RAG brings a new level of sophistication by embedding agentic capabilities into the architecture. Instead of a fixed pipeline, agentic RAG introduces an AI â€œagentâ€ that can: Plan its steps toward a goal. Use multiple tools and sources. Reason iteratively. Reflect and revise its approach if needed. This is where MCP (Model Context Protocol) enters the scene. If we think for a moment in our previous query the best approach would be to retrieve semantically similar results but first filtered by â€œStyleWellâ€ brand. How can an LLM achieve that? Through the use tools designed to enhance the ability to retrieve information or perform actions. The agent first decides if it has enough information to answer the query if not, it triggers an action to get the data it requires. Letâ€™s create a very simple MCP server in Python that our LLM can use to retrieve relevant products. With a â€œsearch_productsâ€ tool that allows to perform vector search with filters for price, brand, and date. Hereâ€™s a simplified example of how to implement an MCP tool for product search, enabling the agent to apply filters like brand and price dynamically. 1 from fastmcp import FastMCP 2 import json 3 import sys 4 from datetime import datetime 5 6 from mongo import search_products_by_embeddings 7 from ai import get_embeddings 8 9 mcp = FastMCP( "Agentic RAG Demo" ) 10 11 @mcp.tool() 12 async def search_products( 13 query: str, 14 limit: int = 10, 15 min_price: float = 0, 16 max_price: float = 1000000, 17 brand: str = None, 18 min_date: datetime = None, 19 max_date: datetime = None 20 ) -> str: 21 """Search for products in the database. 22 23 Args: 24 query: The search query embeddings: str 25 limit: Optional limit to the number of results. Min 10: int 26 min_price: Optional minimum price: float 27 max_price: Optional maximum price: float 28 brand: Optional brand: str 29 min_date: Optional minimum date to filter by date: datetime 30 max_date: Optional maximum date to filter by date: datetime 31 Returns: 32 JSON string containing search results 33 """ 34 embeddings = await get_embeddings(query) 35 matching_products = search_products_by_embeddings( 36 embeddings, limit, min_price, max_price, brand, min_date, max_date 37 ) 38 39 return json.dumps(matching_products, cls= DateTimeEncoder ) 40 41 if __name__ == "__main__" : 42 try : 43 mcp.run(transport= "stdio" ) 44 except Exception as e: 45 print(f "Error in MCP server: {str(e)}" , file=sys.stderr) 46 sys.exit(1) Now with our MCP support the first LLM call will return the instruction to retrieve data with the following information: 1 { 2 "response_type" : "tool" , 3 "tool_name" : "search_products" , 4 "tool_args" : { 5 "query" : "curtain rod" , 6 "brand" : "StyleWell" 7 } 8 } As seen it is setting automatically the â€œStyleWellâ€ as the â€œbrandâ€ filter to fetch data and then our Agentic RAG system can return a better response. In other queries that may involve price ranges, the traditional RAG system can retrieve good results, for example: However, it does not exactly match our requested range close enough, but when using the Agentic RAG approach, we can retrieve way better results based on our defined price range as seen in the following example: Another better filtering example occurs when asking for recent information. If the user needs to request information about a specific product for this year, the most relevant results on the semantic search might not contain matches, but when using the agent-based approach the correct filtering is applied. As part of the user query, we are sending metadata information with current date so the agent request retriev... [truncated]
   â””â”€ [https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037]
      Preview: Error: HTTP 403 for URL: https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
ðŸ”¬ Analyzer [3]: Analyzing 5 results, 9 contents

ðŸ”¬ Analyzer: Max iterations reached, proceeding to Writer

ðŸ”¬ Analyzer [3]: Analyzed 5 results, 9 contents
   â””â”€ New findings:
      [1] Traditional RAG is simpler to implement and maintain, suitable for applications with a fixed set of FAQs or simple query-based tasks.
      [2] Agentic RAG requires more sophisticated engineering but offers a more adaptable system for complex, evolving tasks.
      [3] Agentic RAG incorporates AI agents that can make decisions, select tools, and refine queries, providing more flexibility and accuracy.
      [4] Traditional RAG is typically faster and less expensive, while Agentic RAG works well for asynchronous tasks like research, summarization, and code correction.
      [5] Agentic RAG uses multi-component planning to handle more complex, interactive, and reliable AI solutions.
   â””â”€ Decision: Research complete (15 total findings)

ðŸ“¦ Compress: Compressing 15 findings, 9 contents
   â””â”€ Compressed to 3168 chars (from ~6160 raw chars)
   â””â”€ Sources cited: 7
   â””â”€ Preview:
      ## Key Findings
      - Traditional RAG employs a static workflow, whereas Agentic RAG uses autonomous AI agents that dynamically adjust prompts and strategies [6].
      - RAG is suitable for quick wins with static content and tasks where real-time, context-aware generation isn't crucial [2], [7].
      - Agentic RAG is beneficial for goal-based reasoning, context-aware generation, and real-time actions [3], [8].
      - Traditional RAG is simpler to implement and maintain, making it ideal for applications with fixed FAQs or simple queries [11].
      - Agentic RAG necessitates more sophisticated engineering but provides a more adaptable system for intricate, evolving tasks [12].
      
      ## Detailed Information
      
      **Traditional RAG:**
      
      - Traditional RAG systems use a static workflow [1], [6].
      - They are best suited for quick wins using static content and scenarios where real-time actions and context-aware generation are not critical, such as content creation and basic customer support [2], [7].
      - Traditional RAG features simple retrieval, limited adaptability, and reliance on static knowledge [4], [9].
      - It is simpler to implement and maintain, making it suitable for applications with a fixed set of FAQs or simple query-based tasks [11].
      - Traditional RAG is typically faster and less expensive [14].
      
      **Agentic RAG:**
      
      - Agentic RAG uses autonomous AI agents that can adjust prompts and strategies in real-time [1], [6]. These agents can make decisions, select tools, and refine queries for more intelligent and dependable AI systems [5], [10], [13].
      - It excels in scenarios requiring goal-based reasoning, context-aware generation, and real-time actions, such as robotics, legal advisory, healthcare diagnostics, and ongoing customer service [3], [8].
      - Agentic RAG offers more flexibility and accuracy [13].
      - While it demands more sophisticated engineering, it provides a more adaptable system for complex, evolving tasks [12].
      - Agentic RAG is well-suited for asynchronous tasks such as research, summarization, and code correction [14].
      - It incorporates multi-component planning to handle more complex, interactive, and reliable AI solutions [15].
      
      ## Sources
      [1] https://www.sprinklr.com/blog/agentic-ai-vs-rag/
      [2] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [3] https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/
      [4] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [5] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [6] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
      [7] https://pureinsights.com/blog/2025/agentic-rag-vs-traditional-rag/
      [8]  https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [9]  https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
      [10] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [11] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [12] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [13] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [14] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [15] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/

âœï¸ Writer: Composing response from 15 findings
   â””â”€ Using compressed notes (3168 chars)

âœï¸ Writer: Generated response (3188 chars)
   â””â”€ Sources used:
      [1] https://www.sprinklr.com/blog/agentic-ai-vs-rag/
      [2] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [3] https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/
   â””â”€ Response preview:
      ## í•µì‹¬ ìš”ì•½
      
      RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ëª¨ë‘ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì´ì§€ë§Œ, RAGëŠ” ì •ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´, Agent ê¸°ë°˜ RAGëŠ” ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì™€ ì „ëžµì„ ì¡°ì •í•©ë‹ˆë‹¤. [6] ë”°ë¼ì„œ RAGëŠ” ë¹„êµì  ê°„ë‹¨í•œ ìž‘ì—…ì—, Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ë³µìž¡í•˜ê³  ì—­ë™ì ì¸ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤.
      
      ## ì£¼ìš” ë°œê²¬ ì‚¬í•­
      
      *   **ì „í†µì ì¸ RAG**: ì •ì  ì½˜í…ì¸ ë¥¼ í™œìš©í•œ ë¹ ë¥¸ ê²°ê³¼ ë„ì¶œì— ì í•©í•˜ë©°, ì‹¤ì‹œê°„ ë¬¸ë§¥ ì¸ì‹ì´ë‚˜ ì•¡ì…˜ì´ ì¤‘ìš”í•˜ì§€ ì•Šì€ ìž‘ì—…ì— ìœ ìš©í•©ë‹ˆë‹¤. [2], [7] êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ê°€ ê°„ë‹¨í•©ë‹ˆë‹¤. [11]
      *   **Agent ê¸°ë°˜ RAG**: ëª©í‘œ ê¸°ë°˜ ì¶”ë¡ , ë¬¸ë§¥ ì¸ì‹ ìƒì„±, ì‹¤ì‹œê°„ ì•¡ì…˜ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ì— íš¨ê³¼ì ìž…ë‹ˆë‹¤. [3], [8] ë”ìš± ìœ ì—°í•˜ê³  ì •í™•í•˜ë©°, ë³µìž¡í•˜ê³  ì§„í™”í•˜ëŠ” ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤. [12], [13] ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì™€ ì „ëžµì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. [1], [6]
      
      ## ìƒì„¸ ë¶„ì„
      
      RAGì™€ Agent ê¸°ë°˜ RAGëŠ” LLMì˜ ëŠ¥ë ¥ì„ í™•ìž¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì•„í‚¤í…ì²˜ì´ì§€ë§Œ, ê·¸ ìž‘ë™ ë°©ì‹ê³¼ ì ìš© ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì°¨ì´ì ì„ ë³´ìž…ë‹ˆë‹¤.
      
      **1. ìž‘ë™ ë°©ì‹ì˜ ì°¨ì´ì :**
      
      *   **RAG (ì •ì  ì›Œí¬í”Œë¡œìš°):** ì „í†µì ì¸ RAG ì‹œìŠ¤í…œì€ ë¯¸ë¦¬ ì •ì˜ëœ ìˆœì„œì— ë”°ë¼ ìž‘ë™í•©ë‹ˆë‹¤. ë¨¼ì €, ì§ˆì˜ì— ë”°ë¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì— ìž…ë ¥í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì •ì ì´ë©° ì‹¤ì‹œê°„ ìƒí˜¸ ìž‘ìš©ì´ë‚˜ ì „ëžµ ì¡°ì •ì´ ì—†ìŠµë‹ˆë‹¤. [1], [6] ì¦‰, í•œ ë²ˆ ì •ì˜ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ê³ ì •ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìž…ë‹ˆë‹¤.
      *   **Agent ê¸°ë°˜ RAG (ë™ì  ì›Œí¬í”Œë¡œìš°):** Agent ê¸°ë°˜ RAGëŠ” ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—ì´ì „íŠ¸ëŠ” ìƒí™©ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ê³ , ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ë©°, ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì¿¼ë¦¬ë¥¼ ê°œì„ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [5], [10], [13] ì´ëŠ” ì‹œìŠ¤í…œì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•  ìˆ˜ ìžˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ëŠ” ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ê³ , ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜, ë‹¤ë¥¸ ê²€ìƒ‰ ì „ëžµì„ ì‹œë„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
      
      **2. ì ìš© ë¶„ì•¼ì˜ ì°¨ì´ì :**
      
      *   **RAG**: ì£¼ë¡œ ê³ ì •ëœ FAQ ì„¸íŠ¸ë‚˜ ê°„ë‹¨í•œ ì¿¼ë¦¬ ê¸°ë°˜ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸°ë³¸ì ì¸ ê³ ê° ì§€ì› ì±—ë´‡, ì½˜í…ì¸  ìƒì„±, ë‹¨ìˆœí•œ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë“±ì— í™œìš©ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [2], [7], [11] êµ¬í˜„ê³¼ ìœ ì§€ ê´€ë¦¬ê°€ ë¹„êµì  ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ê³  ìš´ì˜í•  ìˆ˜ ìžˆë‹¤ëŠ” ìž¥ì ì´ ìžˆìŠµë‹ˆë‹¤. ì†ë„ê°€ ë¹ ë¥´ê³  ë¹„ìš©ì´ ì €ë ´í•œ íŽ¸ìž…ë‹ˆë‹¤. [14]
      *   **Agent ê¸°ë°˜ RAG**: ë³µìž¡í•˜ê³  ì§„í™”í•˜ëŠ” ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¡œë³´í‹±ìŠ¤, ë²•ë¥  ìžë¬¸, ì˜ë£Œ ì§„ë‹¨, ì§€ì†ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ ë“± ëª©í‘œ ê¸°ë°˜ ì¶”ë¡ , ë¬¸ë§¥ ì¸ì‹ ìƒì„±, ì‹¤ì‹œê°„ ì•¡ì…˜ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ìœ ìš©í•©ë‹ˆë‹¤. [3], [8]  Agent ê¸°ë°˜ RAGëŠ” ë¹„ë™ê¸° ìž‘ì—…(ì—°êµ¬, ìš”ì•½, ì½”ë“œ ìˆ˜ì • ë“±)ì—ë„ ì í•©í•˜ë©°, ë³µìž¡í•˜ê³  ìƒí˜¸ ìž‘ìš©ì ì¸ AI ì†”ë£¨ì…˜ì„ ìœ„í•´ ë‹¤ì¤‘ êµ¬ì„± ìš”ì†Œ ê³„íšì„ í†µí•©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [14], [15]
      
      **3. êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ì˜ ì°¨ì´ì :**
      
      *   **RAG**: ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì¸í•´ êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ê°€ ìš©ì´í•©ë‹ˆë‹¤. [11] ë”°ë¼ì„œ ê°œë°œ ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì ì¸ ê²½ìš°ë‚˜ ë¹ ë¥¸ í”„ë¡œí† íƒ€ìž… ì œìž‘ì´ í•„ìš”í•œ ê²½ìš°ì— ì í•©í•©ë‹ˆë‹¤.
      *   **Agent ê¸°ë°˜ RAG**: ë³µìž¡í•œ ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•˜ë©°, ì‹œìŠ¤í…œ ì„¤ê³„ ë° ê´€ë¦¬ì— ë” ë§Žì€ ë…¸ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤. [12] í•˜ì§€ë§Œ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° íš¨ê³¼ì ìž…ë‹ˆë‹¤.
      
      ## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜
      
      [1] https://www.sprinklr.com/blog/agentic-ai-vs-rag/
      [2] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
      [3] https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/
      [4] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [5] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
      [6] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
      [7] https://pureinsights.com/blog/2025/agentic-rag-vs-traditional-rag/
      [8] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [9] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
      [10] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [11] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [12] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [13] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [14] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      [15] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
      
      ## ê²°ë¡  ë° í‰ê°€
      
      RAGì™€ Agent ê¸°ë°˜ RAGëŠ” ê°ìžì˜ ìž¥ë‹¨ì ì„ ê°€ì§€ê³  ìžˆìœ¼ë©°, í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ ì‚¬í•­ê³¼ ëª©í‘œì— ë”°ë¼ ì ì ˆí•œ ì ‘ê·¼ ë°©ì‹ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ë¹ ë¥¸ ê²°ê³¼ì™€ ê°„ë‹¨í•œ êµ¬í˜„ì´ ì¤‘ìš”í•˜ë‹¤ë©´ RAGê°€ ì¢‹ì€ ì„ íƒì´ì§€ë§Œ, ë³µìž¡í•˜ê³  ì—­ë™ì ì¸ í™˜ê²½ì—ì„œ ë†’ì€ ì •í™•ì„±ê³¼ ìœ ì—°ì„±ì´ í•„ìš”í•˜ë‹¤ë©´ Agent ê¸°ë°˜ RAGê°€ ë” ë‚˜ì€ ì„ íƒì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê¶ê·¹ì ìœ¼ë¡œ, ìµœì ì˜ ì†”ë£¨ì…˜ì€ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘í•˜ê²Œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, Agent ê¸°ë°˜ RAGëŠ” ì•„ì§ ë°œì „ ì¤‘ì¸ ê¸°ìˆ ì´ë¯€ë¡œ, ì§€ì†ì ì¸ ì—°êµ¬ì™€ ê°œë°œì„ í†µí•´ ê·¸ ìž ìž¬ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìžˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”‚ Phase 2 Benchmark Result
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ Query: RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ì°¨ì´ì ê³¼ ê°ê° ì–¸...
â”‚ Time: 46.2s
â”‚ LLM Calls: 2
â”‚ Search Calls: 3
â”‚ URLs Read: 9
â”‚ Iterations: 3
â”‚ Est. Tokens: 1199
â”‚ Response Length: 3188 chars
â”‚ Has Citations: âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


======================================================================
ðŸ“„ FULL RESPONSE:
======================================================================
## í•µì‹¬ ìš”ì•½

RAG(Retrieval-Augmented Generation)ì™€ Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ëª¨ë‘ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì´ì§€ë§Œ, RAGëŠ” ì •ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´, Agent ê¸°ë°˜ RAGëŠ” ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì™€ ì „ëžµì„ ì¡°ì •í•©ë‹ˆë‹¤. [6] ë”°ë¼ì„œ RAGëŠ” ë¹„êµì  ê°„ë‹¨í•œ ìž‘ì—…ì—, Agent ê¸°ë°˜ ì ‘ê·¼ë²•ì€ ë³µìž¡í•˜ê³  ì—­ë™ì ì¸ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤.

## ì£¼ìš” ë°œê²¬ ì‚¬í•­

*   **ì „í†µì ì¸ RAG**: ì •ì  ì½˜í…ì¸ ë¥¼ í™œìš©í•œ ë¹ ë¥¸ ê²°ê³¼ ë„ì¶œì— ì í•©í•˜ë©°, ì‹¤ì‹œê°„ ë¬¸ë§¥ ì¸ì‹ì´ë‚˜ ì•¡ì…˜ì´ ì¤‘ìš”í•˜ì§€ ì•Šì€ ìž‘ì—…ì— ìœ ìš©í•©ë‹ˆë‹¤. [2], [7] êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ê°€ ê°„ë‹¨í•©ë‹ˆë‹¤. [11]
*   **Agent ê¸°ë°˜ RAG**: ëª©í‘œ ê¸°ë°˜ ì¶”ë¡ , ë¬¸ë§¥ ì¸ì‹ ìƒì„±, ì‹¤ì‹œê°„ ì•¡ì…˜ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ì— íš¨ê³¼ì ìž…ë‹ˆë‹¤. [3], [8] ë”ìš± ìœ ì—°í•˜ê³  ì •í™•í•˜ë©°, ë³µìž¡í•˜ê³  ì§„í™”í•˜ëŠ” ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤. [12], [13] ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì™€ ì „ëžµì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. [1], [6]

## ìƒì„¸ ë¶„ì„

RAGì™€ Agent ê¸°ë°˜ RAGëŠ” LLMì˜ ëŠ¥ë ¥ì„ í™•ìž¥í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì•„í‚¤í…ì²˜ì´ì§€ë§Œ, ê·¸ ìž‘ë™ ë°©ì‹ê³¼ ì ìš© ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì°¨ì´ì ì„ ë³´ìž…ë‹ˆë‹¤.

**1. ìž‘ë™ ë°©ì‹ì˜ ì°¨ì´ì :**

*   **RAG (ì •ì  ì›Œí¬í”Œë¡œìš°):** ì „í†µì ì¸ RAG ì‹œìŠ¤í…œì€ ë¯¸ë¦¬ ì •ì˜ëœ ìˆœì„œì— ë”°ë¼ ìž‘ë™í•©ë‹ˆë‹¤. ë¨¼ì €, ì§ˆì˜ì— ë”°ë¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì— ìž…ë ¥í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì •ì ì´ë©° ì‹¤ì‹œê°„ ìƒí˜¸ ìž‘ìš©ì´ë‚˜ ì „ëžµ ì¡°ì •ì´ ì—†ìŠµë‹ˆë‹¤. [1], [6] ì¦‰, í•œ ë²ˆ ì •ì˜ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ê³ ì •ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìž…ë‹ˆë‹¤.
*   **Agent ê¸°ë°˜ RAG (ë™ì  ì›Œí¬í”Œë¡œìš°):** Agent ê¸°ë°˜ RAGëŠ” ìžìœ¨ì ì¸ AI ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—ì´ì „íŠ¸ëŠ” ìƒí™©ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ê³ , ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ë©°, ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì¿¼ë¦¬ë¥¼ ê°œì„ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [5], [10], [13] ì´ëŠ” ì‹œìŠ¤í…œì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•  ìˆ˜ ìžˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—ì´ì „íŠ¸ëŠ” ê²€ìƒ‰ëœ ì •ë³´ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ê³ , ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ê±°ë‚˜, ë‹¤ë¥¸ ê²€ìƒ‰ ì „ëžµì„ ì‹œë„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

**2. ì ìš© ë¶„ì•¼ì˜ ì°¨ì´ì :**

*   **RAG**: ì£¼ë¡œ ê³ ì •ëœ FAQ ì„¸íŠ¸ë‚˜ ê°„ë‹¨í•œ ì¿¼ë¦¬ ê¸°ë°˜ ìž‘ì—…ì— ì í•©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸°ë³¸ì ì¸ ê³ ê° ì§€ì› ì±—ë´‡, ì½˜í…ì¸  ìƒì„±, ë‹¨ìˆœí•œ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ ë“±ì— í™œìš©ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [2], [7], [11] êµ¬í˜„ê³¼ ìœ ì§€ ê´€ë¦¬ê°€ ë¹„êµì  ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì— ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ê³  ìš´ì˜í•  ìˆ˜ ìžˆë‹¤ëŠ” ìž¥ì ì´ ìžˆìŠµë‹ˆë‹¤. ì†ë„ê°€ ë¹ ë¥´ê³  ë¹„ìš©ì´ ì €ë ´í•œ íŽ¸ìž…ë‹ˆë‹¤. [14]
*   **Agent ê¸°ë°˜ RAG**: ë³µìž¡í•˜ê³  ì§„í™”í•˜ëŠ” ìž‘ì—…ì— ë” ì í•©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¡œë³´í‹±ìŠ¤, ë²•ë¥  ìžë¬¸, ì˜ë£Œ ì§„ë‹¨, ì§€ì†ì ì¸ ê³ ê° ì„œë¹„ìŠ¤ ë“± ëª©í‘œ ê¸°ë°˜ ì¶”ë¡ , ë¬¸ë§¥ ì¸ì‹ ìƒì„±, ì‹¤ì‹œê°„ ì•¡ì…˜ì´ í•„ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ìœ ìš©í•©ë‹ˆë‹¤. [3], [8]  Agent ê¸°ë°˜ RAGëŠ” ë¹„ë™ê¸° ìž‘ì—…(ì—°êµ¬, ìš”ì•½, ì½”ë“œ ìˆ˜ì • ë“±)ì—ë„ ì í•©í•˜ë©°, ë³µìž¡í•˜ê³  ìƒí˜¸ ìž‘ìš©ì ì¸ AI ì†”ë£¨ì…˜ì„ ìœ„í•´ ë‹¤ì¤‘ êµ¬ì„± ìš”ì†Œ ê³„íšì„ í†µí•©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [14], [15]

**3. êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ì˜ ì°¨ì´ì :**

*   **RAG**: ê°„ë‹¨í•œ êµ¬ì¡°ë¡œ ì¸í•´ êµ¬í˜„ ë° ìœ ì§€ ê´€ë¦¬ê°€ ìš©ì´í•©ë‹ˆë‹¤. [11] ë”°ë¼ì„œ ê°œë°œ ë¦¬ì†ŒìŠ¤ê°€ ì œí•œì ì¸ ê²½ìš°ë‚˜ ë¹ ë¥¸ í”„ë¡œí† íƒ€ìž… ì œìž‘ì´ í•„ìš”í•œ ê²½ìš°ì— ì í•©í•©ë‹ˆë‹¤.
*   **Agent ê¸°ë°˜ RAG**: ë³µìž¡í•œ ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•˜ë©°, ì‹œìŠ¤í…œ ì„¤ê³„ ë° ê´€ë¦¬ì— ë” ë§Žì€ ë…¸ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤. [12] í•˜ì§€ë§Œ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° íš¨ê³¼ì ìž…ë‹ˆë‹¤.

## ê´€ë ¨ ìžë£Œ ë° ì¶œì²˜

[1] https://www.sprinklr.com/blog/agentic-ai-vs-rag/
[2] https://medium.com/@datajournal/rag-vs-agentic-rag-a-comprehensive-guide-6711cce24037
[3] https://mitrix.io/blog/llm-fine%E2%80%91tuning-vs-rag-vs-agents-a-practical-comparison/
[4] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[5] https://medium.com/@gaddam.rahul.kumar/agentic-rag-vs-traditional-rag-b1a156f72167
[6] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
[7] https://pureinsights.com/blog/2025/agentic-rag-vs-traditional-rag/
[8] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[9] https://www.pingcap.com/article/agentic-rag-vs-traditional-rag-key-differences-benefits/
[10] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[11] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[12] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[13] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[14] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/
[15] https://www.bitcot.com/rag-vs-agentic-rag-vs-mcp/

## ê²°ë¡  ë° í‰ê°€

RAGì™€ Agent ê¸°ë°˜ RAGëŠ” ê°ìžì˜ ìž¥ë‹¨ì ì„ ê°€ì§€ê³  ìžˆìœ¼ë©°, í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ ì‚¬í•­ê³¼ ëª©í‘œì— ë”°ë¼ ì ì ˆí•œ ì ‘ê·¼ ë°©ì‹ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ë¹ ë¥¸ ê²°ê³¼ì™€ ê°„ë‹¨í•œ êµ¬í˜„ì´ ì¤‘ìš”í•˜ë‹¤ë©´ RAGê°€ ì¢‹ì€ ì„ íƒì´ì§€ë§Œ, ë³µìž¡í•˜ê³  ì—­ë™ì ì¸ í™˜ê²½ì—ì„œ ë†’ì€ ì •í™•ì„±ê³¼ ìœ ì—°ì„±ì´ í•„ìš”í•˜ë‹¤ë©´ Agent ê¸°ë°˜ RAGê°€ ë” ë‚˜ì€ ì„ íƒì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ê¶ê·¹ì ìœ¼ë¡œ, ìµœì ì˜ ì†”ë£¨ì…˜ì€ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì‹ ì¤‘í•˜ê²Œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, Agent ê¸°ë°˜ RAGëŠ” ì•„ì§ ë°œì „ ì¤‘ì¸ ê¸°ìˆ ì´ë¯€ë¡œ, ì§€ì†ì ì¸ ì—°êµ¬ì™€ ê°œë°œì„ í†µí•´ ê·¸ ìž ìž¬ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìžˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.
======================================================================


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Phase 2 Summary (3 tests)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘  Average Time: 36.54s
â•‘  Average Tokens: 1175
â•‘  Citation Rate: 100%
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“ Results saved to: benchmark_results/phase_2_20251220_021217.json
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Benchmark completed at: Sat Dec 20 02:12:18 AM KST 2025
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
