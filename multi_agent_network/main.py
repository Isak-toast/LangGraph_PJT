```
"""
LangGraph Multi-Agent: Network (Mesh) Pattern
==============================================
ì´ ì˜ˆì œëŠ” ì¤‘ì•™ ê°ë…ì ì—†ì´ ì—ì´ì „íŠ¸ë¼ë¦¬ ì„œë¡œ ì‘ì—…ì„ ë„˜ê¸°ëŠ”(Handoff) ë„¤íŠ¸ì›Œí¬(Mesh) íŒ¨í„´ì…ë‹ˆë‹¤.
ê° ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ tools ëª©ë¡ì— "ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ ì´ë™í•˜ëŠ” ë„êµ¬"ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

í•µì‹¬ ê°œë…:
1. Handoff Tool (ì´ê´€ ë„êµ¬):
   - "Researcherì—ê²Œ ì „ë‹¬", "Writerì—ê²Œ ì „ë‹¬"ê³¼ ê°™ì€ íŠ¹ìˆ˜ ë„êµ¬
   - ì´ ë„êµ¬ê°€ í˜¸ì¶œë˜ë©´ ë¼ìš°í„°ê°€ ì´ë¥¼ ê°ì§€í•˜ì—¬ í•´ë‹¹ ì—ì´ì „íŠ¸ ë…¸ë“œë¡œ ì‹¤í–‰ íë¦„ì„ ì˜®ê¹ë‹ˆë‹¤.

2. Decentralized Logic (íƒˆì¤‘ì•™í™” ë¡œì§):
   - ì¤‘ì•™ ê´€ë¦¬ì ì—†ì´, ê° ì—ì´ì „íŠ¸(LLM)ê°€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ ëˆ„êµ¬ì—ê²Œ ë„˜ê¸¸ì§€ ê²°ì •í•©ë‹ˆë‹¤.

ì‹¤í–‰ íë¦„:
[User] -> [Researcher] --(ì •ë³´ ë¶€ì¡±)--> [Search Tool] -> [Researcher]
              |
         (ì •ë³´ ì¶©ë¶„, ì‘ì„± ìš”ì²­)
              â†“
           [Writer] --(ì‘ì„± ì™„ë£Œ)--> [END]
"""

import os
import dotenv
from typing import Annotated, List, Literal, TypedDict, Union
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import ToolNode
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
script_dir = Path(__file__).parent
project_root = script_dir.parent
env_file = project_root / ".env"
if not env_file.exists():
    env_file = script_dir / ".env"
dotenv.load_dotenv(env_file)

# LangSmith ì¶”ì  ì„¤ì •
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print("ğŸ“Š LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   í”„ë¡œì íŠ¸: {os.getenv('LANGCHAIN_PROJECT', 'default')}")


# =============================================================================
# 1. ì„¤ì • ë° ë„êµ¬ (Configs & Tools)
# =============================================================================
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)

# --- Handoff Tools (ì´ê´€ ë„êµ¬) ---
# ì´ ë„êµ¬ë“¤ì€ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸°ë³´ë‹¤ëŠ”, ê·¸ë˜í”„ì˜ ìƒíƒœ(ë‹¤ìŒ ë…¸ë“œ)ë¥¼ ë³€ê²½í•˜ëŠ” ì‹ í˜¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

def transfer_to_writer():
    """Transfer control to the Writer agent."""
    return "Transferred to Writer"

def transfer_to_researcher():
    """Transfer control to the Researcher agent."""
    return "Transferred to Researcher"

# ê²€ìƒ‰ ë„êµ¬
search_tool = TavilySearchResults(k=2)

# --- ì—ì´ì „íŠ¸ë³„ ì‚¬ìš©í•  ë„êµ¬ ì •ì˜ ---
# Researcher: ê²€ìƒ‰ ê°€ëŠ¥ + Writerì—ê²Œ ë„˜ê¸°ê¸° ê°€ëŠ¥
researcher_tools = [search_tool, transfer_to_writer]

# Writer: ê²€ìƒ‰ ë¶ˆê°€(ì œí•œ) + Researcherì—ê²Œ (ì¬)ìš”ì²­ ê°€ëŠ¥
writer_tools = [transfer_to_researcher] 

# LLMì— ë„êµ¬ ë°”ì¸ë”©
researcher_model = llm.bind_tools(researcher_tools)
writer_model = llm.bind_tools(writer_tools)


# --- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê° ì—ì´ì „íŠ¸ì˜ ì—­í•  ì •ì˜) ---
detailed_researcher_prompt = """You are a Researcher. 
1. Search for information requested by the user. 
2. If you have found enough info, transfer to the Writer to draft the response.
3. If you need the Writer to explain something or format it, transfer to them."""

detailed_writer_prompt = """You are a Writer. 
1. Write a high-quality response based on the research provided.
2. If you need more information, transfer back to the Researcher.
3. If you are done, just output the final answer."""


# =============================================================================
# 2. ìƒíƒœ(State) ë° ë…¸ë“œ(Nodes) ì •ì˜
# =============================================================================
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    sender: str # ë§ˆì§€ë§‰ìœ¼ë¡œ ë©”ì‹œì§€ë¥¼ ë³´ë‚¸ ì—ì´ì „íŠ¸ (ì„ íƒì‚¬í•­)

def researcher(state: AgentState):
    """Researcher ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    print("---[Node] Researcher ë™ì‘---")
    res = researcher_model.invoke([SystemMessage(content=detailed_researcher_prompt)] + state["messages"])
    return {"messages": [res], "sender": "researcher"}

def writer(state: AgentState):
    """Writer ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    print("---[Node] Writer ë™ì‘---")
    res = writer_model.invoke([SystemMessage(content=detailed_writer_prompt)] + state["messages"])
    return {"messages": [res], "sender": "writer"}

# ì¼ë°˜ ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ (ê²€ìƒ‰ ë“±)
# Handoff ë„êµ¬ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì‹¤ì œ ë„êµ¬ë§Œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.
tool_node = ToolNode([search_tool])


# =============================================================================
# 3. ë¼ìš°íŒ… ë¡œì§ (Routing Logic)
# =============================================================================
def router(state: AgentState) -> Literal["call_tool", "enter_writer", "enter_researcher", "__end__"]:
    """
    ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ tool_callsë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì´ë™ ê²½ë¡œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    messages = state["messages"]
    last_message = messages[-1]
    
    if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
        tool_name = last_message.tool_calls[0]["name"]
        
        # Handoff ë„êµ¬ì¸ì§€ í™•ì¸
        if tool_name == "transfer_to_writer":
            return "enter_writer"
        elif tool_name == "transfer_to_researcher":
            return "enter_researcher"
        else:
            return "call_tool" # ì¼ë°˜ ë„êµ¬(ê²€ìƒ‰ ë“±)ëŠ” ToolNodeë¡œ ë³´ëƒ„
            
    return "__end__" # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¢…ë£Œ


# =============================================================================
# 4. ê·¸ë˜í”„(Graph) êµ¬ì¶•
# =============================================================================
workflow = StateGraph(AgentState)

workflow.add_node("researcher", researcher)
workflow.add_node("writer", writer)
workflow.add_node("tools", tool_node)

# ì‹œì‘ì : Researcherë¶€í„° ì‹œì‘
workflow.add_edge(START, "researcher")

# Researcherì˜ ì¶œë ¥ì— ë”°ë¥¸ ë¶„ê¸°
workflow.add_conditional_edges(
    "researcher",
    router,
    {
        "enter_writer": "writer",
        "enter_researcher": "researcher", # ìê¸° ìì‹ ì—ê²Œ ëŒì•„ì˜¤ëŠ” ê²½ìš° (ë“œë¬¾)
        "call_tool": "tools",
        "__end__": END
    }
)

# Writerì˜ ì¶œë ¥ì— ë”°ë¥¸ ë¶„ê¸°
workflow.add_conditional_edges(
    "writer",
    router,
    {
        "enter_writer": "writer",
        "enter_researcher": "researcher",
        "call_tool": "tools",
        "__end__": END
    }
)

# ë„êµ¬ ì‹¤í–‰ í›„ì—ëŠ”, ë„êµ¬ë¥¼ í˜¸ì¶œí–ˆë˜ ì—ì´ì „íŠ¸(ì—¬ê¸°ì„  í¸ì˜ìƒ Researcher)ë¡œ ëŒì•„ê°
# (ë” ë³µì¡í•œ êµ¬ì¡°ì—ì„œëŠ” sender í•„ë“œë¥¼ ë³´ê³  ë™ì ìœ¼ë¡œ ëŒì•„ê°ˆ ê³³ì„ ì •í•  ìˆ˜ë„ ìˆìŒ)
workflow.add_edge("tools", "researcher")

app = workflow.compile()


# =============================================================================
# 5. ì‹¤í–‰(Execution)
# =============================================================================
def main():
    print("Initializing Multi-Agent Network (Mesh)...")
    try:
        with open("network_graph.png", "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print("Graph saved to 'network_graph.png'")
    except Exception as e:
        print(f"Skipping visualization: {e}")

    user_input = "Find a short summary of the philosophy of Stoicism and write a haiku about it."
    print(f"\n--- User Query: {user_input} ---\n")
    
    events = app.stream(
        {"messages": [HumanMessage(content=user_input)]},
        {"recursion_limit": 20} # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ì œí•œ
    )
    
    for event in events:
        for k, v in event.items():
            if "messages" in v:
                last_msg = v["messages"][-1]
                
                # ë©”ì‹œì§€ ì¶œë ¥ (ëˆ„ê°€ ë³´ëƒˆëŠ”ì§€ì™€ ë‚´ìš©)
                sender = v.get("sender", "Tool")
                print(f"[{sender}]: {last_msg.content}")
                
                # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ í‘œì‹œ
                if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                     for tc in last_msg.tool_calls:
                        print(f"  â””â”€ ğŸš€ Action: {tc['name']}")

    print("\n--- Final Sequence Completed ---")
    
if __name__ == "__main__":
    main()
```
