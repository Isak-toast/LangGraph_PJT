
"""
LangGraph 06: Agentic RAG (Adaptive RAG)
=========================================
ì´ ì˜ˆì œëŠ” Agentic RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
ë‹¨ìˆœíˆ ê²€ìƒ‰í•˜ê³  ë‹µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìœ ìš©í•œì§€ 'í‰ê°€(Grade)'í•˜ê³ 
í•„ìš”í•˜ë©´ ë‹¤ì‹œ ê²€ìƒ‰í•˜ê±°ë‚˜, ë‹µë³€ ìƒì„±ì„ ì§„í–‰í•˜ëŠ” ëŠ¥ë™ì ì¸ íë¦„ì„ ê°€ì§‘ë‹ˆë‹¤.

í•µì‹¬ ê°œë…:
1. Retrieval (ê²€ìƒ‰): Tavily Searchë¥¼ í†µí•´ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
2. Grading (í‰ê°€): LLM(Structured Output)ì„ ì‚¬ìš©í•´ ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜(Yes/No) íŒë‹¨
3. Conditional Logic (ì¡°ê±´ë¶€ ë¡œì§): 
   - ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ -> ë‹µë³€ ìƒì„±(Generate)
   - ì—†ìœ¼ë©´ -> (ì´ ì˜ˆì œì—ì„ ) ì¢…ë£Œí•˜ê±°ë‚˜ ì¬ê²€ìƒ‰ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

ì‹¤í–‰ íë¦„:
[Retrieve] --> [Grade Documents] --(Relevant?)--> [Generate] --> [END]
                                        |
                                   (No Docs)
                                        â†“
                               [Generate (Unknown)]
"""

import os
import dotenv
from typing import Annotated, List, TypedDict, Literal
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from pydantic import BaseModel, Field
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
script_dir = Path(__file__).parent
project_root = script_dir.parent
env_file = project_root / ".env"
if not env_file.exists():
    env_file = script_dir / ".env"
dotenv.load_dotenv(env_file)

# LangSmith ì¶”ì  ì„¤ì •
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print("ğŸ“Š LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   í”„ë¡œì íŠ¸: {os.getenv('LANGCHAIN_PROJECT', 'default')}")


# =============================================================================
# 1. ì„¤ì • (Config)
# =============================================================================
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)
search_tool = TavilySearchResults(k=3)


# =============================================================================
# 2. ì»´í¬ë„ŒíŠ¸(Components) ì •ì˜: Grader & Generator
# =============================================================================

# --- 2.1 ë¬¸ì„œ í‰ê°€ê¸° (Grader) ---
class GradeDocuments(BaseModel):
    """ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ 'yes' ë˜ëŠ” 'no'ë¡œ í‰ê°€í•˜ëŠ” ë°”ì´ë„ˆë¦¬ ìŠ¤ì½”ì–´ ëª¨ë¸"""
    binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")

# Structured Outputì„ ì‚¬ìš©í•˜ì—¬ LLMì´ í•­ìƒ JSON í˜•íƒœ(GradeDocuments)ë¡œ ì‘ë‹µí•˜ê²Œ ê°•ì œí•¨
structured_llm_grader = llm.with_structured_output(GradeDocuments)

def grade_documents(state):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì—¬ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    print("---[Grade] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì¤‘---")
    question = state["question"]
    documents = state["documents"]
    
    # ì˜ˆì œ ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì²« ë²ˆì§¸ ë¬¸ì„œë§Œ í‰ê°€í•©ë‹ˆë‹¤ (ì‹¤ì œë¡œëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ í‰ê°€í•´ì•¼ í•¨)
    score = structured_llm_grader.invoke(f"User question: {question}\n\nRetrieved document: {documents[0]['content']}")
    grade = score.binary_score
    
    if grade == "yes":
        print("   >>> ê²°ì •: ë¬¸ì„œê°€ ê´€ë ¨ ìˆìŒ (Relevant)")
        return {"documents": documents}
    else:
        print("   >>> ê²°ì •: ë¬¸ì„œê°€ ê´€ë ¨ ì—†ìŒ (Not Relevant)")
        # ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œëŠ” í•„í„°ë§ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
        return {"documents": []} 


# --- 2.2 ë‹µë³€ ìƒì„±ê¸° (Generator) ---
def generate(state):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("---[Generate] ë‹µë³€ ìƒì„± ì¤‘---")
    question = state["question"]
    documents = state["documents"]
    
    # ê´€ë ¨ ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•¨
    if not documents:
        return {"generation": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    # ë¬¸ì„œ ë‚´ìš© ì—°ê²° (Context)
    context = "\n\n".join([doc['content'] for doc in documents])
    
    # RAG í”„ë¡¬í”„íŠ¸
    prompt = f"""You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
    
    Question: {question} 
    
    Context: {context} 
    
    Answer:"""
    
    generation = llm.invoke(prompt)
    return {"generation": generation.content}


# --- 2.3 ê²€ìƒ‰ê¸° (Retriever) ---
def retrieve(state):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    print("---[Retrieve] ì •ë³´ ê²€ìƒ‰ ì¤‘---")
    question = state["question"]
    docs = search_tool.invoke(question)
    # TavilyëŠ” [{'content': '...', 'url': '...'}, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return {"documents": docs}


# =============================================================================
# 3. ìƒíƒœ(State) ì •ì˜
# =============================================================================
class GraphState(TypedDict):
    """
    ê·¸ë˜í”„ ìƒíƒœ: ì§ˆë¬¸, ìƒì„±ëœ ë‹µë³€, ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì €ì¥
    """
    question: str
    generation: str
    documents: List[dict]


# =============================================================================
# 4. ê·¸ë˜í”„(Graph) êµ¬ì¶•
# =============================================================================
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)

# ì—£ì§€ ì—°ê²°
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")

# ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
def decide_to_generate(state):
    """
    ë¬¸ì„œ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    - ë¬¸ì„œê°€ ìˆìœ¼ë©´(Relevant) -> generate
    - ë¬¸ì„œê°€ ì—†ìœ¼ë©´(Not Relevant) -> generate (ì—¬ê¸°ì„  ë°”ë¡œ ì¢…ë£Œí•˜ê±°ë‚˜ ëª¨ë¥¸ë‹¤ê³  í•˜ê¸° ìœ„í•´)
    (Advanced RAGì—ì„œëŠ” ì—¬ê¸°ì„œ 'transform_query'ë¡œ ê°€ì„œ ì¬ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.)
    """
    if not state["documents"]:
        return "generate" # ë¬¸ì„œê°€ ì—†ì–´ë„ generateë¡œ ê°€ì„œ "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€
    return "generate"

workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "generate": "generate",
    }
)
workflow.add_edge("generate", END)

# ì»´íŒŒì¼
app = workflow.compile()


# =============================================================================
# 5. ì‹¤í–‰(Execution)
# =============================================================================
def main():
    print("Initializing Agentic RAG...\n")
    
    # ì‹œê°í™”
    try:
        with open("agentic_rag_graph.png", "wb") as f:
            f.write(app.get_graph().draw_mermaid_png())
        print("Graph saved to 'agentic_rag_graph.png'")
    except Exception as e:
        print(f"Skipping visualization: {e}")

    inputs = {"question": "What are the key features of LangGraph?"}
    print(f"\n--- User Question: {inputs['question']} ---")
    
    for output in app.stream(inputs):
        for key, value in output.items():
            # ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ ë©”ì‹œì§€
            pass # ì¶œë ¥ì€ ê° ë…¸ë“œ í•¨ìˆ˜ ë‚´ë¶€ì˜ printë¬¸ì— ë§¡ê¹€

    print("\n--- Final Result ---")
    # stream()ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ê°’ì´ ìµœì¢… ìƒíƒœë¼ê³  ë³´ì¥í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë§ˆì§€ë§‰ìœ¼ë¡œ ì¡íŒ valueë¥¼ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” run loop êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„)
    if 'generation' in value:
         print(f"ğŸ¤– Answer: {value['generation']}")

if __name__ == "__main__":
    main()
