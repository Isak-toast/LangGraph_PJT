
"""
LangGraph: Reflection (ì„±ì°°)
==============================
ì´ ì˜ˆì œëŠ” Reflection ì—ì´ì „íŠ¸ íŒ¨í„´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
ì—ì´ì „íŠ¸ê°€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ìŠ¤ìŠ¤ë¡œ(í˜¹ì€ ë³„ë„ì˜ ë…¸ë“œê°€) ë¹„í‰(Critique)í•œ ë’¤,
ë¹„í‰ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ê³¼ë¥¼ ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.

êµ¬ì¡° (Loop):
[Generate] -> [Reflect/Critique] -> [should_continue?] --(Yes)--> [Generate] (ê°œì„ )
                                            |
                                          (No)
                                            â†“
                                          [END]
"""

import os
import dotenv
from typing import Annotated, List, TypedDict
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
script_dir = Path(__file__).parent
project_root = script_dir.parent
env_file = project_root / ".env"
if not env_file.exists():
    env_file = script_dir / ".env"
dotenv.load_dotenv(env_file)

# LangSmith ì¶”ì  ì„¤ì •
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print("ğŸ“Š LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   í”„ë¡œì íŠ¸: {os.getenv('LANGCHAIN_PROJECT', 'default')}")


# =============================================================================
# 1. ì„¤ì • ë° ìƒíƒœ
# =============================================================================
class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)


# =============================================================================
# 2. ë…¸ë“œ (Nodes)
# =============================================================================

def generation_node(state: State):
    """
    [ìƒì„±] ì‚¬ìš©ì ìš”ì²­ ë˜ëŠ” ë¹„í‰ì„ ë°˜ì˜í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±/ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    print("---[Generate] í…ìŠ¤íŠ¸ ìƒì„± ì¤‘---")
    # messages ë¦¬ìŠ¤íŠ¸ì— ì´ì „ ëŒ€í™”(ë¹„í‰ í¬í•¨)ê°€ ìŒ“ì´ë¯€ë¡œ, LLMì€ ë¬¸ë§¥ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    return {"messages": [llm.invoke(state["messages"])]}

def reflection_node(state: State):
    """
    [ì„±ì°°] ì´ì „ ìƒì„±ë¬¼ì„ ë¹„í‰(Critique)í•©ë‹ˆë‹¤.
    """
    print("---[Reflect] ë¹„í‰(Critique) ìƒì„± ì¤‘---")
    last_msg = state["messages"][-1]
    
    # LLMì—ê²Œ ë¹„í‰ê°€(Critic) í˜ë¥´ì†Œë‚˜ ë¶€ì—¬
    reflection_prompt = f"You are a strict critic. Critique the following text for style and accuracy. Provide constructive feedback to improve it.\n\nText:\n{last_msg.content}"
    
    critique = llm.invoke(reflection_prompt)
    
    # ë¹„í‰ ë‚´ìš©ì„ HumanMessageë¡œ ë³€í™˜í•˜ì—¬ Generatorê°€ ìœ ì € í”¼ë“œë°±ì²˜ëŸ¼ ëŠë¼ê²Œ í•¨ (ë˜ëŠ” System prompt í™œìš© ê°€ëŠ¥)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ [Critique] ì ‘ë‘ì–´ë¥¼ ë¶™ì—¬ ì „ë‹¬
    return {"messages": [HumanMessage(content=f"[Critique]: {critique.content}")]}

def should_continue(state: State):
    """
    ë£¨í”„ë¥¼ ê³„ì†í• ì§€ ê²°ì •í•©ë‹ˆë‹¤. (ì—¬ê¸°ì„œëŠ” íšŸìˆ˜ ì œí•œ ì‚¬ìš©)
    """
    # ë©”ì‹œì§€ ê°œìˆ˜ë¡œ ë°˜ë³µ íšŸìˆ˜ ì œì–´ (ì˜ˆ: 6ê°œ ì´ˆê³¼ ì‹œ ì¢…ë£Œ)
    # (Initial User Msg + Gen + Ref + Gen + Ref + Gen + Ref ...)
    if len(state["messages"]) > 6: 
        print("---ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼, ì¢…ë£Œ---")
        return END
    return "reflect"


# =============================================================================
# 3. ê·¸ë˜í”„ (Graph)
# =============================================================================
graph_builder = StateGraph(State)

graph_builder.add_node("generate", generation_node)
graph_builder.add_node("reflect", reflection_node)

graph_builder.add_edge(START, "generate")
# Generate í›„ì—ëŠ” ë£¨í”„ ì¡°ê±´ì„ í™•ì¸ (ê³„ì† ë¹„í‰í• ì§€, ëë‚¼ì§€)
graph_builder.add_conditional_edges("generate", should_continue, ["reflect", END])
# Reflect í›„ì—ëŠ” ë‹¤ì‹œ Generateë¡œ ëŒì•„ê°€ì„œ ê°œì„ 
graph_builder.add_edge("reflect", "generate")

graph = graph_builder.compile()


# =============================================================================
# 4. ì‹¤í–‰ (Execution)
# =============================================================================
def main():
    print("Initializing Reflection Agent...")
    try:
        with open("reflection_graph.png", "wb") as f:
            f.write(graph.get_graph().draw_mermaid_png())
        print("Graph saved to 'reflection_graph.png'")
    except Exception as e:
        print(f"Skipping visualization: {e}")
        
    initial_input = "Write a very short poem about coding bugs."
    print(f"--- User Input: {initial_input} ---")
    
    inputs = {"messages": [HumanMessage(content=initial_input)]}
    
    # stream_mode="values"ë¡œ ê° ë‹¨ê³„ì˜ ë©”ì‹œì§€ ë³€í™”ë¥¼ ê´€ì°°
    for event in graph.stream(inputs, stream_mode="values"):
        last_msg = event["messages"][-1]
        print(f"\n[{last_msg.type.upper()}]:\n{last_msg.content}")

if __name__ == "__main__":
    main()
