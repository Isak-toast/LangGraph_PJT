"""
LangGraph Basic: Single Agent (ReAct)
======================================
ì´ ì˜ˆì œëŠ” LangGraphì—ì„œ ê°€ì¥ ê¸°ë³¸ì ì¸ ë‹¨ì¼ ì—ì´ì „íŠ¸(Single Agent)ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
`create_react_agent`ë¼ëŠ” ë¯¸ë¦¬ ë§Œë“¤ì–´ì§„(prebuilt) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´, ë³µì¡í•œ ê·¸ë˜í”„ ì •ì˜ ì—†ì´ë„
ReAct(Reasoning + Acting) ì—ì´ì „íŠ¸ë¥¼ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•µì‹¬ ê°œë…:
1. create_react_agent: 
   - LLMê³¼ Toolì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ [LLM] <-> [Tool] ìˆœí™˜ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
   - ë‚´ë¶€ì ìœ¼ë¡œ MessageStateë¥¼ ì‚¬ìš©í•˜ë©°, Tool Callingì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

2. ReAct íŒ¨í„´:
   - ì§ˆë¬¸ -> ìƒê°(LLM) -> ë„êµ¬ ì„ íƒ -> ë„êµ¬ ì‹¤í–‰ -> ê²°ê³¼ ê´€ì°° -> ìµœì¢… ë‹µë³€
"""

import os
import dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
script_dir = Path(__file__).parent
project_root = script_dir.parent
env_file = project_root / ".env"
if not env_file.exists():
    env_file = script_dir / ".env"
dotenv.load_dotenv(env_file)

# LangSmith ì¶”ì  ì„¤ì •
if os.getenv("LANGCHAIN_TRACING_V2") == "true":
    print("ğŸ“Š LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   í”„ë¡œì íŠ¸: {os.getenv('LANGCHAIN_PROJECT', 'default')}")


# =============================================================================
# ì‹¤í–‰ (Execution)
# =============================================================================
def main():
    print("Initializing Single Agent (ReAct) System...")
    
    # 1. LLM ì„¤ì •
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)
    
    # 2. ë„êµ¬(Tools) ì„¤ì •
    # Tavily Search: ì›¹ ê²€ìƒ‰ ë„êµ¬ (ìµœëŒ€ 3ê°œ ê²°ê³¼)
    tavily_tool = TavilySearchResults(max_results=3)
    tools = [tavily_tool]

    # 3. ì—ì´ì „íŠ¸ ìƒì„± (LangGraph Prebuilt)
    # create_react_agentëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ìŒ êµ¬ì¡°ì˜ ê·¸ë˜í”„ë¥¼ ë§Œë“­ë‹ˆë‹¤:
    # [START] -> [model] --(tool_calls)--> [tools]
    #               ^________(result)________|
    agent = create_react_agent(
        llm, 
        tools, 
        state_modifier="You are a helpful AI assistant. Use tools to find up-to-date information."
        # state_modifierëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—­í• ì„ í•©ë‹ˆë‹¤.
    )

    # 4. ì‹œê°í™” (Optional)
    try:
        png_bytes = agent.get_graph().draw_mermaid_png()
        with open("agent_graph.png", "wb") as f:
            f.write(png_bytes)
        print("Graph visualization saved to 'agent_graph.png'")
    except Exception as e:
        print(f"Skipping visualization: {e}")

    # 5. ì‹¤í–‰
    user_input = "Who won the World Series in 2024? If not played yet, who won in 2023?"
    print(f"\n--- User Query ---\n{user_input}\n")
    
    messages = [HumanMessage(content=user_input)]
    
    print("--- Streaming Execution ---")
    
    # stream_mode="values": ê° ë‹¨ê³„ì˜ ì „ì²´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    for step in agent.stream({"messages": messages}, stream_mode="values"):
        # í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ ëª©ë¡
        current_messages = step["messages"]
        last_message = current_messages[-1]
        
        # ë©”ì‹œì§€ íƒ€ì…ê³¼ ë‚´ìš© ì¶œë ¥
        msg_type = last_message.type
        content = last_message.content
        
        print(f"\n[{msg_type.upper()}]: {content}")
        
        # ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¶œë ¥
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            for tc in last_message.tool_calls:
                 print(f"  â””â”€ ğŸ”§ Tool Call: {tc['name']}({tc['args']})")

if __name__ == "__main__":
    main()
